{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pydts-a-python-package-for-discrete-time-survival-analysis-with-competing-risks-and-optional-penalization","title":"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks and Optional Penalization","text":"<p>PyDTS is a Python package designed for discrete-time survival analysis with competing risks, offering tools for model fitting, evaluation, and simulation.</p> <p>PyDTS offers:</p> <ul> <li>Discrete-time competing-risks regression models.</li> <li>Automated procedures for hyperparameter tuning.</li> <li>Sure Independence Screening methods for feature selection.</li> <li>Model evaluation metrics for predictive accuracy and calibration.</li> <li>Simulation tools for generating synthetic datasets for research and testing.</li> </ul> <p>Additional simulations and illustrative examples are available in Meir and Gorfine (2025), Discrete-Time Competing-Risks Regression with or without Penalization, Biometrics (2025), and in the accompanying Github Repository</p>"},{"location":"#installation","title":"Installation","text":"<p>PyDTS can be installed using PyPI as follows:</p> <pre><code>pip install pydts\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<p>PyDTS supports Python versions 3.9\u20133.13.  </p> <p>The package requires the following dependencies (with version constraints chosen for compatibility across Python and NumPy/SciPy releases):  </p> <ul> <li> <p>NumPy   -- Python 3.9: <code>&gt;=1.26,&lt;2.1</code>   -- Python 3.10: <code>&gt;=1.26,&lt;2.3</code>   -- Python 3.11\u20133.13: <code>&gt;=1.26</code> (including NumPy 2.x)  </p> </li> <li> <p>SciPy   -- Python 3.9: <code>&gt;=1.13,&lt;1.14</code>   -- Python 3.10: <code>&gt;=1.14,&lt;1.16</code>   -- Python 3.11\u20133.13: <code>&gt;=1.15</code> </p> </li> <li> <p>pandas <code>&gt;=2.2.2</code> </p> </li> <li>scikit-learn <code>&gt;=1.6</code> </li> <li>statsmodels <code>&gt;=0.14.2</code> </li> <li>lifelines <code>&gt;=0.27</code> </li> <li>tqdm <code>&gt;=4.66</code> </li> <li>psutil <code>&gt;=5.9</code> </li> <li>seaborn <code>&gt;=0.13</code> </li> <li>formulaic <code>&gt;=1.0</code> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The following example demonstrates how to generate synthetic data and fit a <code>TwoStagesFitter</code> model.</p> <p>Detailed definitions and explanations are available in the methods section. </p> <p>The function <code>generate_quick_start_df</code> simulates a dataset with the following defaults:  </p> <ul> <li>Sample size: <code>n_patients=10000</code> </li> <li>Covariates: <code>n_cov=5</code> independent covariates, each drawn from Uniform(0,1) distribution </li> <li>Competing events: <code>j_events=2</code> event types  </li> <li>Time scale: <code>d_times=14</code> discrete time intervals  </li> <li>Hazard coefficients (default values):  </li> <li>\\(\\alpha_{1t}\\) = \u22121 \u2212 0.3 * log(t)  </li> <li>\\(\\alpha_{2t}\\) = \u22121.75 \u2212 0.15 * log(t)</li> <li>\\(\\beta_1\\) = \u2212log([0.8, 3, 3, 2.5, 2])  </li> <li>\\(\\beta_2\\) = \u2212log([1, 3, 4, 3, 2])  </li> </ul> <p>For each patient, a censoring time \\(C\\) is drawn from Uniform{1, ..., 14}. The observed time is defined as \\(X = min(T, C)\\), where \\(T\\) is the event time, sampled based on the covariates of each patient and the hazard coefficients. If censoring occurs before the event (\\(C &lt; T\\)), the event type is set to \\(J = 0\\).</p> <p>Once the dataset is generated, you can fit a <code>TwoStagesFitter</code> to the data (without columns \\(C\\) and \\(T\\) which are not observed in practice).</p> <p>You can generate synthetic data and fit your first <code>TwoStagesFitter</code> model with the following code: </p> <pre><code>from pydts.fitters import TwoStagesFitter\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\n\n# Generate a synthetic dataset with 10,000 patients,\n# 5 covariates, 14 discrete time intervals, and 2 competing events\npatients_df = generate_quick_start_df(n_patients=10000, n_cov=5, d_times=14, j_events=2, pid_col='pid', seed=0)\n\n# Initialize and fit the discrete-time competing-risk model\nfitter = TwoStagesFitter()\nfitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\n\n# Display model summary\nfitter.print_summary()\n</code></pre>"},{"location":"#citations","title":"Citations","text":"<p>If you found PyDTS software useful to your research, please cite the papers:</p> <pre><code>@article{Meir_PyDTS_2022,\n    author = {Meir, Tomer and Gutman, Rom, and Gorfine, Malka},\n    doi = {10.21105/joss.08815},\n    title = {{PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks and Optional Penalization}},\n    year = {2025},\n    journal = {Journal of Open Source Software},\n    volume = {10},\n    number = {115},\n    url = {https://doi.org/10.21105/joss.08815},\n    pages = {8815}\n}\n\n@article{Meir_Gorfine_DTSP_2025,\n    author = {Meir, Tomer and Gorfine, Malka},\n    doi = {10.1093/biomtc/ujaf040},\n    title = {{Discrete-Time Competing-Risks Regression with or without Penalization}},\n    year = {2025},\n    journal = {Biometrics},\n    volume = {81},\n    number = {2},\n    url = {https://academic.oup.com/biometrics/article/81/2/ujaf040/8120014},\n}\n</code></pre> <p>and please consider starring the project on GitHub</p>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Open Github issues to suggest new features or to report bugs\\errors.</li> <li>Provide feedback on the documentation.</li> <li>Contact Tomer or Rom if you want to add a usage example to the documentation. </li> <li>If you want to become a developer (thank you, we appreciate it!) - please contact Tomer or Rom for developers' on-boarding. </li> </ol> <p>Tomer Meir: tomer1812@gmail.com, Rom Gutman: rom.gutman1@gmail.com</p>"},{"location":"#running-tests-locally","title":"Running Tests Locally","text":"<p>To run the test suite on your local machine, follow these steps:</p> <ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/tomer1812/pydts.git\ncd pydts\n</code></pre> <ol> <li> <p>Install the package in editable mode <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>Run the test suite with Poetry <pre><code>pip install poetry\npoetry run pytest\n</code></pre></p> </li> </ol>"},{"location":"EventTimesSampler/","title":"Event Times Sampler","text":"<p>PyDTS provides <code>EventTimesSampler</code> (ETS) class for sampling discrete-time survival data with competing risks and right censoring under the log-link model. In the following, we present an example of how to use the ETS to sample discrete-time with competing events and right censoring data.</p> <p>A user-supplied covariates should be passed to ETS. For example, consider a setting with \\(n=10,000\\) independent observations and the following covariates</p> \\[ Z_1 \\sim  \\mbox{Bernoulli(0.5)}  \\] \\[ Z_2 | Z_1 \\sim \\mbox{Normal}(72 + 10 Z_1, 12) \\, , \\] <p>and</p> \\[ Z_3 \\sim  1+\\mbox{Poisson}(4) \\, . \\] <p>Any sampling framework can be used for creating the covariates' dataframe. For example:</p> <pre><code>import numpy as np\nimport pandas as pd\n\nn_observations = 10000\n\nobservations_df = pd.DataFrame(columns = ['Z1', 'Z2', 'Z3'])\nobservations_df['Z1'] = np.random.binomial(n = 1, p = 0.5, \n                                           size = n_observations)\nZ1_zero_index = observations_df.loc[observations_df['Z1'] == 0].index\nobservations_df.loc[Z1_zero_index, 'Z2'] = np.random.normal(loc = 72, scale = 12, \n                     size = n_observations - observations_df['Z1'].sum())\nZ1_one_index = observations_df.loc[observations_df['Z1'] == 1].index\nobservations_df.loc[Z1_one_index, 'Z2'] = np.random.normal(loc = 82, scale = 12, \n                     size = observations_df['Z1'].sum())\nobservations_df['Z3'] = 1 + np.random.poisson(lam = 4, size = n_observations)\n</code></pre> <pre><code>from pydts.examples_utils.plots import plot_sampled_covariates_figure \n\nplot_sampled_covariates_figure(observations_df, fname='tmp.png')\n</code></pre> <p>The ETS function assumes that the possible failure times are \\(1, \\ldots, d\\), and the user should supply the value of \\(d\\). Clearly, the time intervals can be irregularly spaced and variable in size. For instance, discrete-time categories 1, 2, and 3 could correspond to specific days like Tuesday, Thursday, and Friday-Sunday, respectively. In the current example, we chose \\(d=7\\). </p> <p>For the competing-events setting the user should decide on the number of competing events, and the values of model parameters, \\(\\alpha_{jt}\\), \\(\\beta_j\\), \\(t=1,\\ldots,d\\), \\(j=1,\\ldots,M\\). For example, consider \\(M=2\\) competing events and</p> \\[ \\alpha_{1t} =  -1.5 - 0.3 \\log t \\, , \\, t =1, \\ldots, 7  \\] \\[ \\alpha_{2t} =  -1.75 - 0.15 \\log t \\, , \\, t=1, \\ldots, 7 \\] \\[ \\beta^\\top_{1} =  (-0.8, -0.008, -0.017) \\] \\[ \\beta^\\top_{2} = (-1, -0.007, -0.02) \\, . \\] <p>All together, the ETS function is defined for sampling event-type and event-time, and adding it to the data frame, as follows:</p> <pre><code>from pydts.data_generation import EventTimesSampler\n\nets = EventTimesSampler(d_times=7, j_event_types=2)\ncoefficients_dict = {\n    \"alpha\": {\n        1: lambda t: -1.5 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t),\n    },\n    \"beta\": {\n        1: -1 * np.array([0.8, 0.008, 0.017]),\n        2: -1 * np.array([1, 0.007, 0.02]),\n}}\nobservations_df = ets.sample_event_times(observations_df, coefficients_dict)\nobservations_df.index.name = 'pid'\n</code></pre> <p>If the sampled covariates and parameters' values lead to impossible survival probabilities (i.e., negative or greater than one), the sampling process will be terminated with an error message. In such scenarios, it may be useful to adjust the coefficients or constrain extreme values of the covariates to ensure that the probabilities are appropriate and the sampling process is executed successfully.</p> <p>Two types of right censoring are implemented in PyDTS, administrative and random right censoring. For administrative censoring, \\(J_i=0\\) and \\(T_i = d + 1\\). These are the default values of observations for which the sampled event type was observed to be greater than \\(d\\). Random right censoring is optional and could be either dependent or independent of the covariates. For example, assume</p> \\[ \\Pr (C_i = t) = 0.02 \\quad  t=1, \\ldots, 7 \\, . \\] <p>The censoring times can be sampled by</p> <pre><code>prob_lof_at_t = [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]\nobservations_df = ets.sample_independent_lof_censoring(observations_df, \n                                                       prob_lof_at_t)\n</code></pre> <p>To generate right-censoring times that depend on the covariates, the user should supply to censoring hazard function, \\(\\lambda_c(t|Z)\\) in the form of the logit-link model. For example,</p> <pre><code>censoring_coef_dict = {\n    \"alpha\": {\n        0: lambda t: -0.3 - 0.3 * np.log(t),\n    },\n    \"beta\": {\n        0: -1 * np.array([0.8, 0.0001, 0.01]),\n}}\nobservations_df = ets.sample_hazard_lof_censoring(observations_df, \n        censoring_coef_dict)\n</code></pre> <p>Finally, the observed data should be updated by \\(X_i = min(T_i, C_i)\\) and \\(J_i\\) as follows</p> <pre><code>observations_df = ets.update_event_or_lof(observations_df)\n</code></pre> <p>The first observations of the sampled data are</p> <pre><code>observations_df.head()\n</code></pre> Z1 Z2 Z3 J T C X pid 0 1 71.561029 3 0 8 2 2 1 0 66.128415 4 0 6 1 1 2 1 107.018354 5 0 6 2 2 3 1 77.090988 6 0 2 1 1 4 1 89.397481 6 0 8 2 2 <p>and the total number of observations at each \\((J, X)\\) is </p> <pre><code>observations_df.reset_index().groupby(['J','X'])['pid'].count().to_frame()\n</code></pre> pid J X 0 1 2692 2 1373 3 810 4 550 5 326 6 253 7 223 8 891 1 1 740 2 371 3 163 4 106 5 73 6 45 7 34 2 1 584 2 335 3 176 4 109 5 70 6 43 7 33 <p>Note that with \\(d=7\\) possible event times, observations with \\(X=8\\) are only permitted when \\(J=0\\) (censoring). Observations with outcome \\((J, X) = (0, d+1)\\) remain in the risk set for the entire follow-up period, i.e., \\(t = 1, \\ldots, d\\), with no event observed.</p> <p>Additionally, the columns \\(C\\) and \\(T\\) are not observed in practice and are included solely for sampling purposes. The observed outcomes consist only of the columns \\(J\\) and \\(X\\).</p>"},{"location":"EventTimesSampler/#event-times-sampler","title":"Event Times Sampler","text":""},{"location":"EventTimesSampler/#covariates","title":"Covariates","text":""},{"location":"EventTimesSampler/#event-times","title":"Event Times","text":""},{"location":"EventTimesSampler/#censoring-time","title":"Censoring Time","text":""},{"location":"EventTimesSampler/#updating-the-observations","title":"Updating the Observations","text":""},{"location":"EventTimesSampler/#references","title":"References","text":"<p>[1] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software (2025), doi: 10.21105/joss.08815</p>"},{"location":"JOSS_mimic_example/","title":"MIMIC Example Summary","text":"<p>The utility of PyDTS is demonstrated through an analysis of patients' length of stay (LOS) in intensive care unit (ICU) [1]. This analysis uses the publicly accessible, large-scale Medical Information Mart for Intensive Care (MIMIC-IV, version 2.0) dataset. </p> <p>Meir and Gorfine (2025) [1] developed a discrete-time survival model to predict ICU LOS based on patients\u2019 clinical characteristics at admission. The dataset comprises 25,170 ICU patients. For each patient, only the last admission is considered, and features related to prior admission history are included. The LOS is recorded in discrete units from 1 to 28 days, resulting in many patients sharing the same event time on each day. </p> <p>Three competing events are considered: </p> <ol> <li> <p>Discharge to home (69.0%),</p> </li> <li> <p>Transfer to another medical facility (21.4%)</p> </li> <li> <p>In-hospital death (6.1%).</p> </li> </ol> <p>Patients who left the ICU against medical advice (1.0%) are treated as right-censored, and administrative censoring is applied to those hospitalized for more than 28 days (2.5%). </p> <p>The analysis includes 36 covariates per patient, comprising patient characteristics and laboratory test results at admission.  Full description of the data is presented below.</p> <p>The preprocessing procedure of [1,2] is implemented in <code>pydts.example_utils.get_mimic_df()</code>. </p> <p>Note that the MIMIC-IV dataset itself is not included in PyDTS; it is available at https://physionet.org/content/mimiciv/2.0/ and requires credentialed access.</p> <pre><code>from pydts.examples_utils.datasets import get_mimic_df\nfrom pydts.examples_utils.mimic_consts import *\n\nMIMIC_IV_v2_FILES_LOCATION = \"/data/mimic-iv-2.0\"\n\nmimic_df, characteristics_table1, labs_table1 = get_mimic_df(MIMIC_IV_v2_FILES_LOCATION, return_table1=True)\n</code></pre> <pre><code>characteristics_table1\n</code></pre> Grouped by Discharge location Overall Censored Died Further Treatment Home n 25170 894 1540 5379 17357 Sex, n (%) Female 12291 (48.8) 373 (41.7) 695 (45.1) 2865 (53.3) 8358 (48.2) Male 12879 (51.2) 521 (58.3) 845 (54.9) 2514 (46.7) 8999 (51.8) Admission age, mean (SD) 64.1 (17.9) 58.4 (16.5) 72.7 (14.5) 73.3 (15.7) 60.8 (17.6) Race, n (%) Asian 1035 (4.1) 27 (3.0) 76 (4.9) 165 (3.1) 767 (4.4) Black 3543 (14.1) 154 (17.2) 197 (12.8) 741 (13.8) 2451 (14.1) Hispanic 1326 (5.3) 53 (5.9) 53 (3.4) 180 (3.3) 1040 (6.0) Other 1671 (6.6) 65 (7.3) 142 (9.2) 316 (5.9) 1148 (6.6) White 17595 (69.9) 595 (66.6) 1072 (69.6) 3977 (73.9) 11951 (68.9) Insurance, n (%) Medicaid 1423 (5.7) 86 (9.6) 66 (4.3) 222 (4.1) 1049 (6.0) Medicare 10609 (42.1) 316 (35.3) 843 (54.7) 3253 (60.5) 6197 (35.7) Other 13138 (52.2) 492 (55.0) 631 (41.0) 1904 (35.4) 10111 (58.3) Marital status, n (%) Divorced 2043 (8.1) 94 (10.5) 121 (7.9) 464 (8.6) 1364 (7.9) Married 11289 (44.9) 329 (36.8) 751 (48.8) 1853 (34.4) 8356 (48.1) Single 8414 (33.4) 403 (45.1) 386 (25.1) 1729 (32.1) 5896 (34.0) Widowed 3424 (13.6) 68 (7.6) 282 (18.3) 1333 (24.8) 1741 (10.0) Direct emergency, n (%) No 22398 (89.0) 790 (88.4) 1413 (91.8) 4924 (91.5) 15271 (88.0) Yes 2772 (11.0) 104 (11.6) 127 (8.2) 455 (8.5) 2086 (12.0) Night admission, n (%) No 11604 (46.1) 404 (45.2) 736 (47.8) 2414 (44.9) 8050 (46.4) Yes 13566 (53.9) 490 (54.8) 804 (52.2) 2965 (55.1) 9307 (53.6) Previous admission this month, n (%) No 23138 (91.9) 795 (88.9) 1318 (85.6) 4821 (89.6) 16204 (93.4) Yes 2032 (8.1) 99 (11.1) 222 (14.4) 558 (10.4) 1153 (6.6) Admissions number, n (%) 1 15471 (61.5) 503 (56.3) 798 (51.8) 3005 (55.9) 11165 (64.3) 2 4121 (16.4) 151 (16.9) 283 (18.4) 926 (17.2) 2761 (15.9) 3+ 5578 (22.2) 240 (26.8) 459 (29.8) 1448 (26.9) 3431 (19.8) LOS (days), mean (SD) 7.0 (6.1) 21.7 (11.6) 8.5 (6.9) 9.0 (5.8) 5.5 (4.3) Discharge location, n (%) Censored 894 (3.6) 894 (100.0) 0 (0.0) 0 (0.0) 0 (0.0) Died 1540 (6.1) 0 (0.0) 1540 (100.0) 0 (0.0) 0 (0.0) Further Treatment 5379 (21.4) 0 (0.0) 0 (0.0) 5379 (100.0) 0 (0.0) Home 17357 (69.0) 0 (0.0) 0 (0.0) 0 (0.0) 17357 (100.0) <pre><code>labs_table1\n</code></pre> Grouped by Discharge location Overall Censored Died Further Treatment Home n 25170 894 1540 5379 17357 Discharge location, n (%) Censored 894 (3.6) 894 (100.0) 0 (0.0) 0 (0.0) 0 (0.0) Died 1540 (6.1) 0 (0.0) 1540 (100.0) 0 (0.0) 0 (0.0) Further Treatment 5379 (21.4) 0 (0.0) 0 (0.0) 5379 (100.0) 0 (0.0) Home 17357 (69.0) 0 (0.0) 0 (0.0) 0 (0.0) 17357 (100.0) Anion gap, n (%) Abnormal 2305 (9.2) 110 (12.3) 401 (26.0) 543 (10.1) 1251 (7.2) Normal 22865 (90.8) 784 (87.7) 1139 (74.0) 4836 (89.9) 16106 (92.8) Bicarbonate, n (%) Abnormal 6135 (24.4) 300 (33.6) 832 (54.0) 1494 (27.8) 3509 (20.2) Normal 19035 (75.6) 594 (66.4) 708 (46.0) 3885 (72.2) 13848 (79.8) Calcium total, n (%) Abnormal 7326 (29.1) 365 (40.8) 756 (49.1) 1823 (33.9) 4382 (25.2) Normal 17844 (70.9) 529 (59.2) 784 (50.9) 3556 (66.1) 12975 (74.8) Chloride, n (%) Abnormal 4848 (19.3) 255 (28.5) 555 (36.0) 1322 (24.6) 2716 (15.6) Normal 20322 (80.7) 639 (71.5) 985 (64.0) 4057 (75.4) 14641 (84.4) Creatinine, n (%) Abnormal 7124 (28.3) 323 (36.1) 893 (58.0) 1945 (36.2) 3963 (22.8) Normal 18046 (71.7) 571 (63.9) 647 (42.0) 3434 (63.8) 13394 (77.2) Glucose, n (%) Abnormal 16426 (65.3) 635 (71.0) 1211 (78.6) 3674 (68.3) 10906 (62.8) Normal 8744 (34.7) 259 (29.0) 329 (21.4) 1705 (31.7) 6451 (37.2) Magnesium, n (%) Abnormal 2220 (8.8) 99 (11.1) 234 (15.2) 517 (9.6) 1370 (7.9) Normal 22950 (91.2) 795 (88.9) 1306 (84.8) 4862 (90.4) 15987 (92.1) Phosphate, n (%) Abnormal 6962 (27.7) 313 (35.0) 663 (43.1) 1510 (28.1) 4476 (25.8) Normal 18208 (72.3) 581 (65.0) 877 (56.9) 3869 (71.9) 12881 (74.2) Potassium, n (%) Abnormal 2109 (8.4) 110 (12.3) 260 (16.9) 520 (9.7) 1219 (7.0) Normal 23061 (91.6) 784 (87.7) 1280 (83.1) 4859 (90.3) 16138 (93.0) Sodium, n (%) Abnormal 2947 (11.7) 171 (19.1) 415 (26.9) 845 (15.7) 1516 (8.7) Normal 22223 (88.3) 723 (80.9) 1125 (73.1) 4534 (84.3) 15841 (91.3) Urea nitrogen, n (%) Abnormal 10032 (39.9) 413 (46.2) 1059 (68.8) 2849 (53.0) 5711 (32.9) Normal 15138 (60.1) 481 (53.8) 481 (31.2) 2530 (47.0) 11646 (67.1) Hematocrit, n (%) Abnormal 17319 (68.8) 691 (77.3) 1250 (81.2) 4111 (76.4) 11267 (64.9) Normal 7851 (31.2) 203 (22.7) 290 (18.8) 1268 (23.6) 6090 (35.1) Hemoglobin, n (%) Abnormal 18355 (72.9) 735 (82.2) 1319 (85.6) 4320 (80.3) 11981 (69.0) Normal 6815 (27.1) 159 (17.8) 221 (14.4) 1059 (19.7) 5376 (31.0) MCH, n (%) Abnormal 6559 (26.1) 306 (34.2) 454 (29.5) 1488 (27.7) 4311 (24.8) Normal 18611 (73.9) 588 (65.8) 1086 (70.5) 3891 (72.3) 13046 (75.2) MCHC, n (%) Abnormal 7762 (30.8) 313 (35.0) 634 (41.2) 2033 (37.8) 4782 (27.6) Normal 17408 (69.2) 581 (65.0) 906 (58.8) 3346 (62.2) 12575 (72.4) MCV, n (%) Abnormal 5106 (20.3) 243 (27.2) 418 (27.1) 1229 (22.8) 3216 (18.5) Normal 20064 (79.7) 651 (72.8) 1122 (72.9) 4150 (77.2) 14141 (81.5) Platelet count, n (%) Abnormal 7280 (28.9) 364 (40.7) 688 (44.7) 1618 (30.1) 4610 (26.6) Normal 17890 (71.1) 530 (59.3) 852 (55.3) 3761 (69.9) 12747 (73.4) RDW, n (%) Abnormal 7280 (28.9) 377 (42.2) 870 (56.5) 2016 (37.5) 4017 (23.1) Normal 17890 (71.1) 517 (57.8) 670 (43.5) 3363 (62.5) 13340 (76.9) Red blood cells, n (%) Abnormal 19170 (76.2) 732 (81.9) 1341 (87.1) 4478 (83.2) 12619 (72.7) Normal 6000 (23.8) 162 (18.1) 199 (12.9) 901 (16.8) 4738 (27.3) White blood cells, n (%) Abnormal 10013 (39.8) 466 (52.1) 1012 (65.7) 2320 (43.1) 6215 (35.8) Normal 15157 (60.2) 428 (47.9) 528 (34.3) 3059 (56.9) 11142 (64.2) <p>Three estimation procedures were compared: </p> <ol> <li>The method of Lee et al. (2018) [3] without regularization</li> <li>The two-step approach of Meir and Gorfine (2025) [2] without regularization.</li> <li>The two-step approach of Meir and Gorfine (2025) [2] with LASSO regularization.</li> </ol> <p>When applying the two-step procedure with LASSO regularization, we need to specify the hyperparameters that control the amount of regularization applied to each competing event. As demonstrated in the regularization section of this documentation, PyDTS provides functionality for tuning these hyperparameters via K-fold cross-validation. By default, the optimal values are those that maximize the out-of-sample global-AUC metric, as defined in Meir and Gorfine (2025), Appendix I. Additional tuning options are also available. Here, a grid search with 4-fold cross-validation was performed to select the optimal hyperparameters that maximize the global-AUC. The code below illustrates such tuning procedure.</p> <pre><code>import numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pydts.cross_validation import PenaltyGridSearchCV\nimport pandas as pd\n\nstep = 1\npenalizers = np.arange(-12, -0.9, step=step)\nn_splits = 4\nseed = 1\n\npenalty_cv_search = PenaltyGridSearchCV()\ngauc_cv_results = penalty_cv_search.cross_validate(full_df=mimic_df, l1_ratio=1, penalizers=np.exp(penalizers),\n                                                   n_splits=n_splits, seed=seed)\nprint(gauc_cv_results['Mean'].max())\nprint(gauc_cv_results['Mean'].idxmax())\nchosen_eta = np.log(gauc_cv_results['Mean'].idxmax())\nprint(chosen_eta)\n</code></pre> <pre>\n<code>0.6509085412936966\n(np.float64(0.006737946999085467), np.float64(0.00012340980408667956), np.float64(1.670170079024566e-05))\n[ -5.  -9. -11.]\n</code>\n</pre> <p>The procedure returns a <code>pd.DataFrame</code> with the penalizers combination as the index and the mean and standard deviation of the global-AUC across folds as the values.</p> <pre><code>gauc_cv_results\n</code></pre> Mean SE 0.000006 0.000006 0.000006 0.649354 0.003654 0.000017 0.649362 0.003654 0.000045 0.649366 0.003650 0.000123 0.649284 0.003640 0.000335 0.649084 0.003556 ... ... ... ... ... 0.367879 0.367879 0.006738 0.632751 0.001759 0.018316 0.632656 0.001746 0.049787 0.632863 0.001756 0.135335 0.633117 0.001749 0.367879 0.633375 0.001758 <p>1728 rows \u00d7 2 columns</p> <p>The chosen penalizaers \\(\\eta_j\\), \\(j=1,2,3\\), are the ones that maximize the global-AUC, thus,</p> <pre><code>print(f\"Maximum global-AUC (mean across folds): {gauc_cv_results['Mean'].max()}\")\nchosen_eta = np.log(gauc_cv_results['Mean'].idxmax())\nprint(f\"Chosen penalizers: exp(\", chosen_eta, \")\")\n</code></pre> <pre>\n<code>Maximum global-AUC (mean across folds): 0.6509085412936966\nChosen penalizers: exp( [ -5.  -9. -11.] )\n</code>\n</pre> <p>Additional metrics are also available - for example, the integrated AUC in each fold for each risk is included in <code>penalty_cv_search.integrated_auc</code></p> <p>We now train and compare the three estimation methods, using the selected penalizers for the regularized two-step procedure:</p> <pre><code>from pydts.fitters import DataExpansionFitter\n\nlee_fitter = DataExpansionFitter()\nlee_fitter.fit(df=mimic_df)\n</code></pre> <pre><code>from pydts.fitters import TwoStagesFitter\n\nnew_fitter = TwoStagesFitter()\nnew_fitter.fit(df=mimic_df)\n</code></pre> <pre><code>reg_fitter = TwoStagesFitter()\nfit_beta_kwargs = {\n        'model_kwargs': { \n            1: {'penalizer': np.exp(chosen_eta[0]), 'l1_ratio': 1},\n            2: {'penalizer': np.exp(chosen_eta[1]), 'l1_ratio': 1},\n            3: {'penalizer': np.exp(chosen_eta[2]), 'l1_ratio': 1},            \n    }\n}\n\nreg_fitter.fit(df=mimic_df, fit_beta_kwargs=fit_beta_kwargs)\n</code></pre> <pre><code>from pydts.examples_utils.plots import create_mimic_analysis_results\n\nrisk1, risk2, risk3 = create_mimic_analysis_results(mimic_df, n_splits, penalizers, penalty_cv_search, \n                                                    chosen_eta, lee_fitter, new_fitter, reg_fitter)\n</code></pre> <pre>\n<code>Risk 1: 16.25 non-zero coefficients at chosen eta -5.0\nRisk 2: 32.75 non-zero coefficients at chosen eta -9.0\nRisk 3: 35.25 non-zero coefficients at chosen eta -11.0\n</code>\n</pre> <pre><code>risk1\n</code></pre> Lee et al. Two-Step Two-Step &amp; LASSO Estimate (SE) Estimate (SE) Estimate (SE) Admissions number 2 2 0.000 (0.024) 0.003 (0.022) -0.000 (0.000) Admissions number 3+ 3+ -0.032 (0.023) -0.027 (0.022) -0.000 (0.000) Anion gap Abnormal -0.137 (0.032) -0.128 (0.030) -0.000 (0.000) Bicarbonate Abnormal -0.208 (0.021) -0.194 (0.020) -0.119 (0.019) Calcium total Abnormal -0.291 (0.020) -0.270 (0.019) -0.190 (0.018) Chloride Abnormal -0.148 (0.024) -0.137 (0.023) -0.071 (0.021) Creatinine Abnormal -0.103 (0.024) -0.098 (0.023) -0.072 (0.021) Direct emergency Yes -0.011 (0.026) -0.014 (0.024) -0.000 (0.000) Ethnicity black Black 0.006 (0.046) 0.009 (0.042) -0.000 (0.000) Ethnicity hispanic Hispanic 0.132 (0.053) 0.120 (0.048) 0.000 (0.000) Ethnicity other Other -0.162 (0.051) -0.146 (0.047) -0.000 (0.000) Ethnicity white White -0.031 (0.041) -0.026 (0.038) -0.000 (0.000) Glucose Abnormal -0.215 (0.018) -0.192 (0.016) -0.088 (0.016) Hematocrit Abnormal -0.042 (0.032) -0.037 (0.029) -0.042 (0.029) Hemoglobin Abnormal -0.080 (0.033) -0.071 (0.030) -0.081 (0.030) Insurance medicare Medicare 0.138 (0.039) 0.125 (0.036) -0.000 (0.000) Insurance other Other 0.219 (0.036) 0.200 (0.033) 0.030 (0.016) MCH Abnormal -0.002 (0.023) -0.002 (0.022) -0.000 (0.000) MCHC Abnormal -0.128 (0.019) -0.116 (0.018) -0.003 (0.017) MCV Abnormal -0.048 (0.026) -0.045 (0.024) -0.000 (0.000) Magnesium Abnormal -0.080 (0.030) -0.074 (0.028) -0.000 (0.000) Marital married Married 0.224 (0.032) 0.205 (0.030) 0.093 (0.016) Marital single Single -0.087 (0.033) -0.079 (0.031) -0.000 (0.000) Marital widowed Widowed 0.026 (0.040) 0.020 (0.037) -0.000 (0.000) Night admission Yes 0.081 (0.017) 0.075 (0.016) 0.000 (0.000) Phosphate Abnormal -0.052 (0.019) -0.048 (0.018) -0.000 (0.000) Platelet count Abnormal -0.068 (0.019) -0.062 (0.018) -0.000 (0.000) Potassium Abnormal -0.103 (0.032) -0.095 (0.030) -0.000 (0.000) RDW Abnormal -0.327 (0.021) -0.308 (0.020) -0.271 (0.019) Recent admission Yes -0.262 (0.035) -0.247 (0.033) -0.001 (0.027) Red blood cells Abnormal -0.089 (0.027) -0.078 (0.024) -0.024 (0.025) Sex Female -0.007 (0.018) -0.006 (0.016) -0.000 (0.000) Sodium Abnormal -0.312 (0.030) -0.297 (0.029) -0.142 (0.026) Standardized age -0.260 (0.011) -0.234 (0.010) -0.162 (0.009) Urea nitrogen Abnormal -0.148 (0.022) -0.139 (0.020) -0.136 (0.020) White blood cells Abnormal -0.276 (0.018) -0.252 (0.016) -0.159 (0.016) <pre><code>risk2\n</code></pre> Lee et al. Two-Step Two-Step &amp; LASSO Estimate (SE) Estimate (SE) Estimate (SE) Admissions number 2 2 0.108 (0.041) 0.107 (0.040) 0.087 (0.038) Admissions number 3+ 3+ 0.194 (0.037) 0.190 (0.036) 0.169 (0.034) Anion gap Abnormal -0.006 (0.048) -0.006 (0.047) -0.000 (0.002) Bicarbonate Abnormal -0.121 (0.033) -0.117 (0.032) -0.110 (0.032) Calcium total Abnormal -0.098 (0.031) -0.094 (0.031) -0.088 (0.030) Chloride Abnormal 0.016 (0.036) 0.015 (0.035) 0.000 (0.002) Creatinine Abnormal -0.199 (0.036) -0.191 (0.035) -0.173 (0.035) Direct emergency Yes -0.373 (0.052) -0.363 (0.050) -0.345 (0.050) Ethnicity black Black 0.084 (0.090) 0.079 (0.088) 0.028 (0.086) Ethnicity hispanic Hispanic -0.068 (0.111) -0.070 (0.108) -0.088 (0.106) Ethnicity other Other 0.026 (0.099) 0.022 (0.097) -0.006 (0.095) Ethnicity white White 0.144 (0.082) 0.138 (0.081) 0.094 (0.079) Glucose Abnormal -0.138 (0.031) -0.132 (0.030) -0.126 (0.030) Hematocrit Abnormal 0.038 (0.057) 0.039 (0.055) 0.032 (0.055) Hemoglobin Abnormal 0.018 (0.062) 0.015 (0.060) 0.005 (0.059) Insurance medicare Medicare 0.237 (0.075) 0.230 (0.074) 0.238 (0.073) Insurance other Other -0.094 (0.074) -0.091 (0.072) -0.081 (0.072) MCH Abnormal 0.042 (0.038) 0.040 (0.037) 0.019 (0.031) MCHC Abnormal -0.010 (0.031) -0.011 (0.030) -0.000 (0.003) MCV Abnormal -0.020 (0.041) -0.019 (0.039) -0.000 (0.003) Magnesium Abnormal -0.039 (0.048) -0.038 (0.047) -0.025 (0.046) Marital married Married -0.254 (0.054) -0.249 (0.053) -0.262 (0.052) Marital single Single 0.209 (0.054) 0.200 (0.053) 0.176 (0.052) Marital widowed Widowed 0.175 (0.058) 0.163 (0.056) 0.149 (0.056) Night admission Yes 0.056 (0.029) 0.054 (0.028) 0.047 (0.028) Phosphate Abnormal -0.042 (0.033) -0.040 (0.032) -0.034 (0.031) Platelet count Abnormal -0.130 (0.032) -0.125 (0.031) -0.118 (0.031) Potassium Abnormal 0.042 (0.048) 0.042 (0.047) 0.023 (0.047) RDW Abnormal -0.107 (0.033) -0.104 (0.032) -0.093 (0.031) Recent admission Yes -0.021 (0.051) -0.023 (0.049) -0.000 (0.004) Red blood cells Abnormal 0.083 (0.052) 0.079 (0.050) 0.073 (0.050) Sex Female 0.090 (0.031) 0.088 (0.030) 0.078 (0.030) Sodium Abnormal -0.056 (0.042) -0.056 (0.041) -0.039 (0.038) Standardized age 0.536 (0.021) 0.525 (0.021) 0.519 (0.021) Urea nitrogen Abnormal 0.100 (0.035) 0.095 (0.034) 0.077 (0.034) White blood cells Abnormal -0.107 (0.029) -0.103 (0.028) -0.099 (0.028) <pre><code>risk3\n</code></pre> Lee et al. Two-Step Two-Step &amp; LASSO Estimate (SE) Estimate (SE) Estimate (SE) Admissions number 2 2 0.147 (0.074) 0.147 (0.073) 0.140 (0.074) Admissions number 3+ 3+ 0.142 (0.069) 0.140 (0.068) 0.134 (0.068) Anion gap Abnormal 0.582 (0.064) 0.573 (0.064) 0.571 (0.064) Bicarbonate Abnormal 0.543 (0.056) 0.537 (0.056) 0.535 (0.056) Calcium total Abnormal 0.204 (0.054) 0.204 (0.054) 0.203 (0.054) Chloride Abnormal 0.147 (0.059) 0.143 (0.058) 0.142 (0.058) Creatinine Abnormal 0.273 (0.067) 0.271 (0.067) 0.271 (0.067) Direct emergency Yes -0.318 (0.096) -0.311 (0.095) -0.302 (0.095) Ethnicity black Black -0.236 (0.140) -0.235 (0.139) -0.203 (0.140) Ethnicity hispanic Hispanic -0.395 (0.183) -0.393 (0.181) -0.351 (0.181) Ethnicity other Other 0.145 (0.147) 0.133 (0.145) 0.155 (0.146) Ethnicity white White -0.156 (0.123) -0.157 (0.122) -0.130 (0.123) Glucose Abnormal 0.215 (0.064) 0.212 (0.063) 0.208 (0.063) Hematocrit Abnormal -0.198 (0.108) -0.194 (0.107) -0.165 (0.108) Hemoglobin Abnormal 0.024 (0.122) 0.023 (0.121) 0.003 (0.121) Insurance medicare Medicare -0.224 (0.136) -0.225 (0.135) -0.171 (0.138) Insurance other Other -0.242 (0.133) -0.240 (0.132) -0.188 (0.135) MCH Abnormal -0.066 (0.070) -0.066 (0.069) -0.057 (0.069) MCHC Abnormal 0.027 (0.056) 0.029 (0.055) 0.027 (0.055) MCV Abnormal 0.060 (0.072) 0.061 (0.071) 0.055 (0.071) Magnesium Abnormal 0.329 (0.073) 0.324 (0.072) 0.320 (0.072) Marital married Married 0.156 (0.102) 0.154 (0.101) 0.127 (0.061) Marital single Single 0.026 (0.107) 0.027 (0.106) 0.000 (0.008) Marital widowed Widowed 0.047 (0.115) 0.048 (0.114) 0.020 (0.084) Night admission Yes -0.096 (0.053) -0.093 (0.052) -0.089 (0.052) Phosphate Abnormal 0.178 (0.056) 0.176 (0.055) 0.174 (0.055) Platelet count Abnormal 0.235 (0.054) 0.232 (0.054) 0.229 (0.054) Potassium Abnormal 0.227 (0.072) 0.221 (0.071) 0.221 (0.071) RDW Abnormal 0.492 (0.058) 0.486 (0.058) 0.483 (0.058) Recent admission Yes 0.250 (0.083) 0.242 (0.082) 0.242 (0.082) Red blood cells Abnormal 0.142 (0.105) 0.140 (0.104) 0.130 (0.104) Sex Female -0.011 (0.057) -0.008 (0.057) -0.005 (0.057) Sodium Abnormal 0.276 (0.064) 0.270 (0.063) 0.268 (0.063) Standardized age 0.580 (0.041) 0.574 (0.040) 0.568 (0.040) Urea nitrogen Abnormal 0.141 (0.070) 0.141 (0.070) 0.141 (0.070) White blood cells Abnormal 0.579 (0.056) 0.571 (0.056) 0.568 (0.055) <p>[1] Meir, Tomer and Gorfine, Malka, \"Discrete-time Competing-Risks Regression with or without Penalization\", Biometrics, Volume 81, Issue 2, 2025.</p> <p>[2] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software, 2025, doi: 10.21105/joss.08815 </p> <p>[3] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics, 2018.</p>"},{"location":"JOSS_mimic_example/#mimic-iv-20-analysis-using-pydts-meir-and-gorfine-2025","title":"MIMIC-IV 2.0 analysis using PyDTS (Meir and Gorfine, 2025)","text":""},{"location":"JOSS_mimic_example/#preprocessed-mimic-iv-v20-dataset","title":"Preprocessed MIMIC-IV v2.0 Dataset","text":""},{"location":"JOSS_mimic_example/#model-estimation","title":"Model Estimation","text":""},{"location":"JOSS_mimic_example/#estimation-using-lee-et-al-2018","title":"Estimation using Lee et al. (2018)","text":""},{"location":"JOSS_mimic_example/#estimation-using-two-step-without-regularization","title":"Estimation using two-step without regularization","text":""},{"location":"JOSS_mimic_example/#estimation-using-two-step-with-regularization","title":"Estimation using two-step with regularization","text":""},{"location":"JOSS_mimic_example/#references","title":"References","text":""},{"location":"ModelsComparison/","title":"Comparing the Estimation Methods","text":"<p>We conducted a simulation study demonstrating the performances of Meir et al. (2022) [1] and comparing it with that of Lee et al. (2018) [2]. The PyDTS implementation was used for both methods. </p> <p>The data was generated using the <code>EventTimesSampler</code> with \\(M=2\\) competing events, \\(n=20,000\\) observations, \\(Z\\) with 10 covariates and right censoring. </p> <p>Failure times were generated based on </p> \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] <p>with </p> <p>\\(\\alpha_{1t} = -2.5 - 0.3 \\log(t)\\), </p> <p>\\(\\alpha_{2t} = -2.8 - 0.3 \\log(t)\\), \\(t=1,\\ldots,d\\),</p> <p>\\(\\beta_1 = -0.5*(\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 4, \\log 1, \\log 3, \\log 2, \\log 2, \\log 3)\\), </p> <p>\\(\\beta_{2} = -0.5*(\\log 1, \\log 3, \\log 2, \\log 1, \\log 4, \\log 3, \\log 4, \\log 3, \\log 3, \\log 4)\\). </p> <p>Censoring time for each observation was sampled from a discrete distribution with \\(P(C_i = t) = \\frac{0.1}{d}\\) for \\(t=1,\\ldots,d\\), and \\(P(C_i = d+1) = 0.9\\). As before, \\(X=\\min(C,T)\\) and \\(J\\) is the event type with \\(J_i=0\\) (censoring) if and only if \\(C_i &lt; T_i\\).</p> <p>We repeated this procedure for \\(d \\in \\{ 10, 20, 30, 40, 50 \\}\\). For each value of \\(d\\), the results are based on 10 replications. </p> <p>We show that the computational running time of our approach is shorter depending on \\(d\\), where the improvement factor increases as a function of \\(d\\).</p> <pre><code>import pandas as pd\nimport numpy as np\nimport os\nfrom time import time\nfrom pydts.fitters import TwoStagesFitter, DataExpansionFitter\nfrom pydts.data_generation import EventTimesSampler\nfrom matplotlib import pyplot as plt\npd.set_option(\"display.max_rows\", 500)\n%matplotlib inline\nslicer = pd.IndexSlice\n</code></pre> <pre><code>OUTPUT_DIR = ''\n</code></pre> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -2.5 - 0.3 * np.log(t),\n        2: lambda t: -2.8 - 0.3 * np.log(t)\n    },\n    \"beta\": {\n        1: -0.5*np.log([0.8, 3, 3, 2.5, 4, 1, 3, 2, 2, 3]),\n        2: -0.5*np.log([1, 3, 2, 1, 4, 3, 4, 3, 3, 2])\n    }\n}\n\nn_patients = 20000\nn_cov = 10\nj_events = 2\n\nd_times = 50\nets = EventTimesSampler(d_times=d_times, j_event_types=j_events)\n\nseed = 0\n\ncovariates = [f'Z{i}' for i in range(n_cov)]\n\nCOEF_COL = '   coef   '\nSTDERR_COL = ' std err '\n</code></pre> <pre><code>patients_df = pd.DataFrame(data=pd.DataFrame(data=np.random.uniform(0,1, size=[n_patients, n_cov]),\n                           columns=covariates))\n\npatients_df = ets.sample_event_times(patients_df, hazard_coefs=real_coef_dict, seed=seed)\npatients_df = ets.sample_independent_lof_censoring(patients_df, prob_lof_at_t=(0.1 / d_times) * np.ones_like(ets.times[:-1]))\npatients_df = ets.update_event_or_lof(patients_df)\n\npatients_df.index.name='pid'                 \npatients_df = patients_df.reset_index()\n\nfrom pydts.examples_utils.plots import plot_events_occurrence\nplot_events_occurrence(patients_df[patients_df['X'] != (d_times+1)])\n#plot_events_occurrence(patients_df)\n</code></pre> <pre><code>k_runs = 10\nd_times_list = [10, 20, 30, 40, 50]\n</code></pre> <pre><code>final_two_step = {}\nfinal_lee = {}\n\nfor idp, d_times in enumerate(d_times_list):\n\n    case = f'timing_d{d_times}_final_'\n    two_step_timing = []\n    lee_timing = []\n    print(f\"Starting d={d_times} times\")\n    #print('**************************************')\n    for k in range(k_runs):\n        try:\n            # Sampling based on different seed each time\n            loop_seed = 1000*idp+k+seed\n            print(f'Sampling Patients, loop seed: {loop_seed}')\n            np.random.seed(loop_seed)\n            ets = EventTimesSampler(d_times=d_times, j_event_types=j_events)\n            patients_df = pd.DataFrame(data=pd.DataFrame(data=np.random.uniform(0,1, size=[n_patients, n_cov]),\n                                       columns=covariates))\n\n            patients_df = ets.sample_event_times(patients_df, hazard_coefs=real_coef_dict, seed=loop_seed)\n            patients_df = ets.sample_independent_lof_censoring(patients_df, prob_lof_at_t=(0.1 / d_times) * np.ones_like(ets.times[:-1]))\n            patients_df = ets.update_event_or_lof(patients_df)\n            patients_df.index.name='pid'\n            patients_df = patients_df.reset_index()\n\n            # Two step fitter\n            new_fitter = TwoStagesFitter()\n            print(f'Starting two-step: {k+1}/{k_runs}')\n            two_step_start = time()\n            new_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), nb_workers=1)\n            two_step_end = time()\n            print(f'Finished two-step: {k+1}/{k_runs}, {two_step_end-two_step_start}sec')\n\n            two_step_timing.append(two_step_end-two_step_start)\n\n            # Lee et al fitter\n            lee_fitter = DataExpansionFitter()\n            print(f'Starting Lee: {k+1}/{k_runs}')\n            lee_start = time()\n            lee_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\n            lee_end = time()\n            print(f'Finished lee: {k+1}/{k_runs}, {lee_end-lee_start}sec')\n\n            lee_timing.append(lee_end-lee_start) \n\n            lee_alpha_ser = lee_fitter.get_alpha_df().loc[:, slicer[:, [COEF_COL, STDERR_COL] ]].unstack().sort_index()\n            lee_beta_ser = lee_fitter.get_beta_SE().loc[:, slicer[:, [COEF_COL, STDERR_COL] ]].unstack().sort_index()\n\n            if k == 0:\n                two_step_alpha_k_results = new_fitter.alpha_df[['J', 'X', 'alpha_jt']]\n                two_step_beta_k_results = new_fitter.get_beta_SE().unstack().to_frame()\n\n                lee_alpha_k_results = lee_alpha_ser.to_frame()\n                lee_beta_k_results = lee_beta_ser.to_frame()\n\n            else:\n                two_step_alpha_k_results = pd.concat([two_step_alpha_k_results, new_fitter.alpha_df['alpha_jt']], axis=1)\n                two_step_beta_k_results = pd.concat([two_step_beta_k_results, new_fitter.get_beta_SE().unstack()], axis=1)\n\n                lee_alpha_k_results = pd.concat([lee_alpha_k_results, lee_alpha_ser], axis=1)\n                lee_beta_k_results = pd.concat([lee_beta_k_results, lee_beta_ser], axis=1)\n\n\n        except Exception as e:\n            print(f'Failed during trial {k}')\n            print(e)\n\n\n    two_step_alpha_k_results = two_step_alpha_k_results.set_index(['J', 'X'])\n    two_step_alpha_k_results.columns = list(range(1, 1+k_runs))\n    two_step_beta_k_results.columns = list(range(1, 1+k_runs))\n    lee_alpha_k_results.columns = list(range(1, 1+k_runs))\n    lee_beta_k_results.columns = list(range(1, 1+k_runs))\n\n    final_two_step[d_times] = two_step_timing\n    final_lee[d_times] = lee_timing\n</code></pre> <pre><code>summary_df = pd.DataFrame(index=d_times_list, columns=['Lee mean', 'Lee std', \n                                                       'two-step mean', 'two-step std'])\nlee_results_df = pd.DataFrame(columns=d_times_list, index=range(1,k_runs+1))\ntwo_step_results_df = pd.DataFrame(columns=d_times_list, index=range(1,k_runs+1))\n\nfor idk, k in enumerate(d_times_list):\n    summary_df.loc[k, 'Lee mean'] = np.mean(final_lee[k])\n    summary_df.loc[k, 'Lee std'] = np.std(final_lee[k])\n    summary_df.loc[k, 'two-step mean'] = np.mean(final_two_step[k])\n    summary_df.loc[k, 'two-step std'] = np.std(final_two_step[k])\n\n    lee_results_df.loc[:, k] = final_lee[k]\n    two_step_results_df.loc[:, k] = final_two_step[k]\n\nsummary_df['ratio'] = summary_df['Lee mean'] / summary_df['two-step mean']\nsummary_df\n</code></pre> Lee mean Lee std two-step mean two-step std ratio 10 29.150699 0.209674 8.240145 0.072949 3.537644 20 76.014074 0.434573 14.993356 0.044223 5.06985 30 140.902811 0.60596 21.326584 0.076011 6.60691 40 226.264236 1.270814 27.312895 0.118994 8.284154 50 328.705489 1.136085 33.004308 0.144304 9.959472 <pre><code>filename = 'fitting_time_comparison.png'\n\nfig, ax = plt.subplots(1, 1, figsize=(6,4))\nax.set_title('n=20000, p=10', fontsize=16)\nflierprops = dict(marker='.', markersize=4)\nlee_boxprops = dict(color='darkgreen')\nlee_medianprops = dict(color='darkgreen')\ntwo_step_boxprops = dict(color='navy')\ntwo_step_medianprops = dict(color='navy')\n\nax.boxplot(lee_results_df, vert=True, positions=lee_results_df.columns, whis=1.5, flierprops=flierprops,\n           widths=8, boxprops=lee_boxprops, medianprops=lee_medianprops)\nax.boxplot(two_step_results_df, vert=True, positions=two_step_results_df.columns, whis=1.5, flierprops=flierprops,\n           widths=8, boxprops=two_step_boxprops, medianprops=two_step_medianprops)\nax.set_xlabel('Number of Discrete Times', fontsize=16)\nax.set_ylabel('Fitting Time [seconds]', fontsize=16)\nax.set_xticks(d_times_list)\nax.set_xticklabels(d_times_list)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\nax.set_xlim([5,55])\nax.set_ylim([0, 360])\n# ax.legend()\nleg = ax.legend(['Lee et al.', 'two-step'], handlelength=0, handletextpad=0)\ncolor_l = ['darkgreen', 'navy']\nfor n, text in enumerate( leg.texts ):\n    text.set_color( color_l[n] )\n\n    ax.grid(alpha=0.5)\n\nfig.tight_layout()\nif filename is not None:\n    fig.savefig(os.path.join(OUTPUT_DIR, filename), dpi=300)\n</code></pre> <p>References</p> <p>[1] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software (2025), doi: 10.21105/joss.08815 </p> <p>[2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \u201cOn the analysis of discrete time competing risks data\u201d, Biometrics (2018) doi: 10.1111/biom.12881</p>"},{"location":"ModelsComparison/#comparing-the-estimation-methods","title":"Comparing the Estimation Methods","text":""},{"location":"PaperCodeFigure/","title":"PaperCodeFigure","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\n\n# Data Generation\nreal_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)},\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])}}\n\npatients_df = generate_quick_start_df(n_patients=50000, n_cov=5, d_times=30, j_events=2, pid_col='pid', \n                                      seed=0, censoring_prob=0.8, real_coef_dict=real_coef_dict)\n\ntrain_df, test_df = train_test_split(patients_df, test_size=0.2)\n\n# DataExpansionFitter Usage\nfrom pydts.fitters import DataExpansionFitter\nfitter = DataExpansionFitter()\nfitter.fit(df=train_df.drop(['C', 'T'], axis=1))\n\npred_df = fitter.predict_cumulative_incident_function(test_df.drop(['J', 'T', 'C', 'X'], axis=1))\n\n# TwoStagesFitter Usage\nfrom pydts.fitters import TwoStagesFitter\nnew_fitter = TwoStagesFitter()\nnew_fitter.fit(df=train_df.drop(['C', 'T'], axis=1))\n\npred_df = fitter.predict_cumulative_incident_function(test_df.drop(['J', 'T', 'C', 'X'], axis=1))\n\n# Training with Regularization\nL1_regularized_fitter = TwoStagesFitter()\nfit_beta_kwargs = {'model_kwargs': {'penalizer': 0.003, 'l1_ratio': 1}}\nL1_regularized_fitter.fit(df=train_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter = TwoStagesFitter()\nfit_beta_kwargs = {'model_kwargs': {'penalizer': 0.003, 'l1_ratio': 0}}\nL2_regularized_fitter.fit(df=train_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nEN_regularized_fitter = TwoStagesFitter()\nfit_beta_kwargs = {'model_kwargs': {'penalizer': 0.003, 'l1_ratio': 0.5}}\nEN_regularized_fitter.fit(df=train_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"PerformanceMeasures/","title":"Evaluation","text":"<p>Model evaluation on test data or by CV, can be done using the evaluation functions available in PyDTS and the measures of performance presented in the Methods section.   </p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nreal_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\nn_patients = 50000\nn_cov = 5\n\npatients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, censoring_prob=0.8, \n                                      real_coef_dict=real_coef_dict)\n\ntrain_df, test_df = train_test_split(patients_df, test_size=0.2)\n\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 <p>For example, in the following code, the survival models are estimated based on the two-stage approach and the dataset train_df. Assume that the event of main interest is \\(j=1\\). Then, \\(\\pi_{i1}(t)\\) are calculated and stored in pred_df, and finally \\(\\widehat{\\mbox{AUC}}_1(t)\\), \\(t=1,\\ldots,d\\), are provided by</p> <pre><code>from pydts.fitters import TwoStagesFitter\nfrom pydts.evaluation import *\n\nfitter = TwoStagesFitter()\nfitter.fit(df = train_df)\npred_df = fitter.predict_prob_event_j_all(test_df, event=1)\nauc_1 = event_specific_auc_at_t_all(pred_df, event=1)\nprint(f'AUC(t) for event 1 is:')\nauc_1\n</code></pre> <pre>\n<code>AUC(t) for event 1 is:\n</code>\n</pre> <pre>\n<code>1     0.988007\n2     0.989981\n3     0.990528\n4     0.992057\n5     0.991414\n6     0.992463\n7     0.992465\n8     0.993415\n9     0.993273\n10    0.992431\n11    0.994390\n12    0.995089\n13    0.992101\n14    0.993462\n15    0.995626\n16    0.994383\n17    0.994551\n18    0.992995\n19    0.993614\n20    0.993318\n21    0.994786\n22    0.992859\n23    0.994097\n24    0.997663\n25    0.998358\n26    0.997495\n27    0.997700\n28    0.998260\n29    0.994920\n30    0.988650\nName: 1, dtype: float64</code>\n</pre> <p>Other measures such as \\(\\widehat{\\mbox{AUC}}_1\\), \\(\\widehat{\\mbox{BS}}_1\\), \\(\\widehat{\\mbox{AUC}}\\), and \\(\\widehat{\\mbox{BS}}\\) can be calculated by</p> <pre><code>pred_df = fitter.predict_prob_events(test_df)\nibs_1 = event_specific_integrated_brier_score(pred_df, event = 1)\niauc_1 = event_specific_integrated_auc(pred_df, event = 1)\nbs = global_brier_score(pred_df)\nauc = global_auc(pred_df)\n</code></pre> <p>Model evaluation based on K-fold CV and <code>TwoStagesFitter</code> can be done by</p> <pre><code>from pydts.cross_validation import TwoStagesCV\n\ncross_validator = TwoStagesCV()\ncross_validator.cross_validate(full_df = patients_df.drop(['C', 'T'], \n                               axis = 1), \n                               n_splits = 5, seed = 0,\n                               metrics = ['BS', 'IBS', 'GBS', \n                                          'AUC', 'IAUC', 'GAUC'])\n</code></pre> <pre>\n<code>Fitting fold 1/5\nFitting fold 2/5\nFitting fold 3/5\nFitting fold 4/5\nFitting fold 5/5\n</code>\n</pre> 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 <p>20 rows \u00d7 30 columns</p> <p>Results of the AUC(t), BS(t) from the cross-validation procedure to each of the folds and each of the risks:</p> <pre><code>cross_validator.results\n</code></pre> 1 2 3 4 5 6 7 8 9 10 ... 21 22 23 24 25 26 27 28 29 30 metric fold BS 0 1 0.063049 0.053907 0.045755 0.043523 0.042300 0.046062 0.037669 0.039918 0.038884 0.039600 ... 0.036635 0.058504 0.046140 0.053152 0.061784 0.050324 0.065542 0.062469 0.090904 0.072492 2 0.024376 0.019168 0.019545 0.017727 0.019594 0.017552 0.021547 0.020059 0.016670 0.019916 ... 0.018437 0.034735 0.019682 0.032066 0.023314 0.031648 0.027103 0.019322 0.048674 0.048384 AUC 0 1 0.652021 0.643642 0.641287 0.643345 0.667766 0.643727 0.655765 0.667565 0.570066 0.653903 ... 0.514947 0.531114 0.499528 0.478013 0.578898 0.500812 0.414534 0.475877 0.582044 0.412960 2 0.672119 0.677260 0.688408 0.646792 0.686449 0.659598 0.629191 0.626242 0.693669 0.635776 ... 0.612318 0.495373 0.510674 0.517435 0.542218 0.504568 0.725689 0.297011 0.563298 0.587978 BS 1 1 0.063104 0.051763 0.047522 0.045185 0.039390 0.040410 0.040533 0.037199 0.035189 0.039389 ... 0.048439 0.039984 0.049740 0.037045 0.054407 0.063798 0.056508 0.063322 0.055487 0.046272 2 0.026567 0.020499 0.023158 0.019674 0.017881 0.018550 0.012963 0.019118 0.018793 0.020397 ... 0.016248 0.024147 0.021958 0.020464 0.024052 0.030794 0.022079 0.047698 0.055665 0.039775 AUC 1 1 0.637347 0.653555 0.670154 0.653391 0.674059 0.664124 0.626964 0.639071 0.622290 0.624590 ... 0.470615 0.619649 0.576780 0.408460 0.472122 0.534623 0.390513 0.458682 0.399079 0.547307 2 0.702004 0.665337 0.698247 0.670620 0.612123 0.662859 0.732118 0.652722 0.610839 0.666786 ... 0.564700 0.564492 0.474570 0.610849 0.528472 0.602485 0.547596 0.466056 0.532451 0.328246 BS 2 1 0.064886 0.049643 0.045128 0.044718 0.040647 0.041574 0.037698 0.035049 0.041622 0.038163 ... 0.048869 0.041845 0.053532 0.052515 0.027061 0.060276 0.048393 0.064393 0.074945 0.069480 2 0.024451 0.016186 0.021169 0.021732 0.021547 0.021889 0.017873 0.020496 0.015362 0.014820 ... 0.018748 0.021711 0.017476 0.020739 0.033913 0.033076 0.031291 0.034467 0.043016 0.055695 AUC 2 1 0.659157 0.654793 0.649652 0.659449 0.674832 0.660598 0.594342 0.621234 0.648151 0.634931 ... 0.611217 0.482856 0.548870 0.517922 0.367427 0.536657 0.581426 0.453719 0.453618 0.531055 2 0.657810 0.657075 0.706675 0.713073 0.645342 0.666765 0.653272 0.674614 0.565575 0.620555 ... 0.552351 0.616195 0.470557 0.589532 0.554307 0.558689 0.494114 0.490396 0.369201 0.509630 BS 3 1 0.059994 0.050895 0.045448 0.043562 0.037141 0.043832 0.038703 0.043434 0.035425 0.037136 ... 0.039925 0.050829 0.056631 0.044824 0.062753 0.075129 0.069417 0.059778 0.069127 0.041407 2 0.024225 0.021932 0.019773 0.015853 0.017388 0.017645 0.017031 0.018022 0.015532 0.019680 ... 0.022589 0.030680 0.029422 0.024571 0.031540 0.020359 0.031400 0.025750 0.042687 0.055311 AUC 3 1 0.659007 0.665953 0.682254 0.636794 0.677254 0.626931 0.650321 0.615777 0.639428 0.600239 ... 0.464096 0.516016 0.450649 0.644558 0.493753 0.510769 0.481684 0.635580 0.504782 0.445025 2 0.702751 0.677411 0.691677 0.699105 0.657049 0.655707 0.675905 0.646852 0.718233 0.665166 ... 0.594124 0.541976 0.492703 0.830657 0.479912 0.431244 0.579373 0.386118 0.505585 0.381362 BS 4 1 0.064885 0.053155 0.047227 0.045384 0.041575 0.035789 0.038320 0.040585 0.031461 0.032940 ... 0.038013 0.043923 0.066064 0.044236 0.061176 0.055079 0.067624 0.050423 0.075282 0.103932 2 0.023856 0.019616 0.022393 0.021449 0.020102 0.016353 0.019711 0.020459 0.021224 0.017895 ... 0.025120 0.015200 0.019491 0.019334 0.022800 0.033254 0.037406 0.033727 0.043146 0.062432 AUC 4 1 0.659357 0.636783 0.654152 0.637880 0.631202 0.647121 0.638941 0.625363 0.599964 0.641299 ... 0.524715 0.494028 0.502059 0.530022 0.579939 0.578329 0.511615 0.481707 0.389172 0.331990 2 0.672312 0.657230 0.687828 0.674431 0.653015 0.616802 0.636077 0.697158 0.613994 0.613293 ... 0.524942 0.592352 0.463863 0.605593 0.477705 0.490352 0.519561 0.739740 0.509843 0.459805 <p>20 rows \u00d7 30 columns</p> <p>with the integrated AUC and BS to each of folds and each of the risks:</p> <pre><code>pd.DataFrame.from_records(cross_validator.integrated_auc)\n</code></pre> 0 1 2 3 4 1 0.629493 0.627465 0.636286 0.634377 0.622095 2 0.651277 0.647620 0.646869 0.655923 0.642630 <pre><code>pd.DataFrame.from_records(cross_validator.integrated_bs)\n</code></pre> 0 1 2 3 4 1 0.048382 0.046735 0.046411 0.046305 0.047747 2 0.020957 0.021362 0.021003 0.020547 0.021381 <p>and lastly, the global AUC and global BS to each of the folds:</p> <pre><code>print(cross_validator.global_auc)\n</code></pre> <pre>\n<code>{0: 0.635987860592661, 1: 0.633667449693418, 2: 0.6395581436224567, 3: 0.6408781976004154, 4: 0.6284087721165604}\n</code>\n</pre> <pre><code>print(cross_validator.global_bs)\n</code></pre> <pre>\n<code>{0: 0.0402049045296208, 1: 0.03892613679526756, 2: 0.03855544852975435, 3: 0.0385327046054388, 4: 0.039640037102306486}\n</code>\n</pre>"},{"location":"PerformanceMeasures/#performance-measures","title":"Performance Measures","text":""},{"location":"PerformanceMeasures/#references","title":"References","text":"<p>[1] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software (2025), doi: 10.21105/joss.08815</p>"},{"location":"Regularization/","title":"Regularization","text":"<p>Regularized regression can be easily accommodated only with <code>TwoStagesFitter</code> where we first estimate \\(\\beta_j\\) and then \\(\\alpha_{jt}\\). Regularization is introduced by <code>CoxPHFitter</code> of lifelines with event-specific tuning parameters, \\(\\eta_j \\geq 0\\), and <code>l1_ratio</code> argument. </p> <p>For each \\(j\\), usually, a path of models in \\(\\eta_j\\) are fitted, and the value of <code>l1_ratio</code> defines the type of prediction model. In particular, ridge regression is performed by setting <code>l1_ratio=0</code>, lasso by <code>l1_ratio=1</code>, and elastic net by 0 &lt; <code>l1_ratio</code> &lt; 1.</p> <p>In the following, we present how to use PyDTS to fit a lasso regularized model, and how to tune the regularization parameters \\(\\eta_j\\).</p> <p>We start by generating data, as discussed in previous sections:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nreal_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\nn_patients = 50000\nn_cov = 5\n\npatients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, censoring_prob=0.8, \n                                      real_coef_dict=real_coef_dict)\n\ntrain_df, test_df = train_test_split(patients_df, test_size=0.2)\n\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 31 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 31 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 31 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 <p>Lasso with \\(\\eta_1=0.003\\) and \\(\\eta_2=0.005\\), can be applied by</p> <pre><code>from pydts.fitters import TwoStagesFitter\n\nL1_regularized_fitter = TwoStagesFitter()\nfit_beta_kwargs = {\n    'model_kwargs': {\n        1: {'penalizer': 0.003, 'l1_ratio': 1},\n        2: {'penalizer': 0.005, 'l1_ratio': 1}\n}}\nL1_regularized_fitter.fit(df = patients_df.drop(['C', 'T'], axis = 1),\n                          fit_beta_kwargs = fit_beta_kwargs)\n\nL1_regularized_fitter.print_summary()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000101 1.981085e-08 0.000024 Z2 -0.772797 0.025400 -7.976064e-07 0.000071 Z3 -0.761229 0.025532 -1.720702e-01 0.038499 Z4 -0.550481 0.025318 -8.073968e-07 0.000072 Z5 -0.338471 0.025211 -3.409610e-07 0.000031 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 3374 True -1.471644 2 2328 True -1.714213 3 1805 True -1.859723 4 1524 True -1.920774 5 1214 True -2.050566 6 1114 True -2.038532 7 916 True -2.142666 8 830 True -2.151764 9 683 True -2.257665 10 626 True -2.258146 11 569 True -2.268550 12 516 True -2.281249 13 419 True -2.406485 14 410 True -2.340125 15 326 True -2.482320 16 320 True -2.415531 17 280 True -2.460601 18 240 True -2.526029 19 243 True -2.422234 20 204 True -2.506867 21 176 True -2.564875 22 167 True -2.524524 23 166 True -2.431232 24 118 True -2.667097 25 114 True -2.596554 26 109 True -2.527812 27 89 True -2.614859 28 70 True -2.731941 29 67 True -2.645782 30 47 True -2.856479 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 1250 True -3.578498 2 839 True -3.857174 3 805 True -3.793806 4 644 True -3.922340 5 570 True -3.953290 6 483 True -4.033435 7 416 True -4.097484 8 409 True -4.031967 9 323 True -4.185559 10 306 True -4.159369 11 240 True -4.326672 12 246 True -4.222570 13 226 True -4.228361 14 198 True -4.280986 15 170 True -4.351662 16 162 True -4.321422 17 147 True -4.335179 18 115 True -4.497948 19 125 True -4.329126 20 118 True -4.299702 21 83 True -4.569291 22 89 True -4.409830 23 65 True -4.633946 24 59 True -4.627577 25 58 True -4.544617 26 53 True -4.528494 27 43 True -4.624557 28 38 True -4.626009 29 43 True -4.376003 30 37 True -4.384266 <p>In penalized regression, one should fit a path of models in each \\(\\eta_j\\), \\(j=1,\\ldots,M\\). The final set of values of \\(\\eta_1,\\ldots,\\eta_M\\) corresponds to the values yielding the best results in terms of  pre-specified criteria, such as maximizing \\(\\widehat{\\mbox{AUC}}_j\\) and  \\(\\widehat{\\mbox{AUC}}\\), or minimizing \\(\\widehat{\\mbox{BS}}_j\\) and \\(\\widehat{\\mbox{BS}}\\). The default criteria in PyDTS is maximizing the global AUC, \\(\\widehat{\\mbox{AUC}}\\). Two \\(M\\)-dimensional grid search options are implemented, <code>PenaltyGridSearch</code> when the user provides train and test datasets, and <code>PenaltyGridSearchCV</code> for applying a K-fold cross validation (CV) approach.</p> <p>When train and test sets are available, by excecuting the following code, all the four optimization criteria are calculated over the \\(M\\)-dimensional grid and optimal_set includes the optimal values of \\(\\eta_1,\\ldots,\\eta_M\\) based on \\(\\widehat{\\mbox{AUC}}\\). Here, the optimal set based on \\(\\widehat{\\mbox{AUC}}\\) is \\(\\log\\eta_1 = -6\\) and \\(\\log\\eta_2 = -6\\).</p> <p>It is noted, that we estimate the parameters of each \\(\\eta_j\\) once. However, since our performance measures requires the evaluation of the overall survival function, we must check each possible combination of \\(\\eta_j\\) seperately. This can be time consuming, especially when we would like to choose between a large number of possible penalizers.</p> <pre><code>from pydts.model_selection import PenaltyGridSearch\n\n\npenalizers = np.exp([-2, -3, -4, -5, -6])\ngrid_search = PenaltyGridSearch()\noptimal_set = grid_search.evaluate(train_df, test_df, l1_ratio = 1, \n                                   penalizers = penalizers,\n                                   metrics = ['IBS', 'GBS', 'IAUC', 'GAUC'])\n\nprint(optimal_set)\n</code></pre> <pre>\n<code>Started estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 199 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 204 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 206 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 207 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 213 seconds\n(0.0024787521766663585, 0.0024787521766663585)\n</code>\n</pre> <p>The user can choose the set of \\(\\eta_j\\), \\(j=1,\\ldots,M\\), values that optimizes other desired criteria. For example, the set that minimizes \\(\\widehat{\\mbox{BS}}\\) can be selected as follows </p> <pre><code>res = grid_search.convert_results_dict_to_df(grid_search.global_bs)\nres.columns = ['BS']\nres.index.set_names(['eta_1', 'eta_2'], inplace=True)\nres\n</code></pre> BS eta_1 eta_2 0.135335 0.135335 0.038662 0.049787 0.038662 0.018316 0.038633 0.006738 0.038468 0.002479 0.038137 0.049787 0.135335 0.038430 0.049787 0.038430 0.018316 0.038403 0.006738 0.038245 0.002479 0.037919 0.018316 0.135335 0.035197 0.049787 0.035197 0.018316 0.035199 0.006738 0.035164 0.002479 0.034911 0.006738 0.135335 0.031541 0.049787 0.031541 0.018316 0.031592 0.006738 0.031685 0.002479 0.031431 0.002479 0.135335 0.028503 0.049787 0.028503 0.018316 0.028563 0.006738 0.028698 0.002479 0.028441 <pre><code>grid_search.convert_results_dict_to_df(grid_search.global_bs).idxmin()\n</code></pre> <pre>\n<code>0    (0.0024787521766663585, 0.0024787521766663585)\ndtype: object</code>\n</pre> <p>the final model can be retrieved by</p> <pre><code>optimal_two_stages_fitter = grid_search.get_mixed_two_stages_fitter(optimal_set)\n</code></pre> <p>Alternatively, 5-fold CV is performed by</p> <pre><code>from pydts.cross_validation import PenaltyGridSearchCV\n\n\npenalizers = np.exp([-2, -3, -4, -5, -6])\ngrid_search_cv = PenaltyGridSearchCV()\nresults_df = grid_search_cv.cross_validate(patients_df, l1_ratio=1, \n                                           penalizers=penalizers, n_splits=5, \n                                           metrics=['IBS', 'GBS', 'IAUC', 'GAUC'])\n</code></pre> <pre>\n<code>Starting fold 1/5\nStarted estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 174 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 186 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 209 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 155 seconds\nFinished fold 1/5, 1003 seconds\nStarting fold 2/5\nStarted estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 175 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 187 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 212 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 178 seconds\nFinished fold 2/5, 1031 seconds\nStarting fold 3/5\nStarted estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 200 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 210 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 186 seconds\nFinished fold 3/5, 1039 seconds\nStarting fold 4/5\nStarted estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 177 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 202 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 211 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 179 seconds\nFinished fold 4/5, 1037 seconds\nStarting fold 5/5\nStarted estimating the coefficients for penalizer 0.1353352832366127 (1/5)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (1/5), 176 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (2/5)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (2/5), 188 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (3/5)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (3/5), 201 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (4/5)\n</code>\n</pre> <pre>\n<code>Finished estimating the coefficients for penalizer 0.006737946999085467 (4/5), 213 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/5)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/5), 182 seconds\nFinished fold 5/5, 1040 seconds\n</code>\n</pre> <pre><code>results_df\n</code></pre> Mean SE 0.135335 0.135335 0.638475 0.002592 0.049787 0.639094 0.003754 0.018316 0.639103 0.003627 0.006738 0.637383 0.002958 0.002479 0.488831 0.003291 0.049787 0.135335 0.638764 0.002806 0.049787 0.638899 0.003862 0.018316 0.639014 0.003780 0.006738 0.637522 0.003173 0.002479 0.488832 0.003291 0.018316 0.135335 0.638666 0.002676 0.049787 0.638953 0.003742 0.018316 0.639031 0.003669 0.006738 0.637576 0.003055 0.002479 0.488833 0.003298 0.006738 0.135335 0.563873 0.001147 0.049787 0.563873 0.001147 0.018316 0.563872 0.001150 0.006738 0.563827 0.001226 0.002479 0.615068 0.002719 0.002479 0.135335 0.568879 0.001269 0.049787 0.568879 0.001269 0.018316 0.568879 0.001268 0.006738 0.568834 0.001290 0.002479 0.630541 0.004160 <pre><code>optimal_set = results_df['Mean'].idxmax()\noptimal_set\n</code></pre> <pre>\n<code>(0.1353352832366127, 0.01831563888873418)</code>\n</pre>"},{"location":"Regularization/#regularization","title":"Regularization","text":""},{"location":"Regularization/#predefined-regularization-parameters","title":"Predefined Regularization Parameters","text":""},{"location":"Regularization/#tuning-regularization-parameters","title":"Tuning Regularization Parameters","text":""},{"location":"Regularization/#penaltygridsearch","title":"PenaltyGridSearch","text":""},{"location":"Regularization/#penaltygridsearchcv","title":"PenaltyGridSearchCV","text":""},{"location":"Regularization/#references","title":"References","text":"<p>[1] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software (2025), doi: 10.21105/joss.08815</p>"},{"location":"Simple%20Simulation/","title":"Simple Simulation","text":"<p>In this example, we present an existing method [1] and our new method on a very simple simulated dataset.  We compare both methods based on simplicity, goodness of fit and performance.   [1] On the Analysis of Discrete Time Competing Risks Data, Biometrics, Lee et al. 2018</p> <p>We simulate N=50,000 patients, with 5 covariate \\(Z_p\\) (\\(p \\in [1,...,5]\\)) randomly sampled from \\(\\mathbb{R} [0,1]\\).   Our timeline is discrete, i.e \\(t \\in [1,...,d]\\). Here, we choose \\(d=30\\).   The event type \\(J_i\\) is randomly sampled from \\(J_i \\sim Uniform[1,...,M]\\), where \\(M\\) is the number of competing events. In this example \\(M=2\\).  For each of the samples, we sample it's time-to-event \\(T_i\\) from the hazard function \\(\\lambda_{j}(T|Z) = \\frac{exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+exp(\\alpha_{jt}+Z^{T}\\beta_{j})}\\) such that  \\(\\alpha_{1t} = -1-0.3 log(t)\\) \\(\\beta_{1} = -log([0.8, 3, 3, 2.5, 2])\\) \\(\\alpha_{2t} = -1.75-0.15 log(t)\\) \\(\\beta_{2} = -log([1, 3, 4, 3, 2])\\) </p> <p>and randomly sample a censoring time \\(C_i\\) such that \\(C_i \\sim Uniform[1, ... , d]\\).  We then calculate \\(X_i = min(T_i, C_i)\\) and for cencored samples we set the event type to be \\(J=0\\).  Lastly, we split the data into two separate datasets for training (75%) and for testing (25%).</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>import numpy as np\n\nreal_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n</code></pre> <pre><code>n_patients = 50000\nn_cov = 5\npatients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, real_coef_dict=real_coef_dict)\ncovariates = [f'Z{i + 1}' for i in range(n_cov)]\n\ntrain_df, test_df = train_test_split(patients_df, test_size=0.25)\nevents = sorted(train_df['J'].unique())\ntimes = sorted(train_df['X'].unique())\ntrain_df.head()  \n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 31111 31111 0.661973 0.211958 0.770829 0.677775 0.653131 0 28 24 24 30206 30206 0.740999 0.927717 0.182855 0.897037 0.084195 0 20 17 17 44602 44602 0.442468 0.146094 0.137102 0.901129 0.077351 1 12 15 12 20383 20383 0.874138 0.374438 0.863105 0.472107 0.653651 0 30 17 17 32433 32433 0.748574 0.937747 0.171156 0.927878 0.666952 0 30 12 12 <pre><code>from pydts.fitters import repetitive_fitters\nrep_dict, times_dict, counts_df = repetitive_fitters(rep=15, n_patients=n_patients, n_cov=n_cov, d_times=30,\n                                                         j_events=2, pid_col='pid', test_size=0.25, verbose=0, \n                                                         allow_fails=20, real_coef_dict=real_coef_dict, \n                                                         censoring_prob=.8)\n</code></pre> <pre>\n<code> 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                      | 14/35 [05:35&lt;08:23, 23.96s/it]</code>\n</pre> <pre>\n<code>final: 15\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre><code>from pydts.examples_utils.plots import plot_reps_coef_std\n\nnew_res_dict = plot_reps_coef_std(rep_dict, True)\n</code></pre> <pre><code>from pydts.examples_utils.plots import plot_models_coefficients\na = new_res_dict['alpha']\nb = new_res_dict['beta']\ntimes = train_df['X'].sort_values().unique()\nn_cov = 5\ntemp_c_df = counts_df.loc[[1,2]].groupby(['X']).sum().values.flatten().astype(int)\nplot_models_coefficients(a, b, times, temp_c_df)\n</code></pre> <pre><code>from pydts.examples_utils.plots import plot_times\n\nplot_times(times_dict)\n</code></pre> <pre>\n<code>&lt;AxesSubplot:xlabel='Model type', ylabel='Fitting Time [seconds]'&gt;</code>\n</pre> <p>Lee et al. suggested to expand the data so that for each patient we have row for each \\(t \\in [1, ... , T_i]\\), with binary event columns which are 1 only at the time of the event. here \\(j_0 = 1 - j_1 - j_2\\).   Then, for each event we estimate {\\(\\alpha_{jt}, \\beta_{j}\\)} using a binary regression model.</p> <pre><code>from pydts.fitters import DataExpansionFitter\nfitter = DataExpansionFitter()\nfitter.fit(df=train_df.drop(['C', 'T'], axis=1))\nfitter.print_summary()\n</code></pre> <pre>\n<code>\n\nModel summary for event: 1\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_1   No. Observations:               357418\nModel:                            GLM   Df Residuals:                   357383\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -54259.\nDate:                Sun, 03 Jul 2022   Deviance:                   1.0852e+05\nTime:                        15:23:56   Pearson chi2:                 3.58e+05\nNo. Iterations:                     7   Pseudo R-squ. (CS):            0.01510\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -0.9516      0.039    -24.639      0.000      -1.027      -0.876\nC(X)[2]       -1.2097      0.041    -29.216      0.000      -1.291      -1.129\nC(X)[3]       -1.3471      0.044    -30.809      0.000      -1.433      -1.261\nC(X)[4]       -1.3832      0.046    -30.366      0.000      -1.472      -1.294\nC(X)[5]       -1.5047      0.048    -31.144      0.000      -1.599      -1.410\nC(X)[6]       -1.4645      0.050    -29.574      0.000      -1.562      -1.367\nC(X)[7]       -1.5566      0.052    -29.750      0.000      -1.659      -1.454\nC(X)[8]       -1.5641      0.054    -28.857      0.000      -1.670      -1.458\nC(X)[9]       -1.7018      0.058    -29.119      0.000      -1.816      -1.587\nC(X)[10]      -1.6294      0.059    -27.517      0.000      -1.745      -1.513\nC(X)[11]      -1.6601      0.062    -26.763      0.000      -1.782      -1.539\nC(X)[12]      -1.6994      0.065    -26.040      0.000      -1.827      -1.571\nC(X)[13]      -1.7980      0.070    -25.626      0.000      -1.936      -1.661\nC(X)[14]      -1.7798      0.073    -24.514      0.000      -1.922      -1.638\nC(X)[15]      -1.8642      0.078    -23.809      0.000      -2.018      -1.711\nC(X)[16]      -1.7972      0.080    -22.479      0.000      -1.954      -1.641\nC(X)[17]      -1.7883      0.084    -21.385      0.000      -1.952      -1.624\nC(X)[18]      -1.9692      0.095    -20.833      0.000      -2.154      -1.784\nC(X)[19]      -1.8447      0.095    -19.479      0.000      -2.030      -1.659\nC(X)[20]      -1.9415      0.104    -18.631      0.000      -2.146      -1.737\nC(X)[21]      -2.0724      0.117    -17.710      0.000      -2.302      -1.843\nC(X)[22]      -1.9505      0.118    -16.555      0.000      -2.181      -1.720\nC(X)[23]      -1.8161      0.119    -15.214      0.000      -2.050      -1.582\nC(X)[24]      -2.0534      0.144    -14.272      0.000      -2.335      -1.771\nC(X)[25]      -2.0323      0.155    -13.081      0.000      -2.337      -1.728\nC(X)[26]      -2.0328      0.173    -11.771      0.000      -2.371      -1.694\nC(X)[27]      -1.9000      0.183    -10.385      0.000      -2.259      -1.541\nC(X)[28]      -2.0244      0.229     -8.832      0.000      -2.474      -1.575\nC(X)[29]      -2.2560      0.321     -7.023      0.000      -2.886      -1.626\nC(X)[30]      -2.0416      0.414     -4.927      0.000      -2.854      -1.229\nZ1             0.1805      0.031      5.865      0.000       0.120       0.241\nZ2            -1.1065      0.031    -35.243      0.000      -1.168      -1.045\nZ3            -1.1032      0.032    -34.901      0.000      -1.165      -1.041\nZ4            -0.8931      0.031    -28.536      0.000      -0.954      -0.832\nZ5            -0.6629      0.031    -21.306      0.000      -0.724      -0.602\n==============================================================================\n\n\nModel summary for event: 2\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_2   No. Observations:               357418\nModel:                            GLM   Df Residuals:                   357383\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -28316.\nDate:                Sun, 03 Jul 2022   Deviance:                       56631.\nTime:                        15:23:56   Pearson chi2:                 3.59e+05\nNo. Iterations:                     8   Pseudo R-squ. (CS):           0.006804\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -1.7249      0.058    -29.825      0.000      -1.838      -1.612\nC(X)[2]       -1.9861      0.063    -31.491      0.000      -2.110      -1.862\nC(X)[3]       -1.8825      0.064    -29.415      0.000      -2.008      -1.757\nC(X)[4]       -1.9856      0.068    -29.285      0.000      -2.118      -1.853\nC(X)[5]       -1.9548      0.070    -28.038      0.000      -2.091      -1.818\nC(X)[6]       -2.0710      0.074    -27.870      0.000      -2.217      -1.925\nC(X)[7]       -2.0900      0.077    -27.041      0.000      -2.241      -1.939\nC(X)[8]       -2.0176      0.078    -25.728      0.000      -2.171      -1.864\nC(X)[9]       -2.1402      0.084    -25.383      0.000      -2.305      -1.975\nC(X)[10]      -2.0986      0.086    -24.341      0.000      -2.268      -1.930\nC(X)[11]      -2.1868      0.092    -23.735      0.000      -2.367      -2.006\nC(X)[12]      -2.1230      0.094    -22.634      0.000      -2.307      -1.939\nC(X)[13]      -2.1778      0.099    -21.913      0.000      -2.373      -1.983\nC(X)[14]      -2.1732      0.103    -21.039      0.000      -2.376      -1.971\nC(X)[15]      -2.1550      0.107    -20.087      0.000      -2.365      -1.945\nC(X)[16]      -2.2824      0.118    -19.375      0.000      -2.513      -2.052\nC(X)[17]      -2.2170      0.121    -18.375      0.000      -2.453      -1.981\nC(X)[18]      -2.4341      0.139    -17.550      0.000      -2.706      -2.162\nC(X)[19]      -2.1037      0.128    -16.470      0.000      -2.354      -1.853\nC(X)[20]      -2.2105      0.141    -15.677      0.000      -2.487      -1.934\nC(X)[21]      -2.4562      0.166    -14.765      0.000      -2.782      -2.130\nC(X)[22]      -2.2282      0.160    -13.940      0.000      -2.541      -1.915\nC(X)[23]      -2.4139      0.186    -12.968      0.000      -2.779      -2.049\nC(X)[24]      -2.4001      0.201    -11.917      0.000      -2.795      -2.005\nC(X)[25]      -2.4684      0.226    -10.905      0.000      -2.912      -2.025\nC(X)[26]      -2.1506      0.217     -9.894      0.000      -2.577      -1.725\nC(X)[27]      -2.0406      0.232     -8.784      0.000      -2.496      -1.585\nC(X)[28]      -2.2066      0.296     -7.454      0.000      -2.787      -1.626\nC(X)[29]      -2.1456      0.361     -5.951      0.000      -2.852      -1.439\nC(X)[30]      -1.8980      0.455     -4.172      0.000      -2.790      -1.006\nZ1             0.0433      0.046      0.940      0.347      -0.047       0.133\nZ2            -1.0656      0.047    -22.688      0.000      -1.158      -0.974\nZ3            -1.3990      0.048    -29.224      0.000      -1.493      -1.305\nZ4            -1.1034      0.047    -23.399      0.000      -1.196      -1.011\nZ5            -0.7174      0.047    -15.386      0.000      -0.809      -0.626\n==============================================================================\n</code>\n</pre> <pre><code>from pydts.examples_utils.plots import plot_first_model_coefs\nplot_first_model_coefs(models=fitter.event_models, times=fitter.times, train_df=patients_df, n_cov=5)\n</code></pre> <p>Here, for each event, we first estimate \\(\\beta_{j}\\) using a time-stratified CoxPH model and the expanded data as we used in the previous approach.   Afterwards, we evaluate \\(\\alpha_{jt}\\) based on the original training data.</p> <pre><code>from pydts.fitters import TwoStagesFitter\nnew_fitter = TwoStagesFitter()\nnew_fitter.fit(df=train_df.drop(['C', 'T'], axis=1))\n\nnew_fitter.print_summary()\n</code></pre> <pre>\n<code>INFO: Pandarallel will run on 4 workers.\nINFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n</code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 2560 True -0.993701 2 1713 True -1.252696 3 1321 True -1.390031 4 1123 True -1.426362 5 888 True -1.545886 6 823 True -1.506916 7 675 True -1.602185 8 603 True -1.610078 9 473 True -1.746932 10 458 True -1.675599 11 400 True -1.706351 12 346 True -1.745275 13 283 True -1.841762 14 259 True -1.824197 15 213 True -1.905305 16 203 True -1.841176 17 182 True -1.832710 18 135 True -2.017525 19 135 True -1.886645 20 108 True -1.990557 21 83 True -2.119080 22 82 True -1.999757 23 80 True -1.859465 24 53 True -2.100951 25 45 True -2.080507 26 36 True -2.081074 27 32 True -1.950757 28 20 True -2.072971 29 10 True -2.292853 30 6 True -2.089735 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 942 True -1.741469 2 617 True -2.004244 3 596 True -1.891698 4 472 True -2.003929 5 430 True -1.973737 6 341 True -2.086884 7 299 True -2.104953 8 288 True -2.034741 9 228 True -2.150781 10 214 True -2.110933 11 176 True -2.191509 12 168 True -2.131838 13 143 True -2.180693 14 129 True -2.175601 15 117 True -2.157802 16 92 True -2.267452 17 87 True -2.209869 18 62 True -2.440465 19 76 True -2.107629 20 60 True -2.200231 21 41 True -2.457340 22 45 True -2.212729 23 32 True -2.418286 24 27 True -2.404584 25 21 True -2.465084 26 23 True -2.142334 27 20 True -2.045518 28 12 True -2.190062 29 8 True -2.137646 30 5 True -1.913849 <pre><code>from pydts.examples_utils.plots import plot_second_model_coefs\nplot_second_model_coefs(new_fitter.alpha_df, new_fitter.beta_models, times, n_cov=5)\n</code></pre> <pre><code>new_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.175630 0.029922 0.042164 0.045464 Z2 -1.077021 0.030535 -1.053325 0.046400 Z3 -1.072990 0.030744 -1.383865 0.047306 Z4 -0.868595 0.030434 -1.090288 0.046584 Z5 -0.643514 0.030250 -0.709322 0.046064 <pre><code>new_fitter.plot_all_events_beta()\n</code></pre> <pre>\n<code>&lt;AxesSubplot:title={'center':'$\\\\beta_{j}$ for all events'}, xlabel='Value', ylabel='$\\\\beta_{j}$'&gt;</code>\n</pre> <pre><code># pred_df = new_fitter.predict_cumulative_incident_function(test_df)\n# pred_df = new_fitter.predict_marginal_prob_all_events(pred_df)\n# pred_df.set_index(['pid']).head().T\npred_df = new_fitter.predict_cumulative_incident_function(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df\n</code></pre> pid 0 1 2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.940455 0.958727 0.930894 overall_survival_t2 0.896901 0.928018 0.880776 overall_survival_t3 0.858059 0.900500 0.836577 overall_survival_t4 0.822953 0.875209 0.796861 overall_survival_t5 0.791664 0.852514 0.761776 overall_survival_t6 0.761745 0.830406 0.728355 overall_survival_t7 0.734941 0.810424 0.698630 overall_survival_t8 0.708674 0.790680 0.669710 overall_survival_t9 0.686386 0.773755 0.645310 overall_survival_t10 0.663455 0.756133 0.620329 overall_survival_t11 0.642267 0.739641 0.597350 overall_survival_t12 0.621925 0.723707 0.575441 overall_survival_t13 0.603735 0.709345 0.555961 overall_survival_t14 0.585841 0.695072 0.536886 overall_survival_t15 0.569274 0.681787 0.519339 overall_survival_t16 0.553066 0.668569 0.502199 overall_survival_t17 0.536944 0.655324 0.485244 overall_survival_t18 0.524081 0.644643 0.471762 overall_survival_t19 0.508820 0.631972 0.455896 overall_survival_t20 0.495393 0.620725 0.441999 overall_survival_t21 0.484366 0.611361 0.430605 overall_survival_t22 0.471714 0.600590 0.417611 overall_survival_t23 0.459024 0.589532 0.404558 overall_survival_t24 0.448504 0.580388 0.393826 overall_survival_t25 0.438286 0.571408 0.383426 overall_survival_t26 0.427112 0.561650 0.372165 overall_survival_t27 0.414887 0.550861 0.359891 overall_survival_t28 0.404448 0.541554 0.349448 overall_survival_t29 0.395303 0.533440 0.340383 overall_survival_t30 0.384281 0.523584 0.329504 hazard_j1_t1 0.044777 0.032540 0.052977 hazard_j1_t10 0.023154 0.016723 0.027508 hazard_j1_t11 0.022469 0.016225 0.026698 hazard_j1_t12 0.021630 0.015615 0.025705 hazard_j1_t13 0.019679 0.014199 0.023396 hazard_j1_t14 0.020021 0.014447 0.023800 hazard_j1_t15 0.018490 0.013337 0.021987 hazard_j1_t16 0.019691 0.014208 0.023409 hazard_j1_t17 0.019855 0.014327 0.023603 hazard_j1_t18 0.016560 0.011938 0.019699 hazard_j1_t19 0.018832 0.013585 0.022392 hazard_j1_t2 0.034917 0.025303 0.041389 hazard_j1_t20 0.017005 0.012260 0.020227 hazard_j1_t21 0.014985 0.010798 0.017831 hazard_j1_t22 0.016852 0.012149 0.020045 hazard_j1_t23 0.019341 0.013954 0.022995 hazard_j1_t24 0.015255 0.010993 0.018151 hazard_j1_t25 0.015565 0.011217 0.018519 hazard_j1_t26 0.015556 0.011211 0.018509 hazard_j1_t27 0.017683 0.012752 0.021031 hazard_j1_t28 0.015681 0.011301 0.018656 hazard_j1_t29 0.012625 0.009091 0.015029 hazard_j1_t3 0.030573 0.022128 0.036271 hazard_j1_t30 0.015424 0.011116 0.018352 hazard_j1_t4 0.029514 0.021355 0.035022 hazard_j1_t5 0.026277 0.018995 0.031200 hazard_j1_t6 0.027292 0.019735 0.032399 hazard_j1_t7 0.024874 0.017974 0.029542 hazard_j1_t8 0.024683 0.017835 0.029317 hazard_j1_t9 0.021594 0.015590 0.025663 hazard_j2_t1 0.014768 0.008733 0.016130 hazard_j2_t10 0.010253 0.006052 0.011203 hazard_j2_t11 0.009467 0.005586 0.010345 hazard_j2_t12 0.010043 0.005927 0.010974 hazard_j2_t13 0.009569 0.005646 0.010456 hazard_j2_t14 0.009617 0.005675 0.010509 hazard_j2_t15 0.009788 0.005776 0.010696 hazard_j2_t16 0.008781 0.005179 0.009596 hazard_j2_t17 0.009296 0.005485 0.010158 hazard_j2_t18 0.007396 0.004360 0.008083 hazard_j2_t19 0.010287 0.006071 0.011240 hazard_j2_t2 0.011395 0.006728 0.012449 hazard_j2_t20 0.009386 0.005537 0.010256 hazard_j2_t21 0.007273 0.004287 0.007949 hazard_j2_t22 0.009270 0.005469 0.010130 hazard_j2_t23 0.007561 0.004457 0.008263 hazard_j2_t24 0.007664 0.004519 0.008376 hazard_j2_t25 0.007218 0.004254 0.007888 hazard_j2_t26 0.009939 0.005866 0.010861 hazard_j2_t27 0.010939 0.006458 0.011951 hazard_j2_t28 0.009481 0.005594 0.010360 hazard_j2_t29 0.009986 0.005893 0.010911 hazard_j2_t3 0.012735 0.007524 0.013911 hazard_j2_t30 0.012459 0.007360 0.013610 hazard_j2_t4 0.011398 0.006730 0.012453 hazard_j2_t5 0.011743 0.006935 0.012829 hazard_j2_t6 0.010500 0.006198 0.011473 hazard_j2_t7 0.010314 0.006088 0.011270 hazard_j2_t8 0.011056 0.006528 0.012079 hazard_j2_t9 0.009857 0.005817 0.010770 prob_j1_at_t1 0.044777 0.032540 0.052977 prob_j1_at_t2 0.032838 0.024259 0.038529 prob_j1_at_t3 0.027421 0.020535 0.031946 prob_j1_at_t4 0.025325 0.019231 0.029299 prob_j1_at_t5 0.021625 0.016625 0.024862 prob_j1_at_t6 0.021606 0.016825 0.024681 prob_j1_at_t7 0.018948 0.014926 0.021517 prob_j1_at_t8 0.018141 0.014454 0.020481 prob_j1_at_t9 0.015303 0.012326 0.017187 prob_j1_at_t10 0.015893 0.012940 0.017751 prob_j1_at_t11 0.014907 0.012268 0.016561 prob_j1_at_t12 0.013892 0.011550 0.015355 prob_j1_at_t13 0.012239 0.010276 0.013463 prob_j1_at_t14 0.012087 0.010248 0.013232 prob_j1_at_t15 0.010832 0.009270 0.011804 prob_j1_at_t16 0.011209 0.009687 0.012157 prob_j1_at_t17 0.010981 0.009578 0.011854 prob_j1_at_t18 0.008892 0.007823 0.009559 prob_j1_at_t19 0.009869 0.008757 0.010564 prob_j1_at_t20 0.008652 0.007748 0.009221 prob_j1_at_t21 0.007423 0.006702 0.007881 prob_j1_at_t22 0.008162 0.007428 0.008631 prob_j1_at_t23 0.009123 0.008380 0.009603 prob_j1_at_t24 0.007002 0.006481 0.007343 prob_j1_at_t25 0.006981 0.006510 0.007293 prob_j1_at_t26 0.006818 0.006406 0.007097 prob_j1_at_t27 0.007553 0.007162 0.007827 prob_j1_at_t28 0.006506 0.006225 0.006714 prob_j1_at_t29 0.005106 0.004923 0.005252 prob_j1_at_t30 0.006097 0.005929 0.006247 prob_j2_at_t1 0.014768 0.008733 0.016130 prob_j2_at_t2 0.010716 0.006451 0.011588 prob_j2_at_t3 0.011422 0.006982 0.012253 prob_j2_at_t4 0.009780 0.006061 0.010418 prob_j2_at_t5 0.009664 0.006070 0.010223 prob_j2_at_t6 0.008313 0.005284 0.008740 prob_j2_at_t7 0.007857 0.005055 0.008208 prob_j2_at_t8 0.008126 0.005290 0.008439 prob_j2_at_t9 0.006985 0.004599 0.007213 prob_j2_at_t10 0.007038 0.004682 0.007229 prob_j2_at_t11 0.006281 0.004224 0.006417 prob_j2_at_t12 0.006450 0.004384 0.006555 prob_j2_at_t13 0.005951 0.004086 0.006017 prob_j2_at_t14 0.005806 0.004025 0.005843 prob_j2_at_t15 0.005734 0.004015 0.005742 prob_j2_at_t16 0.004999 0.003531 0.004983 prob_j2_at_t17 0.005142 0.003667 0.005102 prob_j2_at_t18 0.003971 0.002857 0.003922 prob_j2_at_t19 0.005391 0.003914 0.005302 prob_j2_at_t20 0.004776 0.003500 0.004676 prob_j2_at_t21 0.003603 0.002661 0.003514 prob_j2_at_t22 0.004490 0.003344 0.004362 prob_j2_at_t23 0.003566 0.002677 0.003451 prob_j2_at_t24 0.003518 0.002664 0.003389 prob_j2_at_t25 0.003237 0.002469 0.003107 prob_j2_at_t26 0.004356 0.003352 0.004164 prob_j2_at_t27 0.004672 0.003627 0.004448 prob_j2_at_t28 0.003933 0.003081 0.003728 prob_j2_at_t29 0.004039 0.003191 0.003813 prob_j2_at_t30 0.004925 0.003926 0.004633 cif_j1_at_t1 0.044777 0.032540 0.052977 cif_j1_at_t2 0.077614 0.056799 0.091506 cif_j1_at_t3 0.105035 0.077334 0.123452 cif_j1_at_t4 0.130360 0.096565 0.152751 cif_j1_at_t5 0.151985 0.113190 0.177613 cif_j1_at_t6 0.173591 0.130014 0.202294 cif_j1_at_t7 0.192539 0.144940 0.223811 cif_j1_at_t8 0.210680 0.159394 0.244292 cif_j1_at_t9 0.225983 0.171721 0.261479 cif_j1_at_t10 0.241876 0.184661 0.279231 cif_j1_at_t11 0.256783 0.196929 0.295792 cif_j1_at_t12 0.270675 0.208478 0.311147 cif_j1_at_t13 0.282914 0.218755 0.324609 cif_j1_at_t14 0.295001 0.229003 0.337841 cif_j1_at_t15 0.305834 0.238273 0.349646 cif_j1_at_t16 0.317043 0.247959 0.361803 cif_j1_at_t17 0.328024 0.257537 0.373657 cif_j1_at_t18 0.336915 0.265361 0.383216 cif_j1_at_t19 0.346785 0.274118 0.393779 cif_j1_at_t20 0.355437 0.281866 0.403000 cif_j1_at_t21 0.362860 0.288568 0.410882 cif_j1_at_t22 0.371023 0.295996 0.419513 cif_j1_at_t23 0.380146 0.304376 0.429116 cif_j1_at_t24 0.387148 0.310857 0.436459 cif_j1_at_t25 0.394129 0.317368 0.443752 cif_j1_at_t26 0.400947 0.323774 0.450849 cif_j1_at_t27 0.408500 0.330936 0.458676 cif_j1_at_t28 0.415005 0.337161 0.465390 cif_j1_at_t29 0.420111 0.342084 0.470642 cif_j1_at_t30 0.426208 0.348014 0.476888 cif_j2_at_t1 0.014768 0.008733 0.016130 cif_j2_at_t2 0.025484 0.015183 0.027718 cif_j2_at_t3 0.036906 0.022166 0.039971 cif_j2_at_t4 0.046686 0.028226 0.050388 cif_j2_at_t5 0.056351 0.034296 0.060611 cif_j2_at_t6 0.064663 0.039580 0.069351 cif_j2_at_t7 0.072520 0.044635 0.077559 cif_j2_at_t8 0.080646 0.049925 0.085998 cif_j2_at_t9 0.087631 0.054524 0.093211 cif_j2_at_t10 0.094669 0.059207 0.100440 cif_j2_at_t11 0.100950 0.063430 0.106858 cif_j2_at_t12 0.107400 0.067814 0.113413 cif_j2_at_t13 0.113351 0.071900 0.119430 cif_j2_at_t14 0.119158 0.075926 0.125272 cif_j2_at_t15 0.124892 0.079941 0.131015 cif_j2_at_t16 0.129891 0.083472 0.135998 cif_j2_at_t17 0.135032 0.087139 0.141100 cif_j2_at_t18 0.139004 0.089996 0.145022 cif_j2_at_t19 0.144395 0.093910 0.150324 cif_j2_at_t20 0.149170 0.097409 0.155000 cif_j2_at_t21 0.152773 0.100071 0.158514 cif_j2_at_t22 0.157264 0.103414 0.162875 cif_j2_at_t23 0.160830 0.106091 0.166326 cif_j2_at_t24 0.164348 0.108755 0.169715 cif_j2_at_t25 0.167585 0.111224 0.172822 cif_j2_at_t26 0.171941 0.114576 0.176986 cif_j2_at_t27 0.176614 0.118203 0.181434 cif_j2_at_t28 0.180547 0.121285 0.185162 cif_j2_at_t29 0.184586 0.124476 0.188975 cif_j2_at_t30 0.189511 0.128402 0.193608 <pre><code>pd.DataFrame(index=pd.MultiIndex.from_product([['d=30', 'd=60', 'd=100', 'd=150'],\n                                               ['N=1000', 'N=10000', 'N=100000']], names=['N', 'D']), \n             columns=['Train MSE (oracle)', 'Test MSE (oracle)', 'computation time'])\n</code></pre> Train MSE (oracle) Test MSE (oracle) computation time N D d=30 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=60 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=100 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN d=150 N=1000 NaN NaN NaN N=10000 NaN NaN NaN N=100000 NaN NaN NaN <pre><code>from pydts.examples_utils.plots import compare_beta_models_for_example\nfrom pydts.utils import present_coefs\n\nres_dict = compare_beta_models_for_example(fitter.event_models, new_fitter.event_models,\n                                           real_coef_dict=real_coef_dict)\n\npresent_coefs(res_dict)\n</code></pre> <pre>\n<code>for coef: Alpha\n</code>\n</pre> Lee Ours real a1_1 -0.951613 -0.993701 -1.000000 a2_1 -1.209701 -1.252696 -1.207944 a3_1 -1.347113 -1.390031 -1.329584 a4_1 -1.383203 -1.426362 -1.415888 a5_1 -1.504718 -1.545886 -1.482831 a6_1 -1.464485 -1.506916 -1.537528 a7_1 -1.556570 -1.602185 -1.583773 a8_1 -1.564112 -1.610078 -1.623832 a9_1 -1.701809 -1.746932 -1.659167 a10_1 -1.629386 -1.675599 -1.690776 a11_1 -1.660134 -1.706351 -1.719369 a12_1 -1.699373 -1.745275 -1.745472 a13_1 -1.798048 -1.841762 -1.769485 a14_1 -1.779825 -1.824197 -1.791717 a15_1 -1.864221 -1.905305 -1.812415 a16_1 -1.797205 -1.841176 -1.831777 a17_1 -1.788325 -1.832710 -1.849964 a18_1 -1.969211 -2.017525 -1.867112 a19_1 -1.844675 -1.886645 -1.883332 a20_1 -1.941487 -1.990557 -1.898720 a21_1 -2.072392 -2.119080 -1.913357 a22_1 -1.950505 -1.999757 -1.927313 a23_1 -1.816114 -1.859465 -1.940648 a24_1 -2.053391 -2.100951 -1.953416 a25_1 -2.032329 -2.080507 -1.965663 a26_1 -2.032766 -2.081074 -1.977429 a27_1 -1.899979 -1.950757 -1.988751 a28_1 -2.024438 -2.072971 -1.999661 a29_1 -2.256035 -2.292853 -2.010189 a30_1 -2.041636 -2.089735 -2.020359 a1_2 -1.724923 -1.741469 -1.750000 a2_2 -1.986075 -2.004244 -1.853972 a3_2 -1.882549 -1.891698 -1.914792 a4_2 -1.985604 -2.003929 -1.957944 a5_2 -1.954779 -1.973737 -1.991416 a6_2 -2.071016 -2.086884 -2.018764 a7_2 -2.089997 -2.104953 -2.041887 a8_2 -2.017632 -2.034741 -2.061916 a9_2 -2.140155 -2.150781 -2.079584 a10_2 -2.098609 -2.110933 -2.095388 a11_2 -2.186827 -2.191509 -2.109684 a12_2 -2.123014 -2.131838 -2.122736 a13_2 -2.177759 -2.180693 -2.134742 a14_2 -2.173186 -2.175601 -2.145859 a15_2 -2.154986 -2.157802 -2.156208 a16_2 -2.282404 -2.267452 -2.165888 a17_2 -2.216986 -2.209869 -2.174982 a18_2 -2.434101 -2.440465 -2.183556 a19_2 -2.103665 -2.107629 -2.191666 a20_2 -2.210458 -2.200231 -2.199360 a21_2 -2.456157 -2.457340 -2.206678 a22_2 -2.228180 -2.212729 -2.213656 a23_2 -2.413939 -2.418286 -2.220324 a24_2 -2.400092 -2.404584 -2.226708 a25_2 -2.468446 -2.465084 -2.232831 a26_2 -2.150649 -2.142334 -2.238714 a27_2 -2.040635 -2.045518 -2.244376 a28_2 -2.206609 -2.190062 -2.249831 a29_2 -2.145625 -2.137646 -2.255094 a30_2 -1.898031 -1.913849 -2.260180 <pre>\n<code>for coef: Beta\n</code>\n</pre> Lee Ours real Z1_1 0.180473 0.175630 0.223144 Z2_1 -1.106470 -1.077021 -1.098612 Z3_1 -1.103171 -1.072990 -1.098612 Z4_1 -0.893110 -0.868595 -0.916291 Z5_1 -0.662878 -0.643514 -0.693147 Z1_2 0.043267 0.042164 -0.000000 Z2_2 -1.065630 -1.053325 -1.098612 Z3_2 -1.398985 -1.383865 -1.386294 Z4_2 -1.103417 -1.090288 -1.098612 Z5_2 -0.717390 -0.709322 -0.693147 <pre><code>from pydts.utils import create_df_for_cif_plots\n\ndf_for_plotting = create_df_for_cif_plots(test_df, field=\"Z1\", covariates=new_fitter.covariates,\n                                                     quantiles=[0.25, 0.5, 0.75], zero_others=True )\nour_pred_df = new_fitter.predict_cumulative_incident_function(df_for_plotting)\nour_pred_df = new_fitter.predict_marginal_prob_all_events(our_pred_df)\n\nour_pred_df.head()\n</code></pre> Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.570378 0.370758 0.245091 0.166491 0.115641 ... 0.362827 0.362884 0.362928 0.362975 0.363015 0.363042 0.363063 0.363085 0.636792 0.363085 4839 0.506276 0.0 0.0 0.0 0.0 0.560076 0.359013 0.234264 0.157197 0.107959 ... 0.356779 0.356825 0.356860 0.356897 0.356929 0.356950 0.356966 0.356983 0.642924 0.356983 4839 0.750906 0.0 0.0 0.0 0.0 0.549852 0.347522 0.223807 0.148330 0.100713 ... 0.351012 0.351049 0.351077 0.351107 0.351131 0.351148 0.351161 0.351173 0.648756 0.351173 <p>3 rows \u00d7 217 columns</p> <pre><code>lee_pred_df = fitter.predict_cumulative_incident_function(df_for_plotting)\nlee_pred_df = fitter.predict_marginal_prob_all_events(lee_pred_df)\n\nlee_pred_df.head()\n</code></pre> Z1 Z2 Z3 Z4 Z5 overall_survival_t1 overall_survival_t2 overall_survival_t3 overall_survival_t4 overall_survival_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.559414 0.358099 0.233707 0.156609 0.107446 ... 0.357832 0.357878 0.357912 0.357949 0.357981 0.358001 0.358018 0.358034 0.641872 0.358034 4839 0.506276 0.0 0.0 0.0 0.0 0.548640 0.346007 0.222712 0.147300 0.099849 ... 0.351764 0.351800 0.351827 0.351856 0.351881 0.351896 0.351909 0.351922 0.648008 0.351922 4839 0.750906 0.0 0.0 0.0 0.0 0.537951 0.334190 0.212116 0.138445 0.092709 ... 0.345988 0.346017 0.346038 0.346061 0.346080 0.346092 0.346101 0.346111 0.653837 0.346111 <p>3 rows \u00d7 217 columns</p> <pre><code>from pydts.utils import get_real_hazard\n\ndf_temp = get_real_hazard(df_for_plotting.copy(), real_coef_dict=real_coef_dict, times=times, events=[1,2])\ndf_temp\n</code></pre> Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... hazard_j2_t21 hazard_j2_t22 hazard_j2_t23 hazard_j2_t24 hazard_j2_t25 hazard_j2_t26 hazard_j2_t27 hazard_j2_t28 hazard_j2_t29 hazard_j2_t30 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.099152 0.098531 0.09794 0.097378 0.096841 0.096327 0.095836 0.095364 0.094911 0.094475 <p>3 rows \u00d7 65 columns</p> <pre><code>real_pred_df = new_fitter.predict_cumulative_incident_function(df_temp)\nreal_pred_df = new_fitter.predict_marginal_prob_all_events(real_pred_df)\n\nreal_pred_df.head()\n</code></pre> Z1 Z2 Z3 Z4 Z5 hazard_j1_t1 hazard_j1_t2 hazard_j1_t3 hazard_j1_t4 hazard_j1_t5 ... cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 marginal_prob_j1 marginal_prob_j2 4839 0.255555 0.0 0.0 0.0 0.0 0.280300 0.240321 0.218820 0.204425 0.193753 ... 0.361174 0.361223 0.361260 0.361289 0.361311 0.361329 0.361342 0.361352 0.638562 0.361352 4839 0.506276 0.0 0.0 0.0 0.0 0.291724 0.250683 0.228533 0.213675 0.202643 ... 0.351030 0.351068 0.351096 0.351118 0.351135 0.351148 0.351158 0.351166 0.648772 0.351166 4839 0.750906 0.0 0.0 0.0 0.0 0.303129 0.261076 0.238300 0.222990 0.211606 ... 0.341411 0.341440 0.341462 0.341479 0.341491 0.341501 0.341508 0.341514 0.658441 0.341514 <p>3 rows \u00d7 217 columns</p> <pre><code>import matplotlib.pyplot as plt\nfrom pydts.examples_utils.plots import plot_cif_plots\n\nj_events = 2\npreds = [lee_pred_df, our_pred_df, real_pred_df]\nnames = ['Lee', 'Our', 'Real']\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 20))\n\nsubfigs = fig.subfigures(nrows=len(preds), ncols=1)\nfor row, subfig in enumerate(subfigs):\n    subfig.suptitle(f'CIF for {names[row]} method', fontsize=30, fontweight='bold')\n    pred_df = preds[row]\n    # create 1x3 subplots per subfig\n    axs = subfig.subplots(nrows=1, ncols=j_events)\n    for col, ax in enumerate(axs):\n        ax = plot_cif_plots(pred_df, event=col+1, return_ax=True, ax=ax, pad=0.05, scale=5)\n        h, l = ax.get_legend_handles_labels()\n        ax.legend(h,[\"Z5 - Q25\", \"Z5 - Q50\", \"Z5 - Q75\"], fontsize=15)\n</code></pre> <pre><code>from lifelines.fitters.coxph_fitter import CoxPHFitter\n\nevents = pd.get_dummies(train_df['J']).add_prefix(\"J_\")\nrel_train = train_df.drop(columns=['C', 'T', 'J']).merge(events, how='inner',\n                                                         left_index=True, right_index=True).copy()\nfit_cols = [*covariates, \"X\"]\n\nmodels = {}\nfor event in [1, 2]:\n    model = CoxPHFitter()\n    event_col = f\"J_{event}\"\n    print(event_col)\n    model.fit_right_censoring(df=rel_train[[*fit_cols, event_col]] , duration_col='X', event_col=event_col)\n    model.print_summary()\n    models.update({event: model})\n\n\nrel_times = models[1].timeline.union(models[2].timeline)\n</code></pre> <pre>\n<code>J_1\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'J_1' baseline estimation breslow number of observations 37500 number of events observed 13348 partial log-likelihood -130916.90 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.18 1.19 0.03 0.12 0.23 1.12 1.26 5.87 &lt;0.005 27.77 Z2 -1.08 0.34 0.03 -1.14 -1.02 0.32 0.36 -35.27 &lt;0.005 902.87 Z3 -1.07 0.34 0.03 -1.13 -1.01 0.32 0.36 -34.90 &lt;0.005 884.08 Z4 -0.87 0.42 0.03 -0.93 -0.81 0.40 0.45 -28.54 &lt;0.005 592.75 Z5 -0.64 0.53 0.03 -0.70 -0.58 0.50 0.56 -21.27 &lt;0.005 331.19 Concordance 0.65 Partial AIC 261843.81 log-likelihood ratio test 3537.28 on 5 df -log2(p) of ll-ratio test inf <pre>\n<code>J_2\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'J_2' baseline estimation breslow number of observations 37500 number of events observed 5771 partial log-likelihood -56065.63 time fit was run 2022-07-03 12:24:34 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.04 1.04 0.05 -0.05 0.13 0.95 1.14 0.93 0.35 1.50 Z2 -1.05 0.35 0.05 -1.14 -0.96 0.32 0.38 -22.70 &lt;0.005 376.56 Z3 -1.38 0.25 0.05 -1.48 -1.29 0.23 0.27 -29.25 &lt;0.005 622.51 Z4 -1.09 0.34 0.05 -1.18 -1.00 0.31 0.37 -23.40 &lt;0.005 400.02 Z5 -0.71 0.49 0.05 -0.80 -0.62 0.45 0.54 -15.40 &lt;0.005 175.32 Concordance 0.67 Partial AIC 112141.27 log-likelihood ratio test 2026.96 on 5 df -log2(p) of ll-ratio test inf <p> </p> <pre><code>from pydts.examples_utils.continues_data import hazard_func, survival_func\n\nhazards = hazard_func(models, df_for_plotting)\nsurv_func = survival_func(models, rel_times, df_for_plotting)\n\ncif_1 = pd.DataFrame(hazards[1] * surv_func, index=df_for_plotting.index,\n                     columns=rel_times.astype(int)).cumsum(axis=1)\ncif_2 = pd.DataFrame(hazards[2] * surv_func, index=df_for_plotting.index,\n                     columns=rel_times.astype(int)).cumsum(axis=1)\ncont_cif = pd.merge(\n    cif_1.reset_index(drop=True).add_prefix(\"cif_j1_at_t\"),\n    cif_2.reset_index(drop=True).add_prefix(\"cif_j2_at_t\"),\n    right_index=True, left_index=True\n)\n\ncont_cif\n</code></pre> cif_j1_at_t1 cif_j1_at_t2 cif_j1_at_t3 cif_j1_at_t4 cif_j1_at_t5 cif_j1_at_t6 cif_j1_at_t7 cif_j1_at_t8 cif_j1_at_t9 cif_j1_at_t10 ... cif_j2_at_t21 cif_j2_at_t22 cif_j2_at_t23 cif_j2_at_t24 cif_j2_at_t25 cif_j2_at_t26 cif_j2_at_t27 cif_j2_at_t28 cif_j2_at_t29 cif_j2_at_t30 0 0.056824 0.098869 0.133680 0.165468 0.192327 0.218796 0.241867 0.263644 0.281896 0.300560 ... 0.182505 0.187035 0.190665 0.194228 0.197451 0.201706 0.206258 0.209989 0.213826 0.218531 1 0.059204 0.102907 0.139013 0.171917 0.199668 0.226965 0.250715 0.273095 0.291823 0.310943 ... 0.181421 0.185837 0.189371 0.192836 0.195967 0.200095 0.204506 0.208117 0.211828 0.216372 2 0.061615 0.106988 0.144394 0.178411 0.207048 0.235161 0.259579 0.282547 0.301737 0.321297 ... 0.180266 0.184567 0.188005 0.191371 0.194410 0.198412 0.202682 0.206174 0.209759 0.214143 <p>3 rows \u00d7 60 columns</p> <pre><code>import matplotlib.pyplot as plt\nfrom pydts.examples_utils.plots import plot_cif_plots\n\nj_events = 2\npreds = [lee_pred_df, our_pred_df, real_pred_df, cont_cif]\nnames = ['Lee', 'Our', 'Real', \"Continues\"]\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 20))\n\nsubfigs = fig.subfigures(nrows=len(preds), ncols=1)\nfor row, subfig in enumerate(subfigs):\n    subfig.suptitle(f'CIF for {names[row]} method', fontsize=30, fontweight='bold')\n    pred_df = preds[row]\n    # create 1x3 subplots per subfig\n    axs = subfig.subplots(nrows=1, ncols=j_events)\n    for col, ax in enumerate(axs):\n        ax = plot_cif_plots(pred_df, event=col+1, return_ax=True, ax=ax, pad=0.05, scale=5)\n        h, l = ax.get_legend_handles_labels()\n        ax.legend(h,[\"Z1 - Q25\", \"Z1 - Q50\", \"Z1 - Q75\"], fontsize=15)\n        ax.set_ylim([0, 0.7])\n\nplt.savefig(\"comparison1.jpg\")\n</code></pre> <pre><code>df_for_plotting = create_df_for_cif_plots(test_df, field=\"Z2\", covariates=new_fitter.covariates,\n                                                     quantiles=[0.25, 0.5, 0.75], zero_others=True )\nour_pred_df = new_fitter.predict_cumulative_incident_function(df_for_plotting)\nour_pred_df = new_fitter.predict_marginal_prob_all_events(our_pred_df)\n\nlee_pred_df = fitter.predict_cumulative_incident_function(df_for_plotting)\nlee_pred_df = fitter.predict_marginal_prob_all_events(lee_pred_df)\n\ndf_temp = get_real_hazard(df_for_plotting.copy(), real_coef_dict=real_coef_dict, times=times, events=[1,2])\nreal_pred_df = new_fitter.predict_cumulative_incident_function(df_temp)\nreal_pred_df = new_fitter.predict_marginal_prob_all_events(real_pred_df)\n\nhazards = hazard_func(models, df_for_plotting)\nsurv_func = survival_func(models, rel_times, df_for_plotting)\n\ncif_1 = pd.DataFrame(hazards[1] * surv_func, index=df_for_plotting.index,\n                     columns=rel_times.astype(int)).cumsum(axis=1)\ncif_2 = pd.DataFrame(hazards[2] * surv_func, index=df_for_plotting.index,\n                     columns=rel_times.astype(int)).cumsum(axis=1)\ncont_cif = pd.merge(\n    cif_1.reset_index(drop=True).add_prefix(\"cif_j1_at_t\"),\n    cif_2.reset_index(drop=True).add_prefix(\"cif_j2_at_t\"),\n    right_index=True, left_index=True\n)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom pydts.examples_utils.plots import plot_cif_plots\n\nj_events = 2\npreds = [lee_pred_df, our_pred_df, real_pred_df, cont_cif]\nnames = ['Lee', 'Our', 'Real', \"Continues\"]\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 20))\n\nsubfigs = fig.subfigures(nrows=len(preds), ncols=1)\nfor row, subfig in enumerate(subfigs):\n    subfig.suptitle(f'CIF for {names[row]} method', fontsize=30, fontweight='bold')\n    pred_df = preds[row]\n    # create 1x3 subplots per subfig\n    axs = subfig.subplots(nrows=1, ncols=j_events)\n    for col, ax in enumerate(axs):\n        ax = plot_cif_plots(pred_df, event=col+1, return_ax=True, ax=ax, pad=0.05, scale=5)\n        h, l = ax.get_legend_handles_labels()\n        ax.legend(h,[\"Z2 - Q25\", \"Z2 - Q50\", \"Z2 - Q75\"], fontsize=15)\n        ax.set_ylim([0, 0.7])\n\nplt.savefig(\"comparison2.jpg\")\n</code></pre>"},{"location":"Simple%20Simulation/#simple-example","title":"Simple Example","text":""},{"location":"Simple%20Simulation/#introduction","title":"Introduction","text":""},{"location":"Simple%20Simulation/#data-generation","title":"Data Generation","text":""},{"location":"Simple%20Simulation/#lee-et-al-2018-1","title":"Lee et al. 2018 [1]","text":""},{"location":"Simple%20Simulation/#new-approach","title":"New approach","text":""},{"location":"Simple%20Simulation/#prediction","title":"Prediction","text":""},{"location":"Simple%20Simulation/#comparison-wip","title":"Comparison (WIP)","text":""},{"location":"SimulatedDataset/","title":"Hospitalization LOS Simulation","text":"<p>In this example, we simulate a dataset of 10,000 hospitalized patietns as follows: </p> <p>We sample the covariates such that:</p> <p>\\(\\mbox{Admission Year} \\sim \\mbox{Uniform}(2000,2014)\\)</p> <p>\\(\\mbox{Gender} \\sim \\mbox{Bernoullie}(0.5), \\qquad\\) (1 is Female, 0 is Male)</p> <p>\\(\\mbox{Age} \\sim \\mbox{Normal}(72+5*\\mbox{gender}\\;,\\;12)\\) (years)</p> <p>\\(\\mbox{Height} \\sim \\mbox{Normal}(175-5*\\mbox{gender}\\;,\\;7)\\) (cm) </p> <p>\\(\\mbox{Weight} \\sim \\mbox{Normal}(\\frac{\\mbox{height}}{175}*80 - 5 * \\mbox{gender} + \\frac{\\mbox{age}}{20}\\;,\\;8)\\) (kg)</p> <p>\\(\\mbox{BMI} \\: (\\mbox{Body} \\: \\mbox{Mass} \\: \\mbox{Index}) = \\frac{\\mbox{Weight}}{(\\frac{\\mbox{Height}}{100})^2}\\) (kg/m^2)</p> <p>\\(\\mbox{Admission Serial} \\sim \\mbox{LogNormal}(0, 0.75)\\)</p> <p>\\(\\mbox{Smoking Status} \\sim \\mbox{Multinomial(No, Previously, Currently)} \\quad p=[0.5, 0.3, 0.2]\\)</p> <p>Precondition features:</p> <p>\\(\\mbox{General_p} = 0.003 * \\mbox{bmi} - 0.15 * \\mbox{gender} + 0.002 * \\mbox{age} + 0.1 * \\mbox{smoking}\\)</p> <p>\\(\\mbox{Preconditions_p} = max( min(\\mbox{General_p}, 0.65), 0.05)\\) </p> <p>\\(\\mbox{Hypertension} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\)</p> <p>\\(\\mbox{Diabetes} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.003*\\mbox{BMI})\\)</p> <p>\\(\\mbox{Arterial Fibrillation} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\)</p> <p>\\(\\mbox{Chronic Obstructive Pulmonary Disease} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p} + 0.1*\\mbox{smoking})\\)</p> <p>\\(\\mbox{Chronic Renal Failure} \\sim \\mbox{Bernoulli}(\\mbox{Preconditions_p})\\)</p> <p>Finally, based on the above covariates, we sample LOS and the event type: discharged or in-hospital death.</p> <p>After sampling the LOS, for some patients we remove weight (and BMI) information based on year of admission, to reflect missingness which can occur in real world data.  </p> <pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\npd.set_option('display.max_rows', 500)\nfrom pydts.examples_utils.simulations_data_config import *\nfrom pydts.examples_utils.datasets import load_LOS_simulated_data\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n</code></pre> <pre><code>data_df = load_LOS_simulated_data()\n</code></pre> <p>First, let's look at some descriptive statistics of the columns:</p> <pre><code>paper_table = data_df.describe().T.round(2)\nidx = [r for r in paper_table.index if r not in ['ID', 'Returning_patient', 'Death_date_in_hosp_missing', \n                                                 'In_hospital_death',]]\n\npaper_table = paper_table.loc[idx, :]\npaper_table.rename({\n    'Age': 'Age (years)',\n    'Gender': 'Gender (1 - female)',\n    'Admyear': 'Admyear (year)',\n    'Firstadm': 'Firstadm (1 - yes)',\n    'Admserial': 'Admserial',\n    'Weight': 'Weight (kg)',\n    'Height': 'Height (cm)',\n    'BMI': 'Height (kg/m^2)',\n    'Smoking': f\"Smoking (0 - never, \\\\ \\qquad \\qquad \\, 1 - previously, \\\\ \\qquad \\qquad \\, 2 - currently)\",\n    'Discharge_relative_date': 'LOS (days)', \n    'Hypertension': 'Hypertension (1 - yes)',\n    'Diabities': 'Diabetes (1 - yes)',\n    'AF': 'AF (1 - yes)',\n    'COPD': 'COPD (1 - yes)',\n    'CRF': 'CRF (1 - yes)',\n    'Death_relative_date_in_hosp': 'In-Hospital Death (days)'}, inplace=True)\npaper_table['count'] = paper_table['count'].astype(int) \n</code></pre> <pre><code>paper_table\n</code></pre> count mean std min 25% 50% 75% max Age (years) 10000 74.38 12.12 24.10 66.30 74.40 82.60 122.70 Gender (1 - female) 10000 0.49 0.50 0.00 0.00 0.00 1.00 1.00 Admyear (year) 10000 2006.96 4.29 2000.00 2003.00 2007.00 2011.00 2014.00 Firstadm (1 - yes) 10000 0.70 0.46 0.00 0.00 1.00 1.00 1.00 Admserial 10000 1.50 1.03 1.00 1.00 1.00 2.00 17.00 Weight (kg) 4497 80.05 9.40 46.92 73.66 80.08 86.41 116.46 Height (cm) 10000 172.47 7.41 145.68 167.50 172.41 177.49 201.24 Height (kg/m^2) 4497 26.94 2.97 15.16 24.86 26.89 28.85 40.35 Smoking (0 - never, \\ \\qquad \\qquad \\, 1 - previously, \\ \\qquad \\qquad \\, 2 - currently) 10000 0.70 0.78 0.00 0.00 1.00 1.00 2.00 Hypertension (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 Diabetes 10000 0.31 0.46 0.00 0.00 0.00 1.00 1.00 AF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 COPD (1 - yes) 10000 0.29 0.46 0.00 0.00 0.00 1.00 1.00 CRF (1 - yes) 10000 0.23 0.42 0.00 0.00 0.00 0.00 1.00 LOS (days) 10000 12.96 10.26 1.00 4.00 10.00 21.00 31.00 In-Hospital Death (days) 2362 10.21 7.86 1.00 4.00 8.00 15.00 30.00 <p>When dealing with healthcare data, changes of policy can lead to biased data, so it is also a good idea to see how the data looks like with stratification by year of admission:</p> <pre><code>import tableone as to\ncolumns = [AGE_COL, GENDER_COL, WEIGHT_COL, HEIGHT_COL, BMI_COL, *preconditions, RETURNING_PATIENT_COL]\ncategorical = [GENDER_COL, *preconditions, RETURNING_PATIENT_COL]\ngroupby = ADMISSION_YEAR_COL\nmytable = to.TableOne(data_df, columns, categorical, groupby)\nmytable\n</code></pre> Grouped by Admyear Missing Overall 2000.0 2001.0 2002.0 2003.0 2004.0 2005.0 2006.0 2007.0 2008.0 2009.0 2010.0 2011.0 2012.0 2013.0 2014.0 n 10000 637 659 724 657 671 687 661 711 669 678 655 648 631 662 650 Age, mean (SD) 0 74.4 (12.1) 73.8 (12.4) 74.3 (11.8) 74.7 (12.1) 74.8 (12.0) 74.3 (11.7) 73.9 (11.9) 74.5 (13.1) 73.7 (11.9) 74.8 (12.0) 74.8 (12.1) 75.6 (11.6) 73.8 (12.4) 74.6 (11.9) 73.8 (12.5) 74.2 (12.1) Gender, n (%) 0.0 0 5064 (50.6) 307 (48.2) 349 (53.0) 362 (50.0) 317 (48.2) 346 (51.6) 339 (49.3) 350 (53.0) 367 (51.6) 327 (48.9) 355 (52.4) 343 (52.4) 331 (51.1) 318 (50.4) 332 (50.2) 321 (49.4) 1.0 4936 (49.4) 330 (51.8) 310 (47.0) 362 (50.0) 340 (51.8) 325 (48.4) 348 (50.7) 311 (47.0) 344 (48.4) 342 (51.1) 323 (47.6) 312 (47.6) 317 (48.9) 313 (49.6) 330 (49.8) 329 (50.6) Weight, mean (SD) 5503 80.1 (9.4) 78.4 (10.5) 80.1 (11.6) 81.2 (9.9) 79.8 (9.7) 78.1 (9.6) 80.0 (9.7) 79.8 (9.2) 80.7 (8.7) 80.9 (9.4) 80.9 (9.4) 79.9 (9.2) 80.4 (9.3) 79.6 (9.4) 79.8 (9.7) 79.8 (8.9) Height, mean (SD) 0 172.5 (7.4) 172.7 (7.5) 172.4 (7.8) 172.4 (7.4) 172.4 (7.1) 172.3 (7.3) 172.1 (7.5) 172.5 (7.5) 172.7 (7.2) 172.4 (7.3) 172.9 (7.6) 172.6 (7.5) 172.7 (7.5) 172.3 (7.1) 172.5 (7.7) 172.3 (7.3) BMI, mean (SD) 5503 26.9 (3.0) 26.7 (3.4) 27.0 (3.1) 27.0 (3.0) 26.7 (2.9) 26.4 (3.1) 26.9 (3.2) 26.9 (3.0) 27.2 (2.8) 27.2 (3.1) 27.2 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) 26.9 (3.0) 26.9 (2.9) Smoking, n (%) 0.0 0 4982 (49.8) 323 (50.7) 349 (53.0) 371 (51.2) 325 (49.5) 332 (49.5) 334 (48.6) 325 (49.2) 352 (49.5) 319 (47.7) 336 (49.6) 310 (47.3) 305 (47.1) 327 (51.8) 322 (48.6) 352 (54.2) 1.0 3033 (30.3) 185 (29.0) 177 (26.9) 228 (31.5) 193 (29.4) 195 (29.1) 220 (32.0) 193 (29.2) 214 (30.1) 221 (33.0) 198 (29.2) 223 (34.0) 211 (32.6) 174 (27.6) 206 (31.1) 195 (30.0) 2.0 1985 (19.9) 129 (20.3) 133 (20.2) 125 (17.3) 139 (21.2) 144 (21.5) 133 (19.4) 143 (21.6) 145 (20.4) 129 (19.3) 144 (21.2) 122 (18.6) 132 (20.4) 130 (20.6) 134 (20.2) 103 (15.8) Hypertension, n (%) 0.0 0 7717 (77.2) 505 (79.3) 497 (75.4) 545 (75.3) 523 (79.6) 517 (77.0) 552 (80.3) 482 (72.9) 551 (77.5) 519 (77.6) 520 (76.7) 523 (79.8) 488 (75.3) 478 (75.8) 505 (76.3) 512 (78.8) 1.0 2283 (22.8) 132 (20.7) 162 (24.6) 179 (24.7) 134 (20.4) 154 (23.0) 135 (19.7) 179 (27.1) 160 (22.5) 150 (22.4) 158 (23.3) 132 (20.2) 160 (24.7) 153 (24.2) 157 (23.7) 138 (21.2) Diabetes, n (%) 0.0 0 6932 (69.3) 446 (70.0) 444 (67.4) 512 (70.7) 454 (69.1) 457 (68.1) 491 (71.5) 463 (70.0) 491 (69.1) 464 (69.4) 464 (68.4) 444 (67.8) 442 (68.2) 430 (68.1) 453 (68.4) 477 (73.4) 1.0 3068 (30.7) 191 (30.0) 215 (32.6) 212 (29.3) 203 (30.9) 214 (31.9) 196 (28.5) 198 (30.0) 220 (30.9) 205 (30.6) 214 (31.6) 211 (32.2) 206 (31.8) 201 (31.9) 209 (31.6) 173 (26.6) AF, n (%) 0.0 0 7735 (77.3) 493 (77.4) 500 (75.9) 574 (79.3) 524 (79.8) 527 (78.5) 540 (78.6) 504 (76.2) 551 (77.5) 519 (77.6) 527 (77.7) 487 (74.4) 480 (74.1) 471 (74.6) 518 (78.2) 520 (80.0) 1.0 2265 (22.7) 144 (22.6) 159 (24.1) 150 (20.7) 133 (20.2) 144 (21.5) 147 (21.4) 157 (23.8) 160 (22.5) 150 (22.4) 151 (22.3) 168 (25.6) 168 (25.9) 160 (25.4) 144 (21.8) 130 (20.0) COPD, n (%) 0.0 0 7068 (70.7) 450 (70.6) 468 (71.0) 503 (69.5) 472 (71.8) 469 (69.9) 484 (70.5) 467 (70.7) 512 (72.0) 465 (69.5) 462 (68.1) 453 (69.2) 453 (69.9) 440 (69.7) 481 (72.7) 489 (75.2) 1.0 2932 (29.3) 187 (29.4) 191 (29.0) 221 (30.5) 185 (28.2) 202 (30.1) 203 (29.5) 194 (29.3) 199 (28.0) 204 (30.5) 216 (31.9) 202 (30.8) 195 (30.1) 191 (30.3) 181 (27.3) 161 (24.8) CRF, n (%) 0.0 0 7729 (77.3) 486 (76.3) 513 (77.8) 589 (81.4) 510 (77.6) 505 (75.3) 522 (76.0) 511 (77.3) 555 (78.1) 522 (78.0) 500 (73.7) 501 (76.5) 508 (78.4) 491 (77.8) 501 (75.7) 515 (79.2) 1.0 2271 (22.7) 151 (23.7) 146 (22.2) 135 (18.6) 147 (22.4) 166 (24.7) 165 (24.0) 150 (22.7) 156 (21.9) 147 (22.0) 178 (26.3) 154 (23.5) 140 (21.6) 140 (22.2) 161 (24.3) 135 (20.8) Returning_patient, n (%) 0 0 7048 (70.5) 448 (70.3) 473 (71.8) 505 (69.8) 464 (70.6) 489 (72.9) 503 (73.2) 456 (69.0) 477 (67.1) 470 (70.3) 480 (70.8) 464 (70.8) 452 (69.8) 458 (72.6) 456 (68.9) 453 (69.7) 1 2744 (27.4) 176 (27.6) 173 (26.3) 200 (27.6) 183 (27.9) 171 (25.5) 168 (24.5) 192 (29.0) 218 (30.7) 187 (28.0) 186 (27.4) 176 (26.9) 181 (27.9) 161 (25.5) 189 (28.5) 183 (28.2) 2 185 (1.8) 12 (1.9) 13 (2.0) 17 (2.3) 7 (1.1) 9 (1.3) 15 (2.2) 12 (1.8) 13 (1.8) 11 (1.6) 9 (1.3) 12 (1.8) 14 (2.2) 12 (1.9) 16 (2.4) 13 (2.0) 3 23 (0.2) 1 (0.2) 2 (0.3) 3 (0.5) 2 (0.3) 1 (0.1) 1 (0.2) 3 (0.4) 1 (0.1) 3 (0.4) 3 (0.5) 1 (0.2) 1 (0.2) 1 (0.2) <p>Let's visualize the data. With the following figures we can see:</p> <p>(a) How many patients were hospitalized in total, how many were discharged\\died, stratified by year of admission.</p> <p>(b) Age distributions by sex, males (0) and females (1).</p> <p>(c) Number of patients at each number of admissions, with a separation to 4 groups.</p> <p>(d) Kaplan-Meier curves for LOS with and without death as censoring</p> <pre><code>from pydts.examples_utils.plots import *\nplot_LOS_simulation_figure1(data_df)\n</code></pre> <p>Next, with the following figures we can further visualize the possible outcomes:</p> <p>(a) Description of the events (death and release).</p> <p>(b) Distribution of age, by sex, among the patients who died.</p> <p>(c-d) Number of observed event, by event type. </p> <pre><code>plot_LOS_simulation_figure2(data_df)\n</code></pre> <p>and a visualization of the missingness of the weight variable by year of admission:</p> <pre><code>plot_LOS_simulation_figure3(data_df)\n</code></pre> <pre><code>outcome_cols = [\n    DISCHARGE_RELATIVE_COL,\n    IN_HOSPITAL_DEATH_COL,\n    DEATH_RELATIVE_COL, \n    DEATH_MISSING_COL\n]\n\ny = data_df.set_index(PATIENT_NO_COL)[outcome_cols]\nX = data_df.set_index(PATIENT_NO_COL).drop(columns=outcome_cols)\nX.drop([ADMISSION_SERIAL_COL, FIRST_ADMISSION_COL], axis=1, inplace=True)\n</code></pre> <p>We search for missing data and use median imputation: </p> <pre><code>to_impute = X.isna().sum(axis=0).to_frame(\"value\").query(\"value &amp;gt; 0\").index\nto_impute\n</code></pre> <pre>\n<code>Index(['Weight', 'BMI'], dtype='object')</code>\n</pre> <pre><code>from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(verbose=1, strategy='median')\nX[to_impute] = imputer.fit_transform(X[to_impute])\n</code></pre> <p>In some applications it is customize to standardize the covariates, such that each will be with the mean 0 and standard deviation of 1.</p> <p>For Height, Weight, Age and BMI columns we use Standard scaling, and for Returning Patient and Smoking we use Min-Max scaling:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nto_normalize = [HEIGHT_COL, WEIGHT_COL, AGE_COL, BMI_COL]\nto_minmax = [RETURNING_PATIENT_COL, SMOKING_COL]\n\nstd_scaler = StandardScaler() \nX[to_normalize] = std_scaler.fit_transform(X[to_normalize])\n\nminmax_scaler = MinMaxScaler()\nX[to_minmax] = minmax_scaler.fit_transform(X[to_minmax])\n\nX.head()\n</code></pre> Age Gender Admyear Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient ID 0.0 -0.501435 1.0 2001.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 1.0 -1.829968 1.0 2003.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.216468 1.0 2013.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.356748 1.0 2008.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 4.0 -0.509686 0.0 2011.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 <p>Creating event type and event time:</p> <p>In_hospital_death = 1 means in hospital death (J=1)</p> <p>In_hospital_death = 0 with Discharge_relative_date &lt;= 30 means a discharge event (J=2)</p> <p>Discharge_relative_date = 31 means right censored example, i.e. (J=0 at T=30)</p> <pre><code>y.loc[((y.In_hospital_death == 0) &amp;amp; (y.Discharge_relative_date != 31)), IN_HOSPITAL_DEATH_COL] = 2\ny[DISCHARGE_RELATIVE_COL] = y[DISCHARGE_RELATIVE_COL].clip(upper=30).astype(int)\ny\n</code></pre> Discharge_relative_date In_hospital_death Death_relative_date_in_hosp Death_date_in_hosp_missing ID 0.0 11 2 NaN 1 1.0 15 2 NaN 1 2.0 6 2 NaN 1 3.0 4 2 NaN 1 4.0 1 1 1.0 0 ... ... ... ... ... 9995.0 10 1 10.0 0 9996.0 5 1 5.0 0 9997.0 1 1 1.0 0 9998.0 13 2 NaN 1 9999.0 30 0 NaN 1 <p>10000 rows \u00d7 4 columns</p> <p>Now we can estimate the parameters of the model using a TwoStagesFitter:</p> <pre><code>fit_df = pd.concat([X.drop(ADMISSION_YEAR_COL, axis=1), \n                    y[[IN_HOSPITAL_DEATH_COL, DISCHARGE_RELATIVE_COL]]], axis=1).reset_index()\nfit_df.head()\n</code></pre> ID Age Gender Weight Height BMI Smoking Hypertension Diabetes AF COPD CRF Returning_patient In_hospital_death Discharge_relative_date 0 0.0 -0.501435 1.0 0.001731 0.198481 -0.010699 0.5 0.0 0.0 0.0 1.0 0.0 0.0 2 11 1 1.0 -1.829968 1.0 0.001731 0.255134 -0.010699 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 15 2 2.0 0.216468 1.0 -0.706832 0.715064 -1.496554 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 6 3 3.0 0.356748 1.0 0.001731 -0.738724 -0.010699 1.0 0.0 1.0 1.0 1.0 0.0 0.0 2 4 4 4.0 -0.509686 0.0 0.001731 -0.610285 -0.010699 0.5 1.0 1.0 0.0 0.0 0.0 0.0 1 1 <pre><code>from pydts.fitters import TwoStagesFitter\nfitter = TwoStagesFitter()\nfitter.fit(df=fit_df, event_type_col=IN_HOSPITAL_DEATH_COL, duration_col=DISCHARGE_RELATIVE_COL, \n           pid_col=PATIENT_NO_COL)\nfitter.print_summary()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt In_hospital_death Discharge_relative_date 1 1 212 True -3.901826 2 184 True -3.950053 3 174 True -3.925112 4 138 True -4.085360 5 139 True -3.992844 6 149 True -3.848771 7 128 True -3.922036 8 120 True -3.908204 9 81 True -4.240269 10 95 True -4.018168 11 82 True -4.095930 12 88 True -3.938631 13 74 True -4.050635 14 65 True -4.108051 15 60 True -4.120188 16 43 True -4.377010 17 56 True -4.067527 18 48 True -4.157987 19 52 True -4.023369 20 49 True -4.016882 21 37 True -4.229897 22 35 True -4.222299 23 42 True -3.979030 24 38 True -4.007955 25 29 True -4.217928 26 36 True -3.923879 27 33 True -3.954917 28 30 True -3.978909 29 27 True -4.023108 30 18 True -4.347235 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt In_hospital_death Discharge_relative_date 2 1 628 True -2.755714 2 515 True -2.869952 3 466 True -2.889717 4 383 True -3.012315 5 393 True -2.908478 6 330 True -3.011169 7 332 True -2.923762 8 251 True -3.136989 9 242 True -3.104749 10 232 True -3.082277 11 247 True -2.942857 12 215 True -3.012719 13 209 True -2.966362 14 174 True -3.083501 15 163 True -3.082805 16 145 True -3.138006 17 134 True -3.160356 18 111 True -3.290827 19 123 True -3.127649 20 114 True -3.138881 21 112 True -3.090689 22 105 True -3.093272 23 107 True -3.007764 24 77 True -3.276149 25 99 True -2.956023 26 82 True -3.082226 27 80 True -3.037766 28 59 True -3.280251 29 73 True -2.997231 30 69 True -2.985297 <pre><code>fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Age 0.012234 0.021289 0.018675 0.013089 Gender 0.072557 0.048761 0.032534 0.029895 Weight -0.085851 0.039237 -0.096352 0.024137 Height -0.109585 0.028974 -0.057043 0.017838 BMI -0.113686 0.038058 -0.069863 0.023324 Smoking 0.006120 0.060227 -0.086379 0.037199 Hypertension -0.066527 0.052036 0.021493 0.031215 Diabetes 0.045579 0.045944 0.034855 0.028225 AF 0.042571 0.051099 -0.005837 0.031592 COPD 0.030174 0.049191 0.058040 0.030069 CRF 0.024396 0.051150 0.039778 0.031179 Returning_patient -0.097966 0.122227 0.122055 0.073013 <pre><code>from pydts.examples_utils.plots import add_panel_text\n</code></pre> <pre><code>fig, axes = plt.subplots(1,2, figsize=(18,8))\nax = axes[0]\nfitter.plot_all_events_alpha(ax=ax, show=False)\nax.grid()\nax.legend(fontsize=16, loc='center right')\nadd_panel_text(ax, 'a')\nax = axes[1]\nfitter.plot_all_events_beta(ax=ax, show=False, xlabel='Value')\nax.legend(fontsize=16, loc='center right')\nadd_panel_text(ax, 'b')\nfig.tight_layout()\n</code></pre> <pre><code>pred_df = fitter.predict_cumulative_incident_function(fit_df.iloc[2:5]).T\npred_df = pred_df.iloc[1:]\npred_df.columns = ['ID=2', 'ID=3', 'ID=4']\npred_df.head()\n</code></pre> ID=2 ID=3 ID=4 Age 0.216468 0.356748 -0.509686 Gender 1.000000 1.000000 0.000000 Weight -0.706832 0.001731 0.001731 Height 0.715064 -0.738724 -0.610285 BMI -1.496554 -0.010699 -0.010699 <pre><code>plot_example_pred_output(pred_df)\n</code></pre>"},{"location":"SimulatedDataset/#hospitalization-length-of-stay","title":"Hospitalization length of stay","text":""},{"location":"SimulatedDataset/#loading-simulation-dataset","title":"Loading Simulation Dataset","text":""},{"location":"SimulatedDataset/#data-description","title":"Data Description","text":""},{"location":"SimulatedDataset/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"SimulatedDataset/#missing-values-imputation","title":"Missing Values Imputation","text":""},{"location":"SimulatedDataset/#standardization","title":"Standardization","text":""},{"location":"SimulatedDataset/#estimation","title":"Estimation","text":""},{"location":"SimulatedDataset/#prediction","title":"Prediction","text":""},{"location":"UsageExample-DataPreparation/","title":"Data Preparation","text":"<p>For simplicity of presentation, we considered \\(M=2\\) competing events, though PyDTS can handle any number of competing events as long as there are enough observed failures of each failure type, at each discrete time point.</p> <p>Here, \\(d=30\\) discrete time points, \\(n=50,000\\) observations, and \\(Z\\) with 5 covariates. Failure times of observations, denoted as \\(T\\), were generated based on the model:</p> \\[ \\lambda_{j}(t|Z) = \\frac{\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})}{1+\\exp(\\alpha_{jt}+Z^{T}\\beta_{j})} \\] <p>with </p> <p>\\(\\alpha_{1t} = -1 -0.3 \\log(t)\\), </p> <p>\\(\\alpha_{2t} = -1.75 -0.15\\log(t)\\), \\(t=1,\\ldots,d\\),</p> <p>\\(\\beta_1 = (-\\log 0.8, \\log 3, \\log 3, \\log 2.5, \\log 2)\\), </p> <p>\\(\\beta_{2} = (-\\log 1, \\log 3, \\log 4, \\log 3, \\log 2)\\). </p> <p>Censoring time for each observation was sampled from a discrete uniform distribution, i.e. \\(C_i \\sim \\mbox{Uniform}\\{1,...,d+1\\}\\). The last observed time \\(X\\) is calculated as \\(X_i = \\min(T_i, C_i)\\), and \\(J\\) is the event-type with \\(J_i=0\\) if and only if \\(C_i &lt; T_i\\).</p> <p>Our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) (70 parameters in total) along with the standard error of the estimators.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\nn_patients = 50000\nn_cov = 5\n</code></pre> <pre><code>patients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, censoring_prob=0.8, \n                                      real_coef_dict=real_coef_dict)\n\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 <p>Both estimation methods require enough observed failures of each failure type, at each discrete time point. Therefore, the first step is to make sure this is in fact the case with the data at hand.</p> <p>As shown below, in our example, the data comply with this requirement. </p> <p>Preprocessing suggestions for cases when the data do not comply with this requirement are shown in Data Regrouping Example.</p> <pre><code>patients_df.groupby(['J', 'X'])['pid'].count().unstack('J')\n</code></pre> J 0 1 2 X 1 1236 3374 1250 2 1124 2328 839 3 1029 1805 805 4 972 1524 644 5 939 1214 570 6 889 1114 483 7 830 916 416 8 832 830 409 9 797 683 323 10 685 626 306 11 703 569 240 12 648 516 246 13 679 419 226 14 647 410 198 15 603 326 170 16 601 320 162 17 585 280 147 18 564 240 115 19 505 243 125 20 465 204 118 21 488 176 83 22 465 167 89 23 497 166 65 24 457 118 59 25 440 114 58 26 427 109 53 27 430 89 43 28 396 70 38 29 398 67 43 30 3245 47 37"},{"location":"UsageExample-DataPreparation/#data-generation","title":"Data Generation","text":""},{"location":"UsageExample-DataPreparation/#checking-the-data","title":"Checking the Data","text":""},{"location":"UsageExample-FittingDataExpansionFitter-FULL/","title":"UsageExample FittingDataExpansionFitter FULL","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nfrom pydts.examples_utils.plots import plot_events_occurrence, plot_example_pred_output\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\nn_patients = 50000\nn_cov = 5\n</code></pre> <pre><code>patients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, censoring_prob=0.8, \n                                      real_coef_dict=real_coef_dict)\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 <pre><code>plot_events_occurrence(patients_df[['pid', 'J', 'X']])\n</code></pre> <pre>\n<code>&lt;AxesSubplot:xlabel='Time', ylabel='Number of Observations'&gt;</code>\n</pre> <p>In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'.</p> <pre><code>from pydts.fitters import DataExpansionFitter\nfitter = DataExpansionFitter()\nfitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\n\nfitter.print_summary()\n</code></pre> <pre>\n<code>\n\nModel summary for event: 1\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_1   No. Observations:               536780\nModel:                            GLM   Df Residuals:                   536745\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -78272.\nDate:                Tue, 02 Aug 2022   Deviance:                   1.5654e+05\nTime:                        16:47:21   Pearson chi2:                 5.35e+05\nNo. Iterations:                     7   Pseudo R-squ. (CS):            0.01509\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -0.9459      0.033    -28.924      0.000      -1.010      -0.882\nC(X)[2]       -1.1780      0.035    -33.675      0.000      -1.247      -1.109\nC(X)[3]       -1.3158      0.037    -35.614      0.000      -1.388      -1.243\nC(X)[4]       -1.3671      0.039    -35.452      0.000      -1.443      -1.291\nC(X)[5]       -1.4895      0.041    -36.429      0.000      -1.570      -1.409\nC(X)[6]       -1.4702      0.042    -35.004      0.000      -1.553      -1.388\nC(X)[7]       -1.5688      0.044    -35.325      0.000      -1.656      -1.482\nC(X)[8]       -1.5724      0.046    -34.301      0.000      -1.662      -1.483\nC(X)[9]       -1.6733      0.049    -34.334      0.000      -1.769      -1.578\nC(X)[10]      -1.6693      0.050    -33.240      0.000      -1.768      -1.571\nC(X)[11]      -1.6748      0.052    -32.246      0.000      -1.777      -1.573\nC(X)[12]      -1.6825      0.054    -31.287      0.000      -1.788      -1.577\nC(X)[13]      -1.8026      0.058    -31.121      0.000      -1.916      -1.689\nC(X)[14]      -1.7319      0.058    -29.610      0.000      -1.847      -1.617\nC(X)[15]      -1.8695      0.064    -29.319      0.000      -1.994      -1.745\nC(X)[16]      -1.7987      0.064    -27.960      0.000      -1.925      -1.673\nC(X)[17]      -1.8400      0.068    -27.122      0.000      -1.973      -1.707\nC(X)[18]      -1.9016      0.072    -26.333      0.000      -2.043      -1.760\nC(X)[19]      -1.7936      0.072    -24.918      0.000      -1.935      -1.653\nC(X)[20]      -1.8749      0.077    -24.232      0.000      -2.027      -1.723\nC(X)[21]      -1.9294      0.082    -23.424      0.000      -2.091      -1.768\nC(X)[22]      -1.8858      0.084    -22.362      0.000      -2.051      -1.721\nC(X)[23]      -1.7888      0.085    -21.123      0.000      -1.955      -1.623\nC(X)[24]      -2.0205      0.098    -20.568      0.000      -2.213      -1.828\nC(X)[25]      -1.9474      0.100    -19.500      0.000      -2.143      -1.752\nC(X)[26]      -1.8743      0.102    -18.373      0.000      -2.074      -1.674\nC(X)[27]      -1.9588      0.112    -17.518      0.000      -2.178      -1.740\nC(X)[28]      -2.0736      0.125    -16.608      0.000      -2.318      -1.829\nC(X)[29]      -1.9838      0.128    -15.552      0.000      -2.234      -1.734\nC(X)[30]      -2.1912      0.151    -14.550      0.000      -2.486      -1.896\nZ1             0.1930      0.026      7.495      0.000       0.143       0.244\nZ2            -1.1306      0.026    -42.971      0.000      -1.182      -1.079\nZ3            -1.1237      0.026    -42.515      0.000      -1.176      -1.072\nZ4            -0.8986      0.026    -34.377      0.000      -0.950      -0.847\nZ5            -0.6720      0.026    -25.869      0.000      -0.723      -0.621\n==============================================================================\n\n\nModel summary for event: 2\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_2   No. Observations:               536780\nModel:                            GLM   Df Residuals:                   536745\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -41269.\nDate:                Tue, 02 Aug 2022   Deviance:                       82537.\nTime:                        16:47:22   Pearson chi2:                 5.39e+05\nNo. Iterations:                     8   Pseudo R-squ. (CS):           0.006763\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -1.7207      0.049    -35.253      0.000      -1.816      -1.625\nC(X)[2]       -1.9635      0.053    -36.941      0.000      -2.068      -1.859\nC(X)[3]       -1.8726      0.054    -34.671      0.000      -1.978      -1.767\nC(X)[4]       -1.9732      0.057    -34.515      0.000      -2.085      -1.861\nC(X)[5]       -1.9804      0.059    -33.427      0.000      -2.096      -1.864\nC(X)[6]       -2.0393      0.062    -32.819      0.000      -2.161      -1.918\nC(X)[7]       -2.0853      0.065    -32.085      0.000      -2.213      -1.958\nC(X)[8]       -2.0027      0.066    -30.546      0.000      -2.131      -1.874\nC(X)[9]       -2.1411      0.071    -30.347      0.000      -2.279      -2.003\nC(X)[10]      -2.1014      0.072    -29.209      0.000      -2.242      -1.960\nC(X)[11]      -2.2544      0.078    -28.862      0.000      -2.408      -2.101\nC(X)[12]      -2.1354      0.078    -27.505      0.000      -2.288      -1.983\nC(X)[13]      -2.1257      0.080    -26.538      0.000      -2.283      -1.969\nC(X)[14]      -2.1671      0.084    -25.786      0.000      -2.332      -2.002\nC(X)[15]      -2.2224      0.089    -24.964      0.000      -2.397      -2.048\nC(X)[16]      -2.1811      0.091    -24.026      0.000      -2.359      -2.003\nC(X)[17]      -2.1826      0.094    -23.134      0.000      -2.368      -1.998\nC(X)[18]      -2.3342      0.104    -22.438      0.000      -2.538      -2.130\nC(X)[19]      -2.1546      0.101    -21.382      0.000      -2.352      -1.957\nC(X)[20]      -2.1133      0.103    -20.467      0.000      -2.316      -1.911\nC(X)[21]      -2.3724      0.119    -19.867      0.000      -2.606      -2.138\nC(X)[22]      -2.2038      0.116    -18.983      0.000      -2.431      -1.976\nC(X)[23]      -2.4194      0.133    -18.207      0.000      -2.680      -2.159\nC(X)[24]      -2.3982      0.139    -17.275      0.000      -2.670      -2.126\nC(X)[25]      -2.3070      0.140    -16.480      0.000      -2.581      -2.033\nC(X)[26]      -2.2794      0.146    -15.630      0.000      -2.565      -1.994\nC(X)[27]      -2.3684      0.160    -14.774      0.000      -2.683      -2.054\nC(X)[28]      -2.3635      0.170    -13.926      0.000      -2.696      -2.031\nC(X)[29]      -2.1045      0.161    -13.103      0.000      -2.419      -1.790\nC(X)[30]      -2.1030      0.172    -12.215      0.000      -2.440      -1.766\nZ1             0.0411      0.038      1.074      0.283      -0.034       0.116\nZ2            -1.1128      0.039    -28.419      0.000      -1.190      -1.036\nZ3            -1.4255      0.040    -35.870      0.000      -1.503      -1.348\nZ4            -1.1106      0.039    -28.398      0.000      -1.187      -1.034\nZ5            -0.6620      0.039    -17.135      0.000      -0.738      -0.586\n==============================================================================\n</code>\n</pre> <pre><code>summary = fitter.event_models[1].summary()\nsummary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\nsummary_df.columns = summary_df.iloc[0]\nsummary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\nbeta1_summary = summary_df.iloc[-5:]\n</code></pre> <pre><code>summary = fitter.event_models[2].summary()\nsummary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\nsummary_df.columns = summary_df.iloc[0]\nsummary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\nbeta2_summary = summary_df.iloc[-5:]\nbeta2_summary\n</code></pre> coef std err z P&gt;|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 <pre><code>from pydts.examples_utils.plots import plot_first_model_coefs\nplot_first_model_coefs(models=fitter.event_models, times=fitter.times, train_df=patients_df, n_cov=5)\n</code></pre> <p>Full prediction is given by the method <code>predict_full()</code></p> <p>The input is a <code>pandas.DataFrame()</code> containing for each observation the covariates columns which were used in the <code>fit()</code> method (\\(Z1-Z5\\) in our example).</p> <p>The following columns will be added:</p> <ol> <li>The overall survival at each time point t</li> <li>The hazard for each failure type \\(j\\) at each time point t</li> <li>The probability of event type \\(j\\) at time t</li> <li>The Cumulative Incident Function (CIF) of event type \\(j\\) at time t</li> </ol> <p>In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view.</p> <pre><code>pred_df = fitter.predict_full(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df.index.name = ''\npred_df.columns = ['ID=0', 'ID=1', 'ID=2']\n</code></pre> <pre><code>plot_example_pred_output(pred_df)\n</code></pre> <pre><code>pred_df\n</code></pre> ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520"},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#estimating-with-dataexpansionfitter","title":"Estimating with DataExpansionFitter","text":""},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#estimation","title":"Estimation","text":""},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#standard-errors","title":"Standard Errors","text":""},{"location":"UsageExample-FittingDataExpansionFitter-FULL/#prediction","title":"Prediction","text":""},{"location":"UsageExample-FittingDataExpansionFitter/","title":"Estimation with DataExpansionFitter","text":"<p>In the following we apply the estimation method of Lee et al. (2018). Note that the data dataframe must not contain a column named 'C'.</p> <pre><code>from pydts.fitters import DataExpansionFitter\nfitter = DataExpansionFitter()\nfitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\n\nfitter.print_summary()\n</code></pre> <pre>\n<code>\n\nModel summary for event: 1\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_1   No. Observations:               536780\nModel:                            GLM   Df Residuals:                   536745\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -78272.\nDate:                Tue, 02 Aug 2022   Deviance:                   1.5654e+05\nTime:                        16:47:21   Pearson chi2:                 5.35e+05\nNo. Iterations:                     7   Pseudo R-squ. (CS):            0.01509\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -0.9459      0.033    -28.924      0.000      -1.010      -0.882\nC(X)[2]       -1.1780      0.035    -33.675      0.000      -1.247      -1.109\nC(X)[3]       -1.3158      0.037    -35.614      0.000      -1.388      -1.243\nC(X)[4]       -1.3671      0.039    -35.452      0.000      -1.443      -1.291\nC(X)[5]       -1.4895      0.041    -36.429      0.000      -1.570      -1.409\nC(X)[6]       -1.4702      0.042    -35.004      0.000      -1.553      -1.388\nC(X)[7]       -1.5688      0.044    -35.325      0.000      -1.656      -1.482\nC(X)[8]       -1.5724      0.046    -34.301      0.000      -1.662      -1.483\nC(X)[9]       -1.6733      0.049    -34.334      0.000      -1.769      -1.578\nC(X)[10]      -1.6693      0.050    -33.240      0.000      -1.768      -1.571\nC(X)[11]      -1.6748      0.052    -32.246      0.000      -1.777      -1.573\nC(X)[12]      -1.6825      0.054    -31.287      0.000      -1.788      -1.577\nC(X)[13]      -1.8026      0.058    -31.121      0.000      -1.916      -1.689\nC(X)[14]      -1.7319      0.058    -29.610      0.000      -1.847      -1.617\nC(X)[15]      -1.8695      0.064    -29.319      0.000      -1.994      -1.745\nC(X)[16]      -1.7987      0.064    -27.960      0.000      -1.925      -1.673\nC(X)[17]      -1.8400      0.068    -27.122      0.000      -1.973      -1.707\nC(X)[18]      -1.9016      0.072    -26.333      0.000      -2.043      -1.760\nC(X)[19]      -1.7936      0.072    -24.918      0.000      -1.935      -1.653\nC(X)[20]      -1.8749      0.077    -24.232      0.000      -2.027      -1.723\nC(X)[21]      -1.9294      0.082    -23.424      0.000      -2.091      -1.768\nC(X)[22]      -1.8858      0.084    -22.362      0.000      -2.051      -1.721\nC(X)[23]      -1.7888      0.085    -21.123      0.000      -1.955      -1.623\nC(X)[24]      -2.0205      0.098    -20.568      0.000      -2.213      -1.828\nC(X)[25]      -1.9474      0.100    -19.500      0.000      -2.143      -1.752\nC(X)[26]      -1.8743      0.102    -18.373      0.000      -2.074      -1.674\nC(X)[27]      -1.9588      0.112    -17.518      0.000      -2.178      -1.740\nC(X)[28]      -2.0736      0.125    -16.608      0.000      -2.318      -1.829\nC(X)[29]      -1.9838      0.128    -15.552      0.000      -2.234      -1.734\nC(X)[30]      -2.1912      0.151    -14.550      0.000      -2.486      -1.896\nZ1             0.1930      0.026      7.495      0.000       0.143       0.244\nZ2            -1.1306      0.026    -42.971      0.000      -1.182      -1.079\nZ3            -1.1237      0.026    -42.515      0.000      -1.176      -1.072\nZ4            -0.8986      0.026    -34.377      0.000      -0.950      -0.847\nZ5            -0.6720      0.026    -25.869      0.000      -0.723      -0.621\n==============================================================================\n\n\nModel summary for event: 2\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    j_2   No. Observations:               536780\nModel:                            GLM   Df Residuals:                   536745\nModel Family:                Binomial   Df Model:                           34\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -41269.\nDate:                Tue, 02 Aug 2022   Deviance:                       82537.\nTime:                        16:47:22   Pearson chi2:                 5.39e+05\nNo. Iterations:                     8   Pseudo R-squ. (CS):           0.006763\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nC(X)[1]       -1.7207      0.049    -35.253      0.000      -1.816      -1.625\nC(X)[2]       -1.9635      0.053    -36.941      0.000      -2.068      -1.859\nC(X)[3]       -1.8726      0.054    -34.671      0.000      -1.978      -1.767\nC(X)[4]       -1.9732      0.057    -34.515      0.000      -2.085      -1.861\nC(X)[5]       -1.9804      0.059    -33.427      0.000      -2.096      -1.864\nC(X)[6]       -2.0393      0.062    -32.819      0.000      -2.161      -1.918\nC(X)[7]       -2.0853      0.065    -32.085      0.000      -2.213      -1.958\nC(X)[8]       -2.0027      0.066    -30.546      0.000      -2.131      -1.874\nC(X)[9]       -2.1411      0.071    -30.347      0.000      -2.279      -2.003\nC(X)[10]      -2.1014      0.072    -29.209      0.000      -2.242      -1.960\nC(X)[11]      -2.2544      0.078    -28.862      0.000      -2.408      -2.101\nC(X)[12]      -2.1354      0.078    -27.505      0.000      -2.288      -1.983\nC(X)[13]      -2.1257      0.080    -26.538      0.000      -2.283      -1.969\nC(X)[14]      -2.1671      0.084    -25.786      0.000      -2.332      -2.002\nC(X)[15]      -2.2224      0.089    -24.964      0.000      -2.397      -2.048\nC(X)[16]      -2.1811      0.091    -24.026      0.000      -2.359      -2.003\nC(X)[17]      -2.1826      0.094    -23.134      0.000      -2.368      -1.998\nC(X)[18]      -2.3342      0.104    -22.438      0.000      -2.538      -2.130\nC(X)[19]      -2.1546      0.101    -21.382      0.000      -2.352      -1.957\nC(X)[20]      -2.1133      0.103    -20.467      0.000      -2.316      -1.911\nC(X)[21]      -2.3724      0.119    -19.867      0.000      -2.606      -2.138\nC(X)[22]      -2.2038      0.116    -18.983      0.000      -2.431      -1.976\nC(X)[23]      -2.4194      0.133    -18.207      0.000      -2.680      -2.159\nC(X)[24]      -2.3982      0.139    -17.275      0.000      -2.670      -2.126\nC(X)[25]      -2.3070      0.140    -16.480      0.000      -2.581      -2.033\nC(X)[26]      -2.2794      0.146    -15.630      0.000      -2.565      -1.994\nC(X)[27]      -2.3684      0.160    -14.774      0.000      -2.683      -2.054\nC(X)[28]      -2.3635      0.170    -13.926      0.000      -2.696      -2.031\nC(X)[29]      -2.1045      0.161    -13.103      0.000      -2.419      -1.790\nC(X)[30]      -2.1030      0.172    -12.215      0.000      -2.440      -1.766\nZ1             0.0411      0.038      1.074      0.283      -0.034       0.116\nZ2            -1.1128      0.039    -28.419      0.000      -1.190      -1.036\nZ3            -1.4255      0.040    -35.870      0.000      -1.503      -1.348\nZ4            -1.1106      0.039    -28.398      0.000      -1.187      -1.034\nZ5            -0.6620      0.039    -17.135      0.000      -0.738      -0.586\n==============================================================================\n</code>\n</pre> <pre><code>summary = fitter.event_models[1].summary()\nsummary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\nsummary_df.columns = summary_df.iloc[0]\nsummary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\nbeta1_summary = summary_df.iloc[-5:]\n</code></pre> <pre><code>summary = fitter.event_models[2].summary()\nsummary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\nsummary_df.columns = summary_df.iloc[0]\nsummary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\nbeta2_summary = summary_df.iloc[-5:]\nbeta2_summary\n</code></pre> coef std err z P&gt;|z| [0.025 0.975] Z1 0.0411 0.038 1.074 0.283 -0.034 0.116 Z2 -1.1128 0.039 -28.419 0.000 -1.190 -1.036 Z3 -1.4255 0.040 -35.870 0.000 -1.503 -1.348 Z4 -1.1106 0.039 -28.398 0.000 -1.187 -1.034 Z5 -0.6620 0.039 -17.135 0.000 -0.738 -0.586 <pre><code>from pydts.examples_utils.plots import plot_first_model_coefs\nplot_first_model_coefs(models=fitter.event_models, times=fitter.times, train_df=patients_df, n_cov=5)\n</code></pre> <p>Full prediction is given by the method <code>predict_full()</code></p> <p>The input is a <code>pandas.DataFrame()</code> containing for each observation the covariates columns which were used in the <code>fit()</code> method (\\(Z1-Z5\\) in our example).</p> <p>The following columns will be added:</p> <ol> <li>The overall survival at each time point t</li> <li>The hazard for each failure type \\(j\\) at each time point t</li> <li>The probability of event type \\(j\\) at time t</li> <li>The Cumulative Incident Function (CIF) of event type \\(j\\) at time t</li> </ol> <p>In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view.</p> <pre><code>pred_df = fitter.predict_full(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df.index.name = ''\npred_df.columns = ['ID=0', 'ID=1', 'ID=2']\n</code></pre> <pre><code>plot_example_pred_output(pred_df)\n</code></pre> <pre><code>pred_df\n</code></pre> ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.942684 0.960628 0.932938 overall_survival_t2 0.899636 0.930545 0.883002 overall_survival_t3 0.861480 0.903726 0.839277 overall_survival_t4 0.827201 0.879254 0.800236 overall_survival_t5 0.797018 0.857533 0.766167 overall_survival_t6 0.768048 0.836364 0.733620 overall_survival_t7 0.742313 0.817381 0.704914 overall_survival_t8 0.716876 0.798470 0.676759 overall_survival_t9 0.694881 0.781915 0.652527 overall_survival_t10 0.673241 0.765482 0.628832 overall_survival_t11 0.653276 0.750077 0.607015 overall_survival_t12 0.633323 0.734605 0.585385 overall_survival_t13 0.615405 0.720649 0.566115 overall_survival_t14 0.597401 0.706425 0.546797 overall_survival_t15 0.581732 0.693972 0.530095 overall_survival_t16 0.565528 0.680961 0.512891 overall_survival_t17 0.550207 0.668566 0.496713 overall_survival_t18 0.536576 0.657400 0.482353 overall_survival_t19 0.521450 0.644937 0.466518 overall_survival_t20 0.507307 0.633237 0.451823 overall_survival_t21 0.495118 0.622977 0.439151 overall_survival_t22 0.482190 0.612062 0.425808 overall_survival_t23 0.469586 0.601189 0.412767 overall_survival_t24 0.459059 0.592128 0.401980 overall_survival_t25 0.447933 0.582483 0.390629 overall_survival_t26 0.436435 0.572414 0.378937 overall_survival_t27 0.426143 0.563323 0.368512 overall_survival_t28 0.416810 0.555060 0.359122 overall_survival_t29 0.406209 0.545669 0.348543 overall_survival_t30 0.397051 0.537568 0.339489 hazard_j1_t1 0.043097 0.031017 0.051717 hazard_j1_t10 0.021381 0.015290 0.025774 hazard_j1_t11 0.021267 0.015208 0.025637 hazard_j1_t12 0.021106 0.015092 0.025444 hazard_j1_t13 0.018764 0.013408 0.022632 hazard_j1_t14 0.020110 0.014376 0.024248 hazard_j1_t15 0.017570 0.012551 0.021198 hazard_j1_t16 0.018835 0.013460 0.022717 hazard_j1_t17 0.018086 0.012922 0.021818 hazard_j1_t18 0.017025 0.012160 0.020542 hazard_j1_t19 0.018930 0.013528 0.022831 hazard_j1_t2 0.034478 0.024750 0.041448 hazard_j1_t20 0.017476 0.012484 0.021085 hazard_j1_t21 0.016565 0.011830 0.019989 hazard_j1_t22 0.017291 0.012351 0.020862 hazard_j1_t23 0.019019 0.013592 0.022938 hazard_j1_t24 0.015144 0.010811 0.018280 hazard_j1_t25 0.016275 0.011622 0.019640 hazard_j1_t26 0.017487 0.012491 0.021097 hazard_j1_t27 0.016094 0.011491 0.019422 hazard_j1_t28 0.014374 0.010258 0.017352 hazard_j1_t29 0.015702 0.011211 0.018951 hazard_j1_t3 0.030175 0.021634 0.036308 hazard_j1_t30 0.012799 0.009130 0.015457 hazard_j1_t4 0.028709 0.020575 0.034555 hazard_j1_t5 0.025486 0.018248 0.030696 hazard_j1_t6 0.025969 0.018596 0.031275 hazard_j1_t7 0.023589 0.016880 0.028423 hazard_j1_t8 0.023506 0.016820 0.028323 hazard_j1_t9 0.021298 0.015231 0.025675 hazard_j2_t1 0.014218 0.008355 0.015345 hazard_j2_t10 0.009761 0.005725 0.010538 hazard_j2_t11 0.008387 0.004917 0.009056 hazard_j2_t12 0.009438 0.005535 0.010190 hazard_j2_t13 0.009528 0.005588 0.010287 hazard_j2_t14 0.009146 0.005363 0.009875 hazard_j2_t15 0.008657 0.005076 0.009348 hazard_j2_t16 0.009020 0.005289 0.009739 hazard_j2_t17 0.009006 0.005281 0.009724 hazard_j2_t18 0.007749 0.004542 0.008368 hazard_j2_t19 0.009260 0.005430 0.009998 hazard_j2_t2 0.011188 0.006566 0.012077 hazard_j2_t20 0.009647 0.005658 0.010415 hazard_j2_t21 0.007461 0.004372 0.008057 hazard_j2_t22 0.008819 0.005171 0.009522 hazard_j2_t23 0.007121 0.004172 0.007690 hazard_j2_t24 0.007273 0.004261 0.007853 hazard_j2_t25 0.007961 0.004666 0.008596 hazard_j2_t26 0.008182 0.004796 0.008835 hazard_j2_t27 0.007490 0.004389 0.008088 hazard_j2_t28 0.007527 0.004411 0.008128 hazard_j2_t29 0.009730 0.005707 0.010505 hazard_j2_t3 0.012239 0.007186 0.013211 hazard_j2_t30 0.009745 0.005716 0.010521 hazard_j2_t4 0.011081 0.006503 0.011962 hazard_j2_t5 0.011003 0.006457 0.011878 hazard_j2_t6 0.010379 0.006089 0.011205 hazard_j2_t7 0.009917 0.005817 0.010707 hazard_j2_t8 0.010762 0.006315 0.011618 hazard_j2_t9 0.009384 0.005504 0.010132 prob_j1_at_t1 0.043097 0.031017 0.051717 prob_j1_at_t2 0.032501 0.023776 0.038668 prob_j1_at_t3 0.027146 0.020132 0.032060 prob_j1_at_t4 0.024733 0.018594 0.029001 prob_j1_at_t5 0.021082 0.016044 0.024564 prob_j1_at_t6 0.020698 0.015947 0.023962 prob_j1_at_t7 0.018118 0.014118 0.020852 prob_j1_at_t8 0.017449 0.013749 0.019965 prob_j1_at_t9 0.015268 0.012161 0.017376 prob_j1_at_t10 0.014857 0.011956 0.016819 prob_j1_at_t11 0.014318 0.011641 0.016121 prob_j1_at_t12 0.013788 0.011321 0.015445 prob_j1_at_t13 0.011884 0.009850 0.013248 prob_j1_at_t14 0.012376 0.010360 0.013727 prob_j1_at_t15 0.010497 0.008867 0.011591 prob_j1_at_t16 0.010957 0.009341 0.012042 prob_j1_at_t17 0.010228 0.008799 0.011190 prob_j1_at_t18 0.009367 0.008130 0.010204 prob_j1_at_t19 0.010157 0.008893 0.011013 prob_j1_at_t20 0.009113 0.008051 0.009836 prob_j1_at_t21 0.008404 0.007491 0.009032 prob_j1_at_t22 0.008561 0.007694 0.009162 prob_j1_at_t23 0.009171 0.008319 0.009767 prob_j1_at_t24 0.007112 0.006499 0.007545 prob_j1_at_t25 0.007471 0.006881 0.007895 prob_j1_at_t26 0.007833 0.007276 0.008241 prob_j1_at_t27 0.007024 0.006578 0.007360 prob_j1_at_t28 0.006125 0.005779 0.006395 prob_j1_at_t29 0.006545 0.006223 0.006806 prob_j1_at_t30 0.005199 0.004982 0.005387 prob_j2_at_t1 0.014218 0.008355 0.015345 prob_j2_at_t2 0.010546 0.006308 0.011267 prob_j2_at_t3 0.011010 0.006687 0.011665 prob_j2_at_t4 0.009546 0.005877 0.010040 prob_j2_at_t5 0.009101 0.005677 0.009505 prob_j2_at_t6 0.008272 0.005222 0.008585 prob_j2_at_t7 0.007617 0.004865 0.007855 prob_j2_at_t8 0.007989 0.005162 0.008190 prob_j2_at_t9 0.006727 0.004394 0.006857 prob_j2_at_t10 0.006783 0.004477 0.006877 prob_j2_at_t11 0.005647 0.003764 0.005695 prob_j2_at_t12 0.006165 0.004152 0.006185 prob_j2_at_t13 0.006035 0.004105 0.006022 prob_j2_at_t14 0.005628 0.003865 0.005590 prob_j2_at_t15 0.005172 0.003586 0.005111 prob_j2_at_t16 0.005247 0.003670 0.005162 prob_j2_at_t17 0.005093 0.003596 0.004987 prob_j2_at_t18 0.004264 0.003036 0.004156 prob_j2_at_t19 0.004969 0.003570 0.004822 prob_j2_at_t20 0.005030 0.003649 0.004859 prob_j2_at_t21 0.003785 0.002769 0.003640 prob_j2_at_t22 0.004366 0.003221 0.004181 prob_j2_at_t23 0.003434 0.002554 0.003274 prob_j2_at_t24 0.003415 0.002562 0.003242 prob_j2_at_t25 0.003655 0.002763 0.003456 prob_j2_at_t26 0.003665 0.002794 0.003451 prob_j2_at_t27 0.003269 0.002513 0.003065 prob_j2_at_t28 0.003208 0.002485 0.002995 prob_j2_at_t29 0.004056 0.003168 0.003773 prob_j2_at_t30 0.003958 0.003119 0.003667 cif_j1_at_t1 0.043097 0.031017 0.051717 cif_j1_at_t2 0.075599 0.054792 0.090385 cif_j1_at_t3 0.102745 0.074924 0.122445 cif_j1_at_t4 0.127478 0.093518 0.151447 cif_j1_at_t5 0.148560 0.109563 0.176011 cif_j1_at_t6 0.169258 0.125510 0.199972 cif_j1_at_t7 0.187375 0.139628 0.220824 cif_j1_at_t8 0.204824 0.153376 0.240789 cif_j1_at_t9 0.220092 0.165537 0.258165 cif_j1_at_t10 0.234950 0.177493 0.274983 cif_j1_at_t11 0.249267 0.189135 0.291105 cif_j1_at_t12 0.263055 0.200455 0.306550 cif_j1_at_t13 0.274939 0.210305 0.319798 cif_j1_at_t14 0.287314 0.220665 0.333525 cif_j1_at_t15 0.297811 0.229531 0.345116 cif_j1_at_t16 0.308768 0.238872 0.357158 cif_j1_at_t17 0.318996 0.247671 0.368348 cif_j1_at_t18 0.328364 0.255801 0.378552 cif_j1_at_t19 0.338521 0.264694 0.389565 cif_j1_at_t20 0.347634 0.272745 0.399401 cif_j1_at_t21 0.356038 0.280236 0.408432 cif_j1_at_t22 0.364599 0.287931 0.417594 cif_j1_at_t23 0.373770 0.296250 0.427361 cif_j1_at_t24 0.380881 0.302749 0.434907 cif_j1_at_t25 0.388352 0.309630 0.442802 cif_j1_at_t26 0.396185 0.316906 0.451043 cif_j1_at_t27 0.403209 0.323484 0.458403 cif_j1_at_t28 0.409334 0.329263 0.464797 cif_j1_at_t29 0.415879 0.335485 0.471603 cif_j1_at_t30 0.421078 0.340468 0.476990 cif_j2_at_t1 0.014218 0.008355 0.015345 cif_j2_at_t2 0.024765 0.014663 0.026612 cif_j2_at_t3 0.035775 0.021350 0.038278 cif_j2_at_t4 0.045321 0.027227 0.048317 cif_j2_at_t5 0.054422 0.032905 0.057822 cif_j2_at_t6 0.062695 0.038126 0.066407 cif_j2_at_t7 0.070311 0.042992 0.074262 cif_j2_at_t8 0.078300 0.048154 0.082451 cif_j2_at_t9 0.085027 0.052548 0.089308 cif_j2_at_t10 0.091810 0.057025 0.096185 cif_j2_at_t11 0.097457 0.060789 0.101880 cif_j2_at_t12 0.103622 0.064940 0.108065 cif_j2_at_t13 0.109657 0.069046 0.114087 cif_j2_at_t14 0.115285 0.072911 0.119677 cif_j2_at_t15 0.120457 0.076496 0.124789 cif_j2_at_t16 0.125704 0.080167 0.129951 cif_j2_at_t17 0.130797 0.083763 0.134938 cif_j2_at_t18 0.135061 0.086799 0.139095 cif_j2_at_t19 0.140029 0.090369 0.143917 cif_j2_at_t20 0.145060 0.094018 0.148776 cif_j2_at_t21 0.148845 0.096787 0.152416 cif_j2_at_t22 0.153211 0.100008 0.156598 cif_j2_at_t23 0.156645 0.102561 0.159872 cif_j2_at_t24 0.160060 0.105123 0.163114 cif_j2_at_t25 0.163714 0.107886 0.166569 cif_j2_at_t26 0.167379 0.110680 0.170020 cif_j2_at_t27 0.170648 0.113192 0.173085 cif_j2_at_t28 0.173856 0.115677 0.176081 cif_j2_at_t29 0.177912 0.118845 0.179853 cif_j2_at_t30 0.181870 0.121964 0.183520"},{"location":"UsageExample-FittingDataExpansionFitter/#estimating-with-dataexpansionfitter","title":"Estimating with DataExpansionFitter","text":""},{"location":"UsageExample-FittingDataExpansionFitter/#estimation","title":"Estimation","text":""},{"location":"UsageExample-FittingDataExpansionFitter/#standard-errors","title":"Standard Errors","text":""},{"location":"UsageExample-FittingDataExpansionFitter/#prediction","title":"Prediction","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/","title":"UsageExample FittingTwoStagesFitter FULL","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nfrom pydts.examples_utils.plots import plot_example_pred_output\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\nn_patients = 50000\nn_cov = 5\n</code></pre> <pre><code>patients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=30, j_events=2, \n                                      pid_col='pid', seed=0, censoring_prob=0.8, \n                                      real_coef_dict=real_coef_dict)\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 0 30 10 10 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 0 30 24 24 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 0 17 11 11 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 1 30 1 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 0 15 14 14 <p>In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'.</p> <pre><code>from pydts.fitters import TwoStagesFitter\nnew_fitter = TwoStagesFitter()\nnew_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\nnew_fitter.print_summary()\n</code></pre> <pre>\n<code></code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 <p>The model summary table for \\(\\beta_j\\) reports the estimated parameters for each risk along with their standard errors. The model summary table for \\(\\alpha_{jt}\\) reports, for each \\(J\\) and \\(X\\), the number of observed events (column \\(n_{jt}\\)), the status of the optimization procedure (column success), and the estimated parameter \\(\\alpha_{jt}\\).</p> <pre><code>from pydts.examples_utils.plots import plot_second_model_coefs\nplot_second_model_coefs(new_fitter.alpha_df, new_fitter.beta_models, new_fitter.times, n_cov=5)\n</code></pre> <pre><code>new_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 <p>It is possible to add regularization when estimating the Beta coefficients. It is done by using the <code>CoxPHFitter</code> (lifelines) penalizer and <code>l1_ratio</code> arguments, which can be passed using the <code>fit_beta_kwargs</code> argument to the <code>fit()</code> method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed.</p> <pre><code>L1_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 1\n    }\n}\n\nL1_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL1_regularized_fitter.get_beta_SE()\n</code></pre> <pre>\n<code></code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132 <pre><code>L2_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 0\n    }\n}\n\nL2_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter.get_beta_SE()\n</code></pre> <pre>\n<code></code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911 <pre><code>EN_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 0.5\n    }\n}\n\nEN_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nEN_regularized_fitter.get_beta_SE()\n</code></pre> <pre>\n<code></code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382 <p>The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates \\(Z1\\), \\(Z2\\) can be done as follows:</p> <pre><code>L2_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': np.array([0.04, 0.04, 0, 0, 0]),\n        'l1_ratio': 0\n    }\n}\n\nL2_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter.get_beta_SE()\n</code></pre> <pre>\n<code></code>\n</pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161 <p>Full prediction is given by the method <code>predict_full()</code></p> <p>The input is a <code>pandas.DataFrame()</code> containing for each observation the covariates columns which were used in the <code>fit()</code> method (\\(Z1\\)-\\(Z5\\) in our example).</p> <p>The following columns will be added:</p> <ol> <li>The overall survival at each time point t</li> <li>The hazard for each failure type \\(j\\) at each time point t</li> <li>The probability of event type \\(j\\) at time t</li> <li>The Cumulative Incident Function (CIF) of event type \\(j\\) at time t</li> </ol> <p>Note that each of the above can be added either separately or for specific times/events if not all columns are required. For example, see partial functions such as <code>predict_prob_events()</code>, <code>predict_hazard_all()</code>, and others in the API documentation.</p> <p>In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view.</p> <pre><code>pred_df = new_fitter.predict_full(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df.index.name = ''\npred_df.columns = ['ID=0', 'ID=1', 'ID=2']\n</code></pre> <pre><code>plot_example_pred_output(pred_df)\n</code></pre> <pre><code>pred_df\n</code></pre> ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850"},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#estimating-with-twostagesfitter","title":"Estimating with TwoStagesFitter","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#estimation","title":"Estimation","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#standard-error-of-the-regression-coefficients","title":"Standard Error of the Regression Coefficients","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#regularization","title":"Regularization","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#l1","title":"L1","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#l2","title":"L2","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#elastic-net","title":"Elastic Net","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#separated-penalty-coefficients","title":"Separated Penalty Coefficients","text":""},{"location":"UsageExample-FittingTwoStagesFitter-FULL/#prediction","title":"Prediction","text":""},{"location":"UsageExample-FittingTwoStagesFitter/","title":"Estimation with TwoStagesFitter","text":"<p>In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'.</p> <pre><code>from pydts.fitters import TwoStagesFitter\nnew_fitter = TwoStagesFitter()\nnew_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\nnew_fitter.print_summary()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 3374 True -0.987702 2 2328 True -1.220809 3 1805 True -1.358580 4 1524 True -1.409997 5 1214 True -1.530437 6 1114 True -1.511889 7 916 True -1.614043 8 830 True -1.618019 9 683 True -1.718359 10 626 True -1.714668 11 569 True -1.720344 12 516 True -1.728207 13 419 True -1.845399 14 410 True -1.776981 15 326 True -1.909345 16 320 True -1.841848 17 280 True -1.881339 18 240 True -1.950204 19 243 True -1.837087 20 204 True -1.914093 21 176 True -1.978425 22 167 True -1.935467 23 166 True -1.832599 24 118 True -2.068397 25 114 True -1.996911 26 109 True -1.925090 27 89 True -2.008449 28 70 True -2.120056 29 67 True -2.033129 30 47 True -2.231271 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 1250 True -1.737087 2 839 True -1.981763 3 805 True -1.881945 4 644 True -1.991485 5 570 True -1.998569 6 483 True -2.055976 7 416 True -2.099660 8 409 True -2.019652 9 323 True -2.150486 10 306 True -2.112509 11 240 True -2.250577 12 246 True -2.142076 13 226 True -2.132065 14 198 True -2.168557 15 170 True -2.215715 16 162 True -2.178298 17 147 True -2.178342 18 115 True -2.346988 19 125 True -2.151499 20 118 True -2.113865 21 83 True -2.380588 22 89 True -2.190208 23 65 True -2.421944 24 59 True -2.401785 25 58 True -2.318061 26 53 True -2.291874 27 43 True -2.373117 28 38 True -2.368179 29 43 True -2.115566 30 37 True -2.113986 <p>The model summary table for \\(\\beta_j\\) reports the estimated parameters for each risk along with their standard errors. The model summary table for \\(\\alpha_{jt}\\) reports, for each \\(J\\) and \\(X\\), the number of observed events (column \\(n_{jt}\\)), the status of the optimization procedure (column success), and the estimated parameter \\(\\alpha_{jt}\\).</p> <pre><code>from pydts.examples_utils.plots import plot_second_model_coefs\nplot_second_model_coefs(new_fitter.alpha_df, new_fitter.beta_models, new_fitter.times, n_cov=5)\n</code></pre> <pre><code>new_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.187949 0.025068 0.040169 0.037807 Z2 -1.100792 0.025610 -1.100246 0.038696 Z3 -1.093466 0.025726 -1.410202 0.039280 Z4 -0.874521 0.025437 -1.097849 0.038642 Z5 -0.652655 0.025280 -0.654501 0.038179 <p>It is possible to add regularization when estimating the Beta coefficients. It is done by using the <code>CoxPHFitter</code> (lifelines) penalizer and <code>l1_ratio</code> arguments, which can be passed using the <code>fit_beta_kwargs</code> argument to the <code>fit()</code> method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_ratio}}{2}||\\beta||_{2}^{2} + \\mbox{L1_ratio} ||\\beta||_1 \\Bigg) $$ Examples for adding L1, L2 and Elastic Net regularization are followed.</p> <pre><code>L1_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 1\n    }\n}\n\nL1_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL1_regularized_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.000002 0.000102 5.690226e-08 0.000041 Z2 -0.774487 0.025401 -3.574822e-01 0.038251 Z3 -0.762942 0.025533 -6.516077e-01 0.038510 Z4 -0.552172 0.025318 -3.590965e-01 0.038235 Z5 -0.340120 0.025211 -1.435430e-06 0.000132 <pre><code>L2_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 0\n    }\n}\n\nL2_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.172957 0.024069 0.032774 0.034626 Z2 -1.007262 0.024506 -0.903957 0.035205 Z3 -1.000509 0.024629 -1.162132 0.035589 Z4 -0.799488 0.024384 -0.903531 0.035177 Z5 -0.597079 0.024255 -0.537159 0.034911 <pre><code>EN_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': 0.003,\n        'l1_ratio': 0.5\n    }\n}\n\nEN_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nEN_regularized_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.039322 0.024542 0.000001 0.000190 Z2 -0.895581 0.024938 -0.654614 0.036595 Z3 -0.886332 0.025065 -0.928867 0.036941 Z4 -0.680998 0.024832 -0.655263 0.036573 Z5 -0.473818 0.024711 -0.265356 0.036382 <p>The above methods can be applied with a separate penalty coefficient to each of the covariates by passing a vector (with same length as the number of covariates) to the penalizer keyword instead of a scalar. For example, applying L2 regularization only to covariates \\(Z1\\), \\(Z2\\) can be done as follows:</p> <pre><code>L2_regularized_fitter = TwoStagesFitter()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'penalizer': np.array([0.04, 0.04, 0, 0, 0]),\n        'l1_ratio': 0\n    }\n}\n\nL2_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.088314 0.017178 0.011120 0.020019 Z2 -0.515292 0.017378 -0.306194 0.020269 Z3 -1.069182 0.025695 -1.374391 0.039205 Z4 -0.853807 0.025419 -1.066715 0.038602 Z5 -0.641989 0.025272 -0.637811 0.038161 <p>Full prediction is given by the method <code>predict_full()</code></p> <p>The input is a <code>pandas.DataFrame()</code> containing for each observation the covariates columns which were used in the <code>fit()</code> method (\\(Z1\\)-\\(Z5\\) in our example).</p> <p>The following columns will be added:</p> <ol> <li>The overall survival at each time point t</li> <li>The hazard for each failure type \\(j\\) at each time point t</li> <li>The probability of event type \\(j\\) at time t</li> <li>The Cumulative Incident Function (CIF) of event type \\(j\\) at time t</li> </ol> <p>Note that each of the above can be added either separately or for specific times/events if not all columns are required. For example, see partial functions such as <code>predict_prob_events()</code>, <code>predict_hazard_all()</code>, and others in the API documentation.</p> <p>In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view.</p> <pre><code>pred_df = new_fitter.predict_full(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df.index.name = ''\npred_df.columns = ['ID=0', 'ID=1', 'ID=2']\n</code></pre> <pre><code>from pydts.examples_utils.plots import plot_example_pred_outputplot_example_pred_output(pred_df)\n</code></pre> <pre><code>pred_df\n</code></pre> ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.941845 0.959704 0.932244 overall_survival_t2 0.898252 0.928975 0.881883 overall_survival_t3 0.859546 0.901548 0.837705 overall_survival_t4 0.824888 0.876595 0.798379 overall_survival_t5 0.794348 0.854430 0.764029 overall_survival_t6 0.765050 0.832843 0.731231 overall_survival_t7 0.739087 0.813536 0.702371 overall_survival_t8 0.713461 0.794332 0.674101 overall_survival_t9 0.691252 0.777486 0.649717 overall_survival_t10 0.669427 0.760785 0.625897 overall_survival_t11 0.649221 0.745080 0.603897 overall_survival_t12 0.629094 0.729356 0.582149 overall_survival_t13 0.610989 0.715148 0.562735 overall_survival_t14 0.592808 0.700678 0.543291 overall_survival_t15 0.576893 0.687938 0.526379 overall_survival_t16 0.560503 0.674682 0.509031 overall_survival_t17 0.544986 0.662036 0.492696 overall_survival_t18 0.531334 0.650765 0.478355 overall_survival_t19 0.516058 0.638087 0.462409 overall_survival_t20 0.501756 0.626164 0.447587 overall_survival_t21 0.489552 0.615813 0.434935 overall_survival_t22 0.476519 0.604735 0.421524 overall_survival_t23 0.463848 0.593720 0.408451 overall_survival_t24 0.453297 0.584569 0.397667 overall_survival_t25 0.442188 0.574864 0.386360 overall_survival_t26 0.430726 0.564748 0.374731 overall_survival_t27 0.420435 0.555592 0.364334 overall_survival_t28 0.411084 0.547251 0.354946 overall_survival_t29 0.400506 0.537811 0.344410 overall_survival_t30 0.391319 0.529622 0.335339 hazard_j1_t1 0.043775 0.031795 0.052250 hazard_j1_t10 0.021649 0.015626 0.025957 hazard_j1_t11 0.021529 0.015539 0.025814 hazard_j1_t12 0.021364 0.015419 0.025617 hazard_j1_t13 0.019047 0.013737 0.022849 hazard_j1_t14 0.020368 0.014696 0.024427 hazard_j1_t15 0.017888 0.012897 0.021464 hazard_j1_t16 0.019113 0.013785 0.022928 hazard_j1_t17 0.018387 0.013259 0.022060 hazard_j1_t18 0.017184 0.012387 0.020622 hazard_j1_t19 0.019202 0.013850 0.023035 hazard_j1_t2 0.034991 0.025352 0.041840 hazard_j1_t20 0.017805 0.012837 0.021364 hazard_j1_t21 0.016714 0.012047 0.020060 hazard_j1_t22 0.017435 0.012569 0.020922 hazard_j1_t23 0.019287 0.013912 0.023136 hazard_j1_t24 0.015298 0.011022 0.018365 hazard_j1_t25 0.016413 0.011829 0.019700 hazard_j1_t26 0.017613 0.012698 0.021135 hazard_j1_t27 0.016227 0.011695 0.019478 hazard_j1_t28 0.014539 0.010472 0.017457 hazard_j1_t29 0.015838 0.011413 0.019012 hazard_j1_t3 0.030625 0.022161 0.036652 hazard_j1_t30 0.013028 0.009381 0.015648 hazard_j1_t4 0.029135 0.021074 0.034880 hazard_j1_t5 0.025915 0.018728 0.031045 hazard_j1_t6 0.026387 0.019071 0.031608 hazard_j1_t7 0.023886 0.017251 0.028626 hazard_j1_t8 0.023794 0.017184 0.028516 hazard_j1_t9 0.021571 0.015569 0.025864 hazard_j2_t1 0.014380 0.008500 0.015506 hazard_j2_t10 0.009924 0.005855 0.010704 hazard_j2_t11 0.008655 0.005104 0.009337 hazard_j2_t12 0.009637 0.005686 0.010396 hazard_j2_t13 0.009733 0.005743 0.010499 hazard_j2_t14 0.009388 0.005538 0.010127 hazard_j2_t15 0.008959 0.005284 0.009665 hazard_j2_t16 0.009298 0.005485 0.010030 hazard_j2_t17 0.009297 0.005484 0.010029 hazard_j2_t18 0.007866 0.004637 0.008486 hazard_j2_t19 0.009548 0.005633 0.010299 hazard_j2_t2 0.011294 0.006668 0.012181 hazard_j2_t20 0.009910 0.005848 0.010690 hazard_j2_t21 0.007608 0.004485 0.008208 hazard_j2_t22 0.009189 0.005420 0.009912 hazard_j2_t23 0.007302 0.004304 0.007878 hazard_j2_t24 0.007450 0.004391 0.008037 hazard_j2_t25 0.008095 0.004773 0.008733 hazard_j2_t26 0.008308 0.004899 0.008963 hazard_j2_t27 0.007665 0.004518 0.008269 hazard_j2_t28 0.007702 0.004540 0.008310 hazard_j2_t29 0.009894 0.005838 0.010672 hazard_j2_t3 0.012465 0.007363 0.013443 hazard_j2_t30 0.009909 0.005847 0.010689 hazard_j2_t4 0.011186 0.006604 0.012065 hazard_j2_t5 0.011108 0.006557 0.011981 hazard_j2_t6 0.010495 0.006194 0.011320 hazard_j2_t7 0.010051 0.005931 0.010841 hazard_j2_t8 0.010879 0.006422 0.011734 hazard_j2_t9 0.009558 0.005638 0.010310 prob_j1_at_t1 0.043775 0.031795 0.052250 prob_j1_at_t2 0.032956 0.024330 0.039005 prob_j1_at_t3 0.027509 0.020587 0.032323 prob_j1_at_t4 0.025043 0.018999 0.029219 prob_j1_at_t5 0.021377 0.016416 0.024785 prob_j1_at_t6 0.020961 0.016295 0.024149 prob_j1_at_t7 0.018274 0.014368 0.020932 prob_j1_at_t8 0.017586 0.013980 0.020029 prob_j1_at_t9 0.015390 0.012367 0.017435 prob_j1_at_t10 0.014965 0.012149 0.016865 prob_j1_at_t11 0.014412 0.011822 0.016157 prob_j1_at_t12 0.013870 0.011488 0.015470 prob_j1_at_t13 0.011982 0.010019 0.013301 prob_j1_at_t14 0.012445 0.010510 0.013746 prob_j1_at_t15 0.010604 0.009037 0.011661 prob_j1_at_t16 0.011026 0.009483 0.012069 prob_j1_at_t17 0.010306 0.008945 0.011229 prob_j1_at_t18 0.009365 0.008201 0.010160 prob_j1_at_t19 0.010203 0.009013 0.011019 prob_j1_at_t20 0.009188 0.008191 0.009879 prob_j1_at_t21 0.008386 0.007543 0.008978 prob_j1_at_t22 0.008535 0.007740 0.009100 prob_j1_at_t23 0.009191 0.008413 0.009752 prob_j1_at_t24 0.007096 0.006544 0.007501 prob_j1_at_t25 0.007440 0.006915 0.007834 prob_j1_at_t26 0.007788 0.007300 0.008166 prob_j1_at_t27 0.006990 0.006604 0.007299 prob_j1_at_t28 0.006113 0.005818 0.006360 prob_j1_at_t29 0.006511 0.006246 0.006748 prob_j1_at_t30 0.005218 0.005045 0.005389 prob_j2_at_t1 0.014380 0.008500 0.015506 prob_j2_at_t2 0.010637 0.006399 0.011356 prob_j2_at_t3 0.011197 0.006840 0.011855 prob_j2_at_t4 0.009615 0.005954 0.010107 prob_j2_at_t5 0.009163 0.005748 0.009565 prob_j2_at_t6 0.008337 0.005292 0.008649 prob_j2_at_t7 0.007689 0.004939 0.007928 prob_j2_at_t8 0.008040 0.005224 0.008241 prob_j2_at_t9 0.006819 0.004479 0.006950 prob_j2_at_t10 0.006860 0.004552 0.006955 prob_j2_at_t11 0.005794 0.003883 0.005844 prob_j2_at_t12 0.006257 0.004236 0.006278 prob_j2_at_t13 0.006123 0.004188 0.006112 prob_j2_at_t14 0.005736 0.003961 0.005699 prob_j2_at_t15 0.005311 0.003703 0.005251 prob_j2_at_t16 0.005364 0.003773 0.005279 prob_j2_at_t17 0.005211 0.003700 0.005105 prob_j2_at_t18 0.004287 0.003070 0.004181 prob_j2_at_t19 0.005073 0.003666 0.004927 prob_j2_at_t20 0.005114 0.003731 0.004943 prob_j2_at_t21 0.003817 0.002808 0.003674 prob_j2_at_t22 0.004498 0.003338 0.004311 prob_j2_at_t23 0.003480 0.002603 0.003321 prob_j2_at_t24 0.003455 0.002607 0.003283 prob_j2_at_t25 0.003669 0.002790 0.003473 prob_j2_at_t26 0.003674 0.002816 0.003463 prob_j2_at_t27 0.003301 0.002552 0.003099 prob_j2_at_t28 0.003238 0.002523 0.003027 prob_j2_at_t29 0.004067 0.003195 0.003788 prob_j2_at_t30 0.003969 0.003144 0.003681 cif_j1_at_t1 0.043775 0.031795 0.052250 cif_j1_at_t2 0.076731 0.056126 0.091255 cif_j1_at_t3 0.104240 0.076713 0.123578 cif_j1_at_t4 0.129283 0.095712 0.152797 cif_j1_at_t5 0.150660 0.112129 0.177582 cif_j1_at_t6 0.171621 0.128424 0.201731 cif_j1_at_t7 0.189895 0.142791 0.222664 cif_j1_at_t8 0.207480 0.156771 0.242692 cif_j1_at_t9 0.222870 0.169138 0.260127 cif_j1_at_t10 0.237835 0.181287 0.276992 cif_j1_at_t11 0.252248 0.193109 0.293148 cif_j1_at_t12 0.266118 0.204597 0.308618 cif_j1_at_t13 0.278100 0.214616 0.321919 cif_j1_at_t14 0.290544 0.225126 0.335665 cif_j1_at_t15 0.301148 0.234163 0.347326 cif_j1_at_t16 0.312174 0.243646 0.359395 cif_j1_at_t17 0.322480 0.252591 0.370624 cif_j1_at_t18 0.331845 0.260792 0.380785 cif_j1_at_t19 0.342048 0.269805 0.391804 cif_j1_at_t20 0.351236 0.277996 0.401683 cif_j1_at_t21 0.359623 0.285540 0.410661 cif_j1_at_t22 0.368158 0.293280 0.419761 cif_j1_at_t23 0.377348 0.301693 0.429513 cif_j1_at_t24 0.384444 0.308236 0.437014 cif_j1_at_t25 0.391884 0.315151 0.444848 cif_j1_at_t26 0.399673 0.322451 0.453014 cif_j1_at_t27 0.406662 0.329055 0.460313 cif_j1_at_t28 0.412775 0.334874 0.466673 cif_j1_at_t29 0.419285 0.341119 0.473422 cif_j1_at_t30 0.424503 0.346164 0.478811 cif_j2_at_t1 0.014380 0.008500 0.015506 cif_j2_at_t2 0.025018 0.014900 0.026862 cif_j2_at_t3 0.036214 0.021739 0.038717 cif_j2_at_t4 0.045829 0.027693 0.048824 cif_j2_at_t5 0.054992 0.033441 0.058389 cif_j2_at_t6 0.063329 0.038733 0.067038 cif_j2_at_t7 0.071018 0.043673 0.074965 cif_j2_at_t8 0.079059 0.048897 0.083207 cif_j2_at_t9 0.085878 0.053376 0.090156 cif_j2_at_t10 0.092737 0.057928 0.097111 cif_j2_at_t11 0.098531 0.061811 0.102955 cif_j2_at_t12 0.104788 0.066048 0.109233 cif_j2_at_t13 0.110912 0.070236 0.115345 cif_j2_at_t14 0.116647 0.074197 0.121044 cif_j2_at_t15 0.121959 0.077899 0.126295 cif_j2_at_t16 0.127322 0.081672 0.131574 cif_j2_at_t17 0.132534 0.085372 0.136679 cif_j2_at_t18 0.136820 0.088442 0.140860 cif_j2_at_t19 0.141894 0.092108 0.145787 cif_j2_at_t20 0.147008 0.095839 0.150730 cif_j2_at_t21 0.150825 0.098647 0.154404 cif_j2_at_t22 0.155324 0.101985 0.158715 cif_j2_at_t23 0.158803 0.104588 0.162036 cif_j2_at_t24 0.162259 0.107195 0.165319 cif_j2_at_t25 0.165928 0.109985 0.168792 cif_j2_at_t26 0.169602 0.112801 0.172254 cif_j2_at_t27 0.172903 0.115352 0.175353 cif_j2_at_t28 0.176141 0.117875 0.178380 cif_j2_at_t29 0.180208 0.121070 0.182168 cif_j2_at_t30 0.184177 0.124214 0.185850"},{"location":"UsageExample-FittingTwoStagesFitter/#estimating-with-twostagesfitter","title":"Estimating with TwoStagesFitter","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#estimation","title":"Estimation","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#standard-error-of-the-regression-coefficients","title":"Standard Error of the Regression Coefficients","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#regularization","title":"Regularization","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#l1","title":"L1","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#l2","title":"L2","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#elastic-net","title":"Elastic Net","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#separated-penalty-coefficients","title":"Separated Penalty Coefficients","text":""},{"location":"UsageExample-FittingTwoStagesFitter/#prediction","title":"Prediction","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/","title":"Small Sample Size Example","text":"<p>The <code>CoxPHFitter</code> from the Python lifelines package, which is used in the first stage of <code>TwoStagesFitter</code>, employs Efron\u2019s approximation of the partial likelihood function when ties are present. While Efron's method is computationally efficient for large sample sizes, it may yield biased coefficient estimates when the sample size is small.</p> <p>Therefore, for datasets with up to approximately 500 observations, it is recommended to use the exact method, i.e., <code>TwoStagesFitterExact</code>, as illustrated below. This method employs <code>ConditionalLogit</code> models from statsmodels to estimate the \\(\\beta_j\\) coefficients using the exact likelihood. However, due to its computational complexity, it is suitable only for small sample sizes. Additional tools for model selection and screening available in PyDTS for use with <code>TwoStagesFitter</code> also have corresponding \"Exact\" versions for small sample sizes, which rely on <code>TwoStagesFitterExact</code>.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nfrom pydts.examples_utils.plots import plot_example_pred_output\nimport warnings\npd.set_option(\"display.max_rows\", 500)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1. + 0.4 * np.log(t),\n        2: lambda t: -1. + 0.4 * np.log(t),\n    },\n    \"beta\": {\n        1: -0.4*np.log([0.8, 3, 3, 2.5, 2]),\n        2: -0.3*np.log([1, 3, 4, 3, 2]),\n    }\n}\n\nn_patients = 300\nn_cov = 5\n</code></pre> <pre><code>patients_df = generate_quick_start_df(n_patients=n_patients, n_cov=n_cov, d_times=4, \n                                      j_events=2, pid_col='pid', seed=0,\n                                      real_coef_dict=real_coef_dict, censoring_prob=0.1)\n\npatients_df.head()\n</code></pre> pid Z1 Z2 Z3 Z4 Z5 J T C X 0 0 0.548814 0.715189 0.602763 0.544883 0.423655 1 2 5 2 1 1 0.645894 0.437587 0.891773 0.963663 0.383442 2 2 5 2 2 2 0.791725 0.528895 0.568045 0.925597 0.071036 1 1 5 1 3 3 0.087129 0.020218 0.832620 0.778157 0.870012 1 4 5 4 4 4 0.978618 0.799159 0.461479 0.780529 0.118274 2 1 5 1 <p>In the following we apply the estimation method of Meir et al. (2022). Note that the data dataframe must not contain a column named 'C'.</p> <pre><code>from pydts.fitters import TwoStagesFitterExact\nnew_fitter = TwoStagesFitterExact()\nnew_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1))\nnew_fitter.print_summary()\n</code></pre> 1 2 coef std err z P&gt;|z| [0.025 0.975] coef std err z P&gt;|z| [0.025 0.975] Z1 -0.2946 0.347 -0.848 0.397 -0.976 0.386 0.2337 0.321 0.728 0.467 -0.395 0.863 Z2 -0.8902 0.367 -2.427 0.015 -1.609 -0.171 -0.3483 0.341 -1.022 0.307 -1.016 0.319 Z3 -0.1380 0.348 -0.397 0.692 -0.820 0.544 -0.7829 0.333 -2.349 0.019 -1.436 -0.130 Z4 -0.4728 0.328 -1.442 0.149 -1.115 0.170 0.1123 0.310 0.362 0.718 -0.496 0.721 Z5 -0.3284 0.349 -0.941 0.347 -1.012 0.356 -0.0659 0.330 -0.200 0.842 -0.712 0.580 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 48 True -0.632059 2 32 True -0.552136 3 27 True -0.083812 4 15 True 0.056266 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 56 True -1.093067 2 43 True -0.817784 3 27 True -0.744723 4 16 True -0.545621 <pre><code>new_fitter.get_beta_SE()\n</code></pre> 1 2 coef std err z P&gt;|z| [0.025 0.975] coef std err z P&gt;|z| [0.025 0.975] Z1 -0.2946 0.347 -0.848 0.397 -0.976 0.386 0.2337 0.321 0.728 0.467 -0.395 0.863 Z2 -0.8902 0.367 -2.427 0.015 -1.609 -0.171 -0.3483 0.341 -1.022 0.307 -1.016 0.319 Z3 -0.1380 0.348 -0.397 0.692 -0.820 0.544 -0.7829 0.333 -2.349 0.019 -1.436 -0.130 Z4 -0.4728 0.328 -1.442 0.149 -1.115 0.170 0.1123 0.310 0.362 0.718 -0.496 0.721 Z5 -0.3284 0.349 -0.941 0.347 -1.012 0.356 -0.0659 0.330 -0.200 0.842 -0.712 0.580 <p>The Exact version supports adding regularization when estimating the Beta coefficients. It is done by passing the <code>fit_beta_kwargs</code> argument to the <code>fit()</code> method. The added regularization term is of the form: $$ \\mbox{Penalizer} \\cdot \\Bigg( \\frac{1-\\mbox{L1_wt}}{2}||\\beta||_{2}^{2} + \\mbox{L1_wt} ||\\beta||_1 \\Bigg) $$ In statsmodels, the penalization parameter is denoted as <code>alpha</code>. Thus, adding L1, L2, or Elastic Net regularization can be done as follows:</p> <pre><code>L1_regularized_fitter = TwoStagesFitterExact()\n\nfit_beta_kwargs = {\n    'model_fit_kwargs': {\n        1: {\n                'alpha': 0.003,\n                'L1_wt': 1\n        },\n        2: {\n                'alpha': 0.005,\n                'L1_wt': 1\n        }\n    }\n}\n\nL1_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL1_regularized_fitter.get_beta_SE()\n</code></pre> 1 2 Z1 -0.058633 0.000000 Z2 -0.653238 0.000000 Z3 0.000000 -0.455494 Z4 -0.290221 0.000000 Z5 -0.143967 0.000000 <pre><code>L2_regularized_fitter = TwoStagesFitterExact()\n\nfit_beta_kwargs = {\n    'model_fit_kwargs': {\n        1: {\n                'alpha': 0.0,\n                'L1_wt': 0\n        },\n        2: {\n                'alpha': 0.002,\n                'L1_wt': 0\n        }\n    }\n}\n\nL2_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nL2_regularized_fitter.get_beta_SE()\n</code></pre> 1 2 Z1 -0.294621 0.203402 Z2 -0.890082 -0.305744 Z3 -0.137424 -0.685264 Z4 -0.473069 0.096045 Z5 -0.328632 -0.074304 <pre><code>EN_regularized_fitter = TwoStagesFitterExact()\n\nfit_beta_kwargs = {\n    'model_kwargs': {\n        'alpha': 0.003,\n        'L1_wt': 0.5\n    }\n}\n\nEN_regularized_fitter.fit(df=patients_df.drop(['C', 'T'], axis=1), fit_beta_kwargs=fit_beta_kwargs)\n\nEN_regularized_fitter.get_beta_SE()\n</code></pre> 1 2 coef std err z P&gt;|z| [0.025 0.975] coef std err z P&gt;|z| [0.025 0.975] Z1 -0.2946 0.347 -0.848 0.397 -0.976 0.386 0.2337 0.321 0.728 0.467 -0.395 0.863 Z2 -0.8902 0.367 -2.427 0.015 -1.609 -0.171 -0.3483 0.341 -1.022 0.307 -1.016 0.319 Z3 -0.1380 0.348 -0.397 0.692 -0.820 0.544 -0.7829 0.333 -2.349 0.019 -1.436 -0.130 Z4 -0.4728 0.328 -1.442 0.149 -1.115 0.170 0.1123 0.310 0.362 0.718 -0.496 0.721 Z5 -0.3284 0.349 -0.941 0.347 -1.012 0.356 -0.0659 0.330 -0.200 0.842 -0.712 0.580 <p>Full prediction is given by the method <code>predict_full()</code></p> <p>The input is a <code>pandas.DataFrame()</code> containing for each observation the covariates columns which were used in the <code>fit()</code> method (\\(Z1\\)-\\(Z5\\) in our example).</p> <p>The following columns will be added:</p> <ol> <li>The overall survival at each time point t</li> <li>The hazard for each failure type \\(j\\) at each time point t</li> <li>The probability of event type \\(j\\) at time t</li> <li>The Cumulative Incident Function (CIF) of event type \\(j\\) at time t</li> </ol> <p>In the following, we provide predictions for the individuals with ID values (pid) 0, 1 and 2. We transposed the output for easy view.</p> <pre><code>pred_df = new_fitter.predict_full(\n    patients_df.drop(['J', 'T', 'C', 'X'], axis=1).head(3)).set_index('pid').T\npred_df.index.name = ''\npred_df.columns = ['ID=0', 'ID=1', 'ID=2']\n</code></pre> <pre><code>pred_df\n</code></pre> ID=0 ID=1 ID=2 Z1 0.548814 0.645894 0.791725 Z2 0.715189 0.437587 0.528895 Z3 0.602763 0.891773 0.568045 Z4 0.544883 0.963663 0.925597 Z5 0.423655 0.383442 0.071036 overall_survival_t1 0.710298 0.718560 0.675201 overall_survival_t2 0.469059 0.481542 0.418215 overall_survival_t3 0.273296 0.285601 0.225275 overall_survival_t4 0.143109 0.152871 0.107048 hazard_j1_t1 0.128945 0.128253 0.132910 hazard_j1_t2 0.138191 0.137457 0.142394 hazard_j1_t3 0.203904 0.202903 0.209619 hazard_j1_t4 0.227586 0.226502 0.233770 hazard_j2_t1 0.160757 0.153188 0.191889 hazard_j2_t2 0.201439 0.192393 0.238213 hazard_j2_t3 0.213448 0.204001 0.251724 hazard_j2_t4 0.248774 0.238237 0.291040 prob_j1_at_t1 0.128945 0.128253 0.132910 prob_j1_at_t2 0.098157 0.098771 0.096145 prob_j1_at_t3 0.095643 0.097706 0.087666 prob_j1_at_t4 0.062198 0.064689 0.052662 prob_j2_at_t1 0.160757 0.153188 0.191889 prob_j2_at_t2 0.143082 0.138246 0.160842 prob_j2_at_t3 0.100120 0.098235 0.105275 prob_j2_at_t4 0.067989 0.068041 0.065564 cif_j1_at_t1 0.128945 0.128253 0.132910 cif_j1_at_t2 0.227102 0.227024 0.229055 cif_j1_at_t3 0.322745 0.324730 0.316721 cif_j1_at_t4 0.384944 0.389420 0.369383 cif_j2_at_t1 0.160757 0.153188 0.191889 cif_j2_at_t2 0.303839 0.291434 0.352730 cif_j2_at_t3 0.403958 0.389669 0.458005 cif_j2_at_t4 0.471948 0.457710 0.523569"},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#estimating-with-twostagesfitterexact","title":"Estimating with TwoStagesFitterExact","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#introduction","title":"Introduction","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#data-preparation","title":"Data Preparation","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#estimation","title":"Estimation","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#standard-error-of-the-regression-coefficients","title":"Standard Error of the Regression Coefficients","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#regularization","title":"Regularization","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#l1","title":"L1","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#l2","title":"L2","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#elastic-net","title":"Elastic Net","text":""},{"location":"UsageExample-FittingTwoStagesFitterExact-FULL/#prediction","title":"Prediction","text":""},{"location":"UsageExample-Intro/","title":"Introduction","text":"<p>PyDTS currently includes two estimation procedures:</p> <ol> <li> <p><code>TwoStagesFitter</code> - a python implementation of our estimation method [1].</p> </li> <li> <p><code>DataExpansionFitter</code> - a python implementation of the estimation method of Lee et al. (2018) [2].</p> </li> </ol> <p>The following example demonstrates the required data preparation steps, and applies our estimation procedure (Meir et. al., 2022) and that of Lee et al. (2018). As expected, both methods perform similarly in terms of point estimation and standard errors, while that of Meir et al (2022) is much faster. </p> <p>The following sections includes:</p> <ol> <li>Data Preparation</li> <li>Estimating with <code>TwoStagesFitter</code></li> <li><code>TwoStagesFitter</code> Prediction</li> <li>Estimating with <code>DataExpansionFitter</code></li> <li><code>DataExpansionFitter</code> Prediction</li> </ol> <p>For details about the estimation methods, please see Methods section.</p> <p>For a detailed comparison between the two estimation techniques, based on as simulation study, see Meir et al. (2022).</p>"},{"location":"UsageExample-Intro/#introduction","title":"Introduction","text":""},{"location":"UsageExample-Intro/#references","title":"References","text":"<p>[1] Meir, Tomer, Gutman, Rom, and Gorfine, Malka, \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks with Optional Penalization\", Journal of Open Source Software (2025), doi: 10.21105/joss.08815 </p> <p>[2] Lee, Minjung and Feuer, Eric J. and Fine, Jason P., \"On the analysis of discrete time competing risks data\", Biometrics (2018) doi: 10.1111/biom.12881</p>"},{"location":"UsageExample-RegroupingData/","title":"Data Regrouping Example","text":"<p>As previously discussed, both estimators require enough observed failures for each (j, t). Sometimes, the data do not comply with this requirement. For example, when dealing with hospitalization length of stay, patients are more likely to be released after a few days rather than after a month, and releases can be less frequent on weekends.</p> <p>In this example we demonstrate data regrouping that can be part of the preprocessing stage, which will allow a successful estimation. </p> <pre><code>import warnings\nimport sys \n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nfrom pydts.examples_utils.plots import plot_events_occurrence, add_panel_text, plot_example_estimated_params\nfrom pydts.fitters import TwoStagesFitter\n\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 25)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <p>We consider a setting in which the observed events become less frequent in later times by simply reducing the sample size to \\(n=1000\\).</p> <pre><code>real_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -1 - 0.3 * np.log(t),\n        2: lambda t: -1.75 - 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: -np.log([0.8, 3, 3, 2.5, 2]),\n        2: -np.log([1, 3, 4, 3, 2])\n    }\n}\n\ndf = generate_quick_start_df(n_patients=1000, n_cov=5, d_times=30, j_events=2, pid_col='pid', seed=0, \n                             real_coef_dict=real_coef_dict)\n</code></pre> <p>Evidently, we see that we do not observe enough events in later times. For example, \\(n_{j=1,t=25} = 1\\) and \\(n_{j=2,t=25} = 0\\)</p> <pre><code>ax = plot_events_occurrence(df)\nadd_panel_text(ax, 'a')\n</code></pre> <p>Trying to fit the model with such data will result in the following error message:</p> <pre><code>m2 = TwoStagesFitter()\ntry:\n    m2.fit(df.drop(columns=['C', 'T']), verbose=0)\nexcept RuntimeError as e:\n    raise e.with_traceback(None)\n</code></pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [4], in &lt;cell line: 2&gt;()\n      3     m2.fit(df.drop(columns=['C', 'T']), verbose=0)\n      4 except RuntimeError as e:\n----&gt; 5     raise e.with_traceback(None)\n\nRuntimeError: Number of observed events at some time points are too small. Consider collapsing neighbor time points.\n See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details.</pre> <p>For fixing zero events and the tail of the distribution, events occured later than the 21st day (either \\(J=1\\) or \\(J=2\\)) are considered to be in a 21+ event time.</p> <pre><code>regrouped_df = df.copy()\nregrouped_df['X'].clip(upper=21, inplace=True)\nax = plot_events_occurrence(regrouped_df)\nadd_panel_text(ax, 'b')\n</code></pre> <p>Now, we can successfully estimate the parameters:</p> <pre><code>fig, axes = plt.subplots(2,1, figsize=(10,8))\nax = axes[0]\nax = plot_events_occurrence(df, ax=ax)\nadd_panel_text(ax, 'a')\nax = axes[1]\nax = plot_events_occurrence(regrouped_df, ax=ax)\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels[-1] = '21+'\nax.set_xticklabels(labels)\nadd_panel_text(ax, 'b')\nfig.tight_layout()\n</code></pre> <pre><code>m2 = TwoStagesFitter()\nm2.fit(regrouped_df.drop(columns=['C', 'T']))\nplot_example_estimated_params(m2)\n</code></pre> <pre><code>m2.print_summary()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.059593 0.187268 0.433584 0.286376 Z2 -0.977635 0.192579 -0.651536 0.285630 Z3 -1.012224 0.191226 -1.147630 0.290300 Z4 -1.048233 0.180891 -0.221056 0.272420 Z5 -0.817598 0.180479 -0.475688 0.272666 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 63 True -0.947684 2 49 True -1.051086 3 34 True -1.295866 4 34 True -1.178560 5 15 True -1.903136 6 25 True -1.298924 7 20 True -1.401847 8 17 True -1.451159 9 11 True -1.788478 10 12 True -1.564731 11 15 True -1.247966 12 12 True -1.373390 13 7 True -1.834668 14 5 True -2.060652 15 14 True -0.884086 16 5 True -1.761146 17 5 True -1.645269 18 4 True -1.729781 19 1 True -2.928615 20 3 True -1.769298 21 8 True -0.566276 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 24 True -2.770174 2 24 True -2.619309 3 13 True -3.105049 4 11 True -3.164241 5 9 True -3.269706 6 12 True -2.900518 7 14 True -2.616379 8 6 True -3.361561 9 13 True -2.468053 10 3 True -3.827000 11 1 True -4.497481 12 3 True -3.627228 13 4 True -3.288580 14 3 True -3.455462 15 3 True -3.369194 16 3 True -3.193872 17 1 True -4.094674 18 2 True -3.325382 19 2 True -3.218786 20 1 True -3.768195 21 4 True -2.222326 <p>Consider the case of almost no discharge events during the weekends. In the following we resample the data to reflect this setting:</p> <pre><code>from random import random \ndef map_days(row):\n    if row['X'] in [7, 14, 21] and row['J'] in [1]:\n        if (random() &amp;gt; 0.1) or (row['X'] == 21):\n            row['X'] -= 1\n            row['X'].astype(int)\n    return row\n\nregrouped_df = regrouped_df.apply(map_days, axis=1)\nregrouped_df[['J', 'T', 'C', 'X']] = regrouped_df[['J', 'T', 'C', 'X']].astype('int64')\n(regrouped_df.groupby(['J'])['X'].value_counts()).to_frame().unstack()\n</code></pre> X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 J 0 30.0 20.0 28.0 21.0 22.0 22.0 23.0 21.0 20.0 11.0 18.0 15.0 15.0 13.0 21.0 16.0 19.0 14.0 14.0 14.0 108.0 1 63.0 49.0 34.0 34.0 15.0 43.0 2.0 17.0 11.0 12.0 15.0 12.0 11.0 1.0 14.0 5.0 5.0 4.0 1.0 11.0 NaN 2 24.0 24.0 13.0 11.0 9.0 12.0 14.0 6.0 13.0 3.0 1.0 3.0 4.0 3.0 3.0 3.0 1.0 2.0 2.0 1.0 4.0 <pre><code>df = regrouped_df.copy()\nplot_events_occurrence(regrouped_df)\n</code></pre> <p>Trying to fit the model with such data will result in the following error message:</p> <pre><code>m2 = TwoStagesFitter()\ntry: \n    m2.fit(regrouped_df.drop(columns=['C', 'T']), verbose=0)\nexcept RuntimeError as e:\n    raise e.with_traceback(None)\n</code></pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [11], in &lt;cell line: 2&gt;()\n      3     m2.fit(regrouped_df.drop(columns=['C', 'T']), verbose=0)\n      4 except RuntimeError as e:\n----&gt; 5     raise e.with_traceback(None)\n\nRuntimeError: Number of observed events at some time points are too small. Consider collapsing neighbor time points.\n See https://tomer1812.github.io/pydts/UsageExample-RegroupingData/ for more details.</pre> <p>We suggest to regroup empty times with the preceding days:</p> <pre><code>def map_days_second_try(row):\n    if row['X'] in [7, 14, 21]:\n        row['X'] -= 1\n        row['X'].astype(int)\n    return row\n\nregrouped_df = regrouped_df.apply(map_days_second_try, axis=1)\nregrouped_df[['J', 'T', 'C', 'X']] = regrouped_df[['J', 'T', 'C', 'X']].astype('int64')\n(regrouped_df.groupby(['J'])['X'].value_counts()).to_frame().unstack()\n</code></pre> X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 J 0 30 20 28 21 22 45 21 20 11 18 15 28 21 16 19 14 14 122 1 63 49 34 34 15 45 17 11 12 15 12 12 14 5 5 4 1 11 2 24 24 13 11 9 26 6 13 3 1 3 7 3 3 1 2 2 5 <pre><code>plot_events_occurrence(regrouped_df)\n</code></pre> <pre><code>fig, axes = plt.subplots(2,1, figsize=(10,8))\nax = axes[0]\nax = plot_events_occurrence(df, ax=ax)\nadd_panel_text(ax, 'a')\nax = axes[1]\nax = plot_events_occurrence(regrouped_df, ax=ax)\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels[5] = '6-7'\nlabels[11] = '13-14'\nlabels[17] = '20-21'\nax.set_xticklabels(labels)\nadd_panel_text(ax, 'b')\nfig.tight_layout()\n</code></pre> <p>Now, we can estimate the parameters, while the interpretation of the parameters related to the grouped time points should be interpreted with care.</p> <pre><code>m2 = TwoStagesFitter()\nm2.fit(regrouped_df.drop(columns=['C', 'T']), verbose=0)\nplot_example_estimated_params(m2)\n</code></pre> <pre><code>m2.print_summary()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 0.125497 0.226571 0.559011 0.344835 Z2 -0.590350 0.230030 -0.146264 0.338669 Z3 -0.577012 0.230431 -0.601869 0.343531 Z4 -0.813756 0.221557 0.119530 0.328842 Z5 -0.586253 0.231495 -0.254760 0.345008 <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> n_jt success alpha_jt J X 1 1 63 True -1.544779 2 49 True -1.658131 3 34 True -1.911357 4 34 True -1.795520 5 15 True -2.530238 6 45 True -1.296992 8 17 True -2.090432 9 11 True -2.431846 10 12 True -2.211441 11 15 True -1.904531 12 12 True -2.032937 13 12 True -1.938316 15 14 True -1.565972 16 5 True -2.453997 17 5 True -2.335441 18 4 True -2.424637 19 1 True -3.647561 20 11 True -1.103130 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> n_jt success alpha_jt J X 2 1 24 True -3.574037 2 24 True -3.442172 3 13 True -3.946532 4 11 True -4.010704 5 9 True -4.104630 6 26 True -2.947698 8 6 True -4.205070 9 13 True -3.331808 10 3 True -4.678181 11 1 True -5.489276 12 3 True -4.531819 13 7 True -3.595795 15 3 True -4.254731 16 3 True -4.099612 17 1 True -4.963138 18 2 True -4.249112 19 2 True -4.146457 20 5 True -3.099039"},{"location":"UsageExample-RegroupingData/#data-regrouping-example","title":"Data Regrouping Example","text":""},{"location":"UsageExample-RegroupingData/#not-enough-observed-failures-in-later-times","title":"Not enough observed failures in later times","text":""},{"location":"UsageExample-RegroupingData/#not-enough-observed-events-at-specific-times","title":"Not enough observed events at specific times","text":""},{"location":"UsageExample-SIS-SIS-L/","title":"Screening Example","text":"<p>Sure independent screening (SIS) has been shown to effectively filter out many uninformative variables in ultra-high-dimensional settings, where the number of covariates greatly exceeds the number of observations (as is common in genetic datasets, for example). Penalized variable selection methods are typically applied to the remaining covariates after screening.</p> <p>In the following example, we demonstrate how such an analysis can be performed on discrete-time survival data with competing events using the <code>SISTwoStagesFitter</code> of PyDTS. An \"Exact\" version for small sample sizes is also available via <code>SISTwoStagesFitterExact</code>. </p> <p>To demonstrate the screening process, we first sample a dataset with \\(p = 1000\\) covariates and \\(n = 500\\) observations. Clearly, \\(p \\gg n\\), placing us in an ultra-high-dimensional setting.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pydts.data_generation import EventTimesSampler\nfrom pydts.screening import SISTwoStagesFitter\nfrom pydts.cross_validation import PenaltyGridSearchCV\nfrom pydts.fitters import TwoStagesFitter\n</code></pre> <pre><code>n_cov = 1000\nbeta1 = np.zeros(n_cov)\nbeta1[:5] = 1.5*np.array([-0.6, 0.5, -0.5, 0.6, -0.6])\nbeta2 = np.zeros(n_cov)\nbeta2[:5] = 1.5*np.array([0.5, -0.7, 0.7, -0.5, -0.7])\n\nreal_coef_dict = {\n    \"alpha\": {\n        1: lambda t: -3.1 + 0.15 * np.log(t),\n        2: lambda t: -3.2 + 0.15 * np.log(t)\n    },\n    \"beta\": {\n        1: beta1,\n        2: beta2\n    }\n}\n\nn_patients = 500\nd_times = 6\nj_events = 2\n\nets = EventTimesSampler(d_times=d_times, j_event_types=j_events)\n\nseed = 97\nmeans_vector = np.zeros(n_cov)\ncovariance_matrix = np.identity(n_cov)\n\nclip_value = 2.5\n\ncovariates = [f'Z{i + 1}' for i in range(n_cov)]\n\npatients_df = pd.DataFrame(data=pd.DataFrame(data=np.random.multivariate_normal(means_vector, covariance_matrix,\n                                                                                size=n_patients),\n                                             columns=covariates))\npatients_df.clip(lower=-1 * clip_value, upper=clip_value, inplace=True)\npatients_df = ets.sample_event_times(patients_df, hazard_coefs=real_coef_dict, seed=seed)\npatients_df = ets.sample_independent_lof_censoring(patients_df, prob_lof_at_t=0.01 * np.ones(d_times),\n                                                   seed=seed + 1)\npatients_df = ets.update_event_or_lof(patients_df)\npatients_df.index.name = 'pid'\npatients_df = patients_df.reset_index()\n</code></pre> <p>The resulting dataset contains the following observed event-types and event-times:</p> <pre><code>from pydts.examples_utils.plots import plot_events_occurrence\nplot_events_occurrence(patients_df)\n</code></pre> <p><code>SISTwoStagesFitter</code> implements the screening process. As described in the Methods section, we fit marginal estimates for the \\(\\beta_j\\) coefficients using both the original data and permuted data that follow the null model. The maximum absolute value of the marginal coefficients from the null model (fitted using the permuted data) is selected as a data-driven threshold. We then retain only those variables whose marginal coefficients from the original data exceed this threshold.</p> <pre><code>fitter = SISTwoStagesFitter()\nfitter.fit(df=patients_df.drop(['C', 'T'], axis=1), seed=10)\n</code></pre> <pre>\n<code>[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.4s\n[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    7.2s\n[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:   13.7s\n[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed:   22.7s\n[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:   34.2s finished\n[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    4.8s\n[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:   11.3s\n[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed:   20.3s\n[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:   32.7s finished\n</code>\n</pre> <p>The results of the marginal estimates under the null model using permuted data, the marginal estimates using the original data, and the data-driven threshold are provided below.</p> <pre><code>fitter.null_model_df\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 -0.051194 0.079323 -0.109907 0.079738 Z2 0.018975 0.082467 -0.032187 0.082622 Z3 -0.048221 0.074646 -0.129129 0.076010 Z4 -0.034971 0.076312 -0.056456 0.076418 Z5 0.034691 0.073673 -0.043497 0.074451 ... ... ... ... ... Z996 -0.068723 0.084455 -0.029909 0.084860 Z997 0.199384 0.079347 -0.141407 0.078816 Z998 0.048464 0.075687 0.029086 0.075630 Z999 0.038851 0.077472 0.018370 0.077997 Z1000 -0.162900 0.075047 0.027251 0.076968 <p>1000 rows \u00d7 4 columns</p> <pre><code>fitter.threshold\n</code></pre> <pre>\n<code>0.24893913811535112</code>\n</pre> <pre><code>fitter.marginal_estimates_df\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 -0.440522 0.080529 0.290845 0.078372 Z2 0.450510 0.083694 -0.661550 0.084618 Z3 -0.439468 0.082773 0.524361 0.077278 Z4 0.672241 0.081552 -0.496182 0.078566 Z5 -0.458879 0.080453 -0.714228 0.083198 ... ... ... ... ... Z996 -0.001290 0.080151 -0.098645 0.080883 Z997 0.144605 0.078395 0.142037 0.079094 Z998 -0.069460 0.073183 -0.012518 0.072763 Z999 0.049599 0.079498 -0.094697 0.079714 Z1000 -0.050592 0.077711 -0.085265 0.078140 <p>1000 rows \u00d7 4 columns</p> <p>The informative coefficients that exceed the threshold are selected separately for each event type:</p> <pre><code>fitter.chosen_covariates_j\n</code></pre> <pre>\n<code>{1: ['Z1', 'Z2', 'Z3', 'Z4', 'Z5', 'Z95'],\n 2: ['Z1', 'Z2', 'Z3', 'Z4', 'Z5', 'Z198', 'Z355']}</code>\n</pre> <p>Evidently, we successfully identified all informative variables (\\(Z_1\\)\u2013\\(Z_5\\)) and filtered out almost all non-informative ones, except for one false positive in event-type 1 (\\(Z_{95}\\)) and two false positives (\\(Z_{198}\\), \\(Z_{355}\\)) in event-type 2. Now, we can use the selected variables to train a <code>TwoStagesFitter</code> as follows.</p> <pre><code>selected_covariates = list(np.unique(fitter.chosen_covariates_j[1]+fitter.chosen_covariates_j[2]))\n\ntwo_step = TwoStagesFitter()\ntwo_step.fit(patients_df[['pid', 'X', 'J'] + selected_covariates], covariates=fitter.chosen_covariates_j)\ntwo_step.get_beta_SE()\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 -0.677825 0.078953 0.460589 0.086621 Z2 0.615446 0.086701 -0.953396 0.096915 Z3 -0.651071 0.093876 0.668352 0.078943 Z4 0.802648 0.081182 -0.639696 0.084932 Z5 -0.724804 0.090150 -0.815123 0.084053 Z95 0.102926 0.083056 NaN NaN Z198 NaN NaN 0.139889 0.081325 Z355 NaN NaN -0.205345 0.083858 <p>As an additional variable selection step, LASSO regression can be applied to the set of covariates retained by the screening process.</p> <p>To select the optimal penalization parameter, we perform a penalty grid search using cross-validation and the evaluation metrics described in the Methods section. By default, model selection is guided by the global-AUC metric.</p> <pre><code>step = 0.5\npenalizers = np.arange(-8., -1.4, step=step) \nn_splits = 3\n</code></pre> <pre><code>penalty_cv_search = PenaltyGridSearchCV()\ngauc_cv_results = penalty_cv_search.cross_validate(full_df=patients_df[['pid', 'X', 'J'] + selected_covariates], \n                                                   l1_ratio=1, \n                                                   penalizers=np.exp(penalizers),  \n                                                   n_splits=n_splits, \n                                                   seed=20)\n</code></pre> <pre>\n<code>Starting fold 1/3\nStarted estimating the coefficients for penalizer 0.00033546262790251185 (1/14)\nFinished estimating the coefficients for penalizer 0.00033546262790251185 (1/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0005530843701478336 (2/14)\nFinished estimating the coefficients for penalizer 0.0005530843701478336 (2/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0009118819655545162 (3/14)\nFinished estimating the coefficients for penalizer 0.0009118819655545162 (3/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0015034391929775724 (4/14)\nFinished estimating the coefficients for penalizer 0.0015034391929775724 (4/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/14)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.004086771438464067 (6/14)\nFinished estimating the coefficients for penalizer 0.004086771438464067 (6/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (7/14)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (7/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.011108996538242306 (8/14)\nFinished estimating the coefficients for penalizer 0.011108996538242306 (8/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (9/14)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (9/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0301973834223185 (10/14)\nFinished estimating the coefficients for penalizer 0.0301973834223185 (10/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (11/14)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (11/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0820849986238988 (12/14)\nFinished estimating the coefficients for penalizer 0.0820849986238988 (12/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.1353352832366127 (13/14)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (13/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.22313016014842982 (14/14)\nFinished estimating the coefficients for penalizer 0.22313016014842982 (14/14), 1 seconds\nFinished fold 1/3, 58 seconds\nStarting fold 2/3\nStarted estimating the coefficients for penalizer 0.00033546262790251185 (1/14)\nFinished estimating the coefficients for penalizer 0.00033546262790251185 (1/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0005530843701478336 (2/14)\nFinished estimating the coefficients for penalizer 0.0005530843701478336 (2/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0009118819655545162 (3/14)\nFinished estimating the coefficients for penalizer 0.0009118819655545162 (3/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0015034391929775724 (4/14)\nFinished estimating the coefficients for penalizer 0.0015034391929775724 (4/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/14)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.004086771438464067 (6/14)\nFinished estimating the coefficients for penalizer 0.004086771438464067 (6/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (7/14)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (7/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.011108996538242306 (8/14)\nFinished estimating the coefficients for penalizer 0.011108996538242306 (8/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (9/14)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (9/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0301973834223185 (10/14)\nFinished estimating the coefficients for penalizer 0.0301973834223185 (10/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (11/14)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (11/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0820849986238988 (12/14)\nFinished estimating the coefficients for penalizer 0.0820849986238988 (12/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.1353352832366127 (13/14)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (13/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.22313016014842982 (14/14)\nFinished estimating the coefficients for penalizer 0.22313016014842982 (14/14), 1 seconds\nFinished fold 2/3, 59 seconds\nStarting fold 3/3\nStarted estimating the coefficients for penalizer 0.00033546262790251185 (1/14)\nFinished estimating the coefficients for penalizer 0.00033546262790251185 (1/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0005530843701478336 (2/14)\nFinished estimating the coefficients for penalizer 0.0005530843701478336 (2/14), 0 seconds\nStarted estimating the coefficients for penalizer 0.0009118819655545162 (3/14)\nFinished estimating the coefficients for penalizer 0.0009118819655545162 (3/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0015034391929775724 (4/14)\nFinished estimating the coefficients for penalizer 0.0015034391929775724 (4/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0024787521766663585 (5/14)\nFinished estimating the coefficients for penalizer 0.0024787521766663585 (5/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.004086771438464067 (6/14)\nFinished estimating the coefficients for penalizer 0.004086771438464067 (6/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.006737946999085467 (7/14)\nFinished estimating the coefficients for penalizer 0.006737946999085467 (7/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.011108996538242306 (8/14)\nFinished estimating the coefficients for penalizer 0.011108996538242306 (8/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.01831563888873418 (9/14)\nFinished estimating the coefficients for penalizer 0.01831563888873418 (9/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0301973834223185 (10/14)\nFinished estimating the coefficients for penalizer 0.0301973834223185 (10/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.049787068367863944 (11/14)\nFinished estimating the coefficients for penalizer 0.049787068367863944 (11/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.0820849986238988 (12/14)\nFinished estimating the coefficients for penalizer 0.0820849986238988 (12/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.1353352832366127 (13/14)\nFinished estimating the coefficients for penalizer 0.1353352832366127 (13/14), 1 seconds\nStarted estimating the coefficients for penalizer 0.22313016014842982 (14/14)\nFinished estimating the coefficients for penalizer 0.22313016014842982 (14/14), 1 seconds\nFinished fold 3/3, 59 seconds\n</code>\n</pre> <pre>\n<code>array([-5. , -6.5])</code>\n</pre> <p>The mean and standard error (SE) of the global-AUC, calculated via cross-validation for all possible combinations of penalization parameters, are as follows:</p> <pre><code>gauc_cv_results\n</code></pre> Mean SE 0.000335 0.000335 0.839310 0.016736 0.000553 0.839301 0.017051 0.000912 0.839296 0.017185 0.001503 0.839121 0.017664 0.002479 0.839049 0.017628 0.004087 0.838560 0.017396 0.006738 0.838120 0.017466 0.011109 0.837332 0.016710 0.018316 0.833451 0.013611 0.030197 0.821111 0.007837 0.049787 0.783010 0.011579 0.082085 0.772589 0.016024 0.135335 0.772869 0.016011 0.223130 0.772824 0.016058 0.000553 0.000335 0.839319 0.016691 0.000553 0.839215 0.016970 0.000912 0.839322 0.017114 0.001503 0.839048 0.017512 0.002479 0.839088 0.017568 0.004087 0.838381 0.017399 0.006738 0.838048 0.017620 0.011109 0.837413 0.016404 0.018316 0.833464 0.013506 0.030197 0.821080 0.007901 0.049787 0.782982 0.012069 0.082085 0.772556 0.016003 0.135335 0.772837 0.015990 0.223130 0.772791 0.016036 0.000912 0.000335 0.839445 0.016549 0.000553 0.839508 0.016806 0.000912 0.839674 0.017078 0.001503 0.839211 0.017539 0.002479 0.839136 0.017455 0.004087 0.838765 0.017007 0.006738 0.838096 0.017474 0.011109 0.837514 0.016094 0.018316 0.833507 0.013212 0.030197 0.821264 0.007708 0.049787 0.783023 0.012378 0.082085 0.772664 0.016456 0.135335 0.772945 0.016442 0.223130 0.772899 0.016487 0.001503 0.000335 0.839244 0.016515 0.000553 0.839205 0.016741 0.000912 0.839271 0.017036 0.001503 0.838917 0.017737 0.002479 0.838679 0.017799 0.004087 0.838435 0.017271 0.006738 0.838051 0.017731 0.011109 0.837635 0.016325 0.018316 0.833447 0.013246 0.030197 0.821356 0.007606 0.049787 0.783343 0.012864 0.082085 0.772856 0.016586 0.135335 0.773137 0.016572 0.223130 0.773091 0.016614 0.002479 0.000335 0.839097 0.016811 0.000553 0.839035 0.016926 0.000912 0.839103 0.017126 0.001503 0.838893 0.017719 0.002479 0.838964 0.017633 0.004087 0.838691 0.017244 0.006738 0.838744 0.017172 0.011109 0.838227 0.015683 0.018316 0.833432 0.012906 0.030197 0.821181 0.006973 0.049787 0.783506 0.012074 0.082085 0.773286 0.015911 0.135335 0.773566 0.015897 0.223130 0.773520 0.015939 0.004087 0.000335 0.838782 0.016410 0.000553 0.838854 0.016409 0.000912 0.839057 0.016399 0.001503 0.838994 0.016779 0.002479 0.838890 0.016642 0.004087 0.838758 0.016638 0.006738 0.838664 0.017101 0.011109 0.838381 0.015460 0.018316 0.834031 0.011877 0.030197 0.821658 0.006876 0.049787 0.785117 0.012208 0.082085 0.774618 0.015692 0.135335 0.774898 0.015677 0.223130 0.774853 0.015721 0.006738 0.000335 0.839506 0.015608 0.000553 0.839563 0.015375 0.000912 0.839494 0.015641 0.001503 0.839695 0.015665 0.002479 0.839303 0.015955 0.004087 0.839278 0.015767 0.006738 0.839070 0.015203 0.011109 0.838922 0.014411 0.018316 0.835204 0.011192 0.030197 0.822213 0.005912 0.049787 0.785897 0.012848 0.082085 0.776100 0.015153 0.135335 0.776380 0.015139 0.223130 0.776335 0.015184 0.011109 0.000335 0.839046 0.016343 0.000553 0.839020 0.016408 0.000912 0.839202 0.016388 0.001503 0.838919 0.016603 0.002479 0.838946 0.016389 0.004087 0.839008 0.016914 0.006738 0.839067 0.016148 0.011109 0.838108 0.015115 0.018316 0.834542 0.012781 0.030197 0.822281 0.006193 0.049787 0.787564 0.014657 0.082085 0.776986 0.017123 0.135335 0.777267 0.017108 0.223130 0.777221 0.017148 0.018316 0.000335 0.835257 0.016302 0.000553 0.835103 0.016123 0.000912 0.835091 0.015831 0.001503 0.835023 0.015652 0.002479 0.834942 0.015868 0.004087 0.834990 0.015789 0.006738 0.835301 0.015642 0.011109 0.835309 0.015189 0.018316 0.831576 0.011445 0.030197 0.821664 0.006425 0.049787 0.788653 0.016959 0.082085 0.780039 0.020316 0.135335 0.780320 0.020299 0.223130 0.780274 0.020321 0.030197 0.000335 0.814710 0.015876 0.000553 0.814745 0.015663 0.000912 0.814631 0.015975 0.001503 0.814247 0.016350 0.002479 0.814106 0.016617 0.004087 0.813703 0.016317 0.006738 0.813543 0.016564 0.011109 0.813497 0.016480 0.018316 0.809512 0.014985 0.030197 0.798333 0.016604 0.049787 0.770253 0.027722 0.082085 0.759912 0.027838 0.135335 0.760192 0.027821 0.223130 0.760147 0.027759 0.049787 0.000335 0.767913 0.014539 0.000553 0.768009 0.014360 0.000912 0.768047 0.014529 0.001503 0.767932 0.014657 0.002479 0.768087 0.014794 0.004087 0.767950 0.015078 0.006738 0.767536 0.014674 0.011109 0.767847 0.013736 0.018316 0.762140 0.011930 0.030197 0.738421 0.022566 0.049787 0.693013 0.031937 0.082085 0.697943 0.020535 0.135335 0.698223 0.020517 0.223130 0.698178 0.020518 0.082085 0.000335 0.784645 0.018999 0.000553 0.784499 0.019231 0.000912 0.784805 0.019623 0.001503 0.784836 0.019550 0.002479 0.784909 0.019320 0.004087 0.785301 0.018848 0.006738 0.786166 0.020660 0.011109 0.785523 0.019952 0.018316 0.780788 0.017355 0.030197 0.752271 0.019530 0.049787 0.672445 0.028903 0.082085 0.826968 0.010104 0.135335 0.826940 0.010734 0.223130 0.826910 0.010692 0.135335 0.000335 0.784838 0.018920 0.000553 0.784692 0.019158 0.000912 0.784998 0.019550 0.001503 0.785029 0.019472 0.002479 0.785102 0.019250 0.004087 0.785494 0.018774 0.006738 0.786359 0.020611 0.011109 0.785716 0.019881 0.018316 0.780982 0.017265 0.030197 0.752464 0.019229 0.049787 0.672638 0.028580 0.082085 0.828181 0.009055 0.135335 0.828035 0.010011 0.223130 0.827822 0.010142 0.223130 0.000335 0.784937 0.018952 0.000553 0.784790 0.019195 0.000912 0.785097 0.019588 0.001503 0.785127 0.019506 0.002479 0.785201 0.019289 0.004087 0.785593 0.018811 0.006738 0.786458 0.020666 0.011109 0.785815 0.019919 0.018316 0.781080 0.017290 0.030197 0.752562 0.019069 0.049787 0.672737 0.028392 0.082085 0.828618 0.009264 0.135335 0.828195 0.009819 0.223130 0.828290 0.010145 <p>We choose the optimal penalizers to be the ones that maximize the global-AUC, i.e., \\(\\log(\\eta_j)\\), \\(j=1,2\\), are</p> <pre><code>chosen_eta = np.log(gauc_cv_results['Mean'].idxmax())\nchosen_eta\n</code></pre> <pre>\n<code>array([-5. , -6.5])</code>\n</pre> <p>Lastly, we train a regularized <code>TwoStagesFitter</code> using the entire dataset and the chosen optimal penalizers:</p> <pre><code>L1_regularized_fitter = TwoStagesFitter()\nfit_beta_kwargs = {\n    'model_kwargs': {\n        1: {'penalizer': np.exp(chosen_eta[0]), 'l1_ratio': 1},\n        2: {'penalizer': np.exp(chosen_eta[1]), 'l1_ratio': 1}\n}}\nL1_regularized_fitter.fit(df = patients_df[['pid', 'X', 'J'] + selected_covariates],\n                          fit_beta_kwargs = fit_beta_kwargs, covariates=fitter.chosen_covariates_j)\n\nlasso_beta = L1_regularized_fitter.get_beta_SE() \n</code></pre> <p>Thus, the final SIS-L model is </p> <pre><code>lasso_beta\n</code></pre> j1_params j1_SE j2_params j2_SE covariate Z1 -0.545206 0.079020 0.435527 0.086359 Z2 0.510060 0.086179 -0.915843 0.095965 Z3 -0.509418 0.089982 0.645257 0.078984 Z4 0.689470 0.080248 -0.614332 0.084495 Z5 -0.567603 0.087249 -0.785802 0.083732 Z95 0.065064 0.082256 NaN NaN Z198 NaN NaN 0.125372 0.081348 Z355 NaN NaN -0.188075 0.083523 <p>Recall that the true non-zero \\(\\beta_j\\) values were:</p> <pre><code>true_values = pd.concat([pd.Series(beta1[:5]), pd.Series(beta2[:5])], axis=1)\ntrue_values.index = ['Z1', 'Z2', 'Z3', 'Z4', 'Z5']\ntrue_values.columns = ['j1', 'j2']\ntrue_values\n</code></pre> j1 j2 Z1 -0.90 0.75 Z2 0.75 -1.05 Z3 -0.75 1.05 Z4 0.90 -0.75 Z5 -0.90 -1.05 <p>Thus, using SIS, we identified the informative variables and filtered out most of the non-informative ones. Adding LASSO as a subsequent step further reduced the false positive coefficient in event-type 1 to nearly zero. In this specific example, SIS-L did not eliminate additional false positives for event-type 2; however, given the large number of initial covariates, this still represents a substantial improvement.</p>"},{"location":"UsageExample-SIS-SIS-L/#sure-independent-screening","title":"Sure Independent Screening","text":""},{"location":"UsageExample-SIS-SIS-L/#data-generation","title":"Data Generation","text":""},{"location":"UsageExample-SIS-SIS-L/#sis","title":"SIS","text":""},{"location":"UsageExample-SIS-SIS-L/#adding-lasso-sis-l","title":"Adding LASSO (SIS-L)","text":""},{"location":"User%20Story/","title":"User Story","text":"<pre><code>import warnings\nimport sys \n\nimport pandas as pd\nfrom pydts.examples_utils.generate_simulations_data import generate_quick_start_df\nfrom pydts.fitters import TwoStagesFitter\n\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 25)\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n</code></pre> <pre><code>df = generate_quick_start_df(n_patients=1000, n_cov=5, d_times=30, j_events=2, pid_col='pid', seed=0)\n\n(df.groupby(['J'])['X'].value_counts()).to_frame().unstack()\n</code></pre> X X 1 2 3 4 5 6 7 8 9 10 11 12 ... 19 20 21 22 23 24 25 26 27 28 29 30 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 ... 16.0 13.0 20.0 13.0 14.0 14.0 12.0 9.0 7.0 11.0 6.0 9.0 1 74.0 41.0 36.0 33.0 21.0 25.0 20.0 13.0 11.0 13.0 10.0 10.0 ... 2.0 4.0 2.0 5.0 1.0 6.0 1.0 2.0 1.0 2.0 NaN 1.0 2 29.0 11.0 19.0 15.0 17.0 4.0 7.0 6.0 8.0 7.0 11.0 3.0 ... 1.0 1.0 1.0 1.0 1.0 1.0 1.0 NaN NaN NaN NaN 1.0 <p>3 rows \u00d7 30 columns</p> <pre><code>m2 = TwoStagesFitter()\ntry:\n    m2.fit(df.drop(columns=['C', 'T']),verbose=0)\nexcept RuntimeError as e:\n    raise e.with_traceback(None)\n</code></pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [3], in &lt;cell line: 2&gt;()\n      3     m2.fit(df.drop(columns=['C', 'T']),verbose=0)\n      4 except RuntimeError as e:\n----&gt; 5     raise e.with_traceback(None)\n\nRuntimeError: In event J=1, The method didn't have events D=[29]. Consider changing the problem definition. \n See TBD for more details.</pre> <p> In this case we see that the method doesn't have events for D=29. let's see the distrubtion: </p> <pre><code>axes = df.hist(column=['X'], by=['J'], bins=30, figsize=(10,10))\nfor ax in axes.flatten():\n    ax.set_xlim(0, 30)\n</code></pre> <p>As one can easily see from the data, We don't have events during the 25-30 days. \\ for example - we can think about a patients length of stay in hospital - patints are more likely to stay in hospital or to leave hospital soon after hospitalization, but only few are leaving after it.\\ To tackle this challenge, we can induce adminstrative censorship, such that patients that had event (either \\(J=1\\) or \\(J=2\\)) after the 25th day (25+), is considered to be similar to patient that had event in the 25th day.</p> <pre><code>temp_df = df.copy() # so we don't change the original data\ntemp_df['X'].clip(upper=25, inplace=True ) # we are clipping all the patients over 25 days to 25\nm2 = TwoStagesFitter()\nm2.fit(temp_df.drop(columns=['C', 'T']))\nm2.print_summary()\n</code></pre> <pre>\n<code>INFO: Pandarallel will run on 4 workers.\nINFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n\n\nModel summary for event: 1\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 375 partial log-likelihood -2313.33 time fit was run 2022-03-22 16:13:49 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.26 0.44 0.77 1.56 0.50 0.62 0.70 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.24 0.51 -5.65 &lt;0.005 25.93 Z3 -1.07 0.34 0.19 -1.44 -0.71 0.24 0.49 -5.81 &lt;0.005 27.26 Z4 -0.53 0.59 0.17 -0.88 -0.19 0.42 0.82 -3.08 &lt;0.005 8.91 Z5 -0.52 0.59 0.18 -0.87 -0.18 0.42 0.84 -2.98 &lt;0.005 8.46 Concordance 0.63 Partial AIC 4636.66 log-likelihood ratio test 81.75 on 5 df -log2(p) of ll-ratio test 51.30 n_jt success alpha_jt J X 1 1 74 True -1.095820 2 41 True -1.564119 3 36 True -1.580492 4 33 True -1.542081 5 21 True -1.896471 6 25 True -1.598934 7 20 True -1.737360 8 13 True -2.066961 9 11 True -2.135509 10 13 True -1.859094 11 10 True -2.002466 12 10 True -1.879609 13 5 True -2.486454 14 12 True -1.501572 15 9 True -1.703977 16 7 True -1.856787 17 5 True -2.067510 18 3 True -2.482490 19 2 True -2.806679 20 4 True -1.998939 21 2 True -2.568323 22 5 True -1.487019 23 1 True -2.914662 24 6 True -0.859760 25 7 True -0.335299 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9560 number of events observed 161 partial log-likelihood -987.17 time fit was run 2022-03-22 16:13:50 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.17 1.18 0.27 -0.37 0.71 0.69 2.02 0.61 0.54 0.89 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.85 &lt;0.005 13.03 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.73 &lt;0.005 26.59 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.30 0.86 -2.53 0.01 6.46 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.41 0.16 2.66 Concordance 0.66 Partial AIC 1984.33 log-likelihood ratio test 56.08 on 5 df -log2(p) of ll-ratio test 33.57 n_jt success alpha_jt J X 2 1 29 True -1.889441 2 11 True -2.713222 3 19 True -2.027058 4 15 True -2.141383 5 17 True -1.889341 6 4 True -3.162640 7 7 True -2.589820 8 6 True -2.631269 9 8 True -2.229403 10 7 True -2.257762 11 11 True -1.664896 12 3 True -2.836411 13 5 True -2.246092 14 1 True -3.607395 15 3 True -2.582261 16 4 True -2.176043 17 1 True -3.368219 18 2 True -2.626271 19 1 True -3.202843 20 1 True -3.042474 21 1 True -2.958991 22 1 True -2.840443 23 1 True -2.655788 24 1 True -2.408854 25 2 True -1.382297 <p> </p> <p>Now, we can predict the surviavl curves for patients that had events (\\(J\\in\\{1,2\\}\\)) in days 1-24 or in day 25 and after.</p> <pre><code>(temp_df.groupby(['J'])['X'].value_counts()).to_frame().unstack()\n</code></pre> X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24 25 25 24 19 19 22 17 23 20 15 16 14 12 7 16 11 11 16 13 20 13 14 14 54 1 74 41 36 33 21 25 20 13 11 13 10 10 5 12 9 7 5 3 2 4 2 5 1 6 7 2 29 11 19 15 17 4 7 6 8 7 11 3 5 1 3 4 1 2 1 1 1 1 1 1 2 <pre><code>def map_days(row):\n    if row['X'] in [7, 14, 21] and row['J'] in [1,2]:\n        row['X'] -= 1\n        row['X'].astype(int)\n    return row\n\ntemp_df = temp_df.apply(map_days, axis=1)\ntemp_df[['J', 'T', 'C', 'X']] = temp_df[['J', 'T', 'C', 'X']].astype('int64')\n</code></pre> <p> As expected in this case, the fitter is falied to converage, because it lacks events in days 7,14,21 (\"The weekend\") </p> <pre><code>m2 = TwoStagesFitter()\ntry: \n    m2.fit(temp_df.drop(columns=['C', 'T']), verbose=0)\nexcept RuntimeError as e:\n    raise e.with_traceback(None)\n</code></pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [8], in &lt;cell line: 2&gt;()\n      3     m2.fit(temp_df.drop(columns=['C', 'T']), verbose=0)\n      4 except RuntimeError as e:\n----&gt; 5     raise e.with_traceback(None)\n\nRuntimeError: In event J=1, The method didn't have events D=[7, 14, 21]. Consider changing the problem definition. \n See TBD for more details.</pre> <p> And we get error as expected </p> <p>Further exploring the disribution, we notice that we have a \"hole\" during the weekends, so we need do fix this behavior</p> <pre><code>display((temp_df.groupby(['J'])['X'].value_counts()).to_frame().unstack())\n\naxes = temp_df.hist(column=['X'], by=['J'], bins=25, figsize=(10,10))\nfor ax in axes.flatten():\n    ax.set_xlim(0, 25)\n</code></pre> X X 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 J 0 24.0 25.0 25.0 24.0 19.0 19.0 22.0 17.0 23.0 20.0 15.0 16.0 14.0 12.0 7.0 16.0 11.0 11.0 16.0 13.0 20.0 13.0 14.0 14.0 54.0 1 74.0 41.0 36.0 33.0 21.0 45.0 NaN 13.0 11.0 13.0 10.0 10.0 17.0 NaN 9.0 7.0 5.0 3.0 2.0 6.0 NaN 5.0 1.0 6.0 7.0 2 29.0 11.0 19.0 15.0 17.0 11.0 NaN 6.0 8.0 7.0 11.0 3.0 6.0 NaN 3.0 4.0 1.0 2.0 1.0 2.0 NaN 1.0 1.0 1.0 2.0 <p>One of such fixes can be stacking the events which occurs during the weekend, as one \"bucket\". </p> <pre><code>def map_days_second_try(row):\n    if row['X'] in [7, 14, 21] and row['J'] == 0:\n        row['X'] -= 1\n        row['X'].astype(int)\n    return row\n\ntemp_df = temp_df.apply(map_days_second_try, axis=1)\ntemp_df[['J', 'T', 'C', 'X']] = temp_df[['J', 'T', 'C', 'X']].astype('int64')\n</code></pre> <pre><code>display((temp_df.groupby(['J'])['X'].value_counts()).to_frame().unstack())\n\naxes = temp_df.hist(column=['X'], by=['J'], bins=25, figsize=(10,10))\nfor ax in axes.flatten():\n    ax.set_xlim(0, 25)\n</code></pre> X X 1 2 3 4 5 6 8 9 10 11 12 13 15 16 17 18 19 20 22 23 24 25 J 0 24 25 25 24 19 41 17 23 20 15 16 26 7 16 11 11 16 33 13 14 14 54 1 74 41 36 33 21 45 13 11 13 10 10 17 9 7 5 3 2 6 5 1 6 7 2 29 11 19 15 17 11 6 8 7 11 3 6 3 4 1 2 1 2 1 1 1 2 <p>Now, we can our fit would work as planned, and the model provides \\(\\alpha\\) only for the times we provided. \\ Thus, we should note our model can't predict during those days.</p> <pre><code>m2 = TwoStagesFitter()\nm2.fit(temp_df.drop(columns=['C', 'T']), verbose=0)\nm2.print_summary()\n</code></pre> <pre>\n<code>\n\nModel summary for event: 1\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'j_1' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 375 partial log-likelihood -2315.15 time fit was run 2022-03-22 16:14:19 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.09 1.09 0.18 -0.27 0.44 0.76 1.55 0.48 0.63 0.66 Z2 -1.04 0.35 0.18 -1.41 -0.68 0.25 0.51 -5.65 &lt;0.005 25.87 Z3 -1.08 0.34 0.18 -1.44 -0.72 0.24 0.49 -5.84 &lt;0.005 27.49 Z4 -0.53 0.59 0.17 -0.87 -0.19 0.42 0.83 -3.06 &lt;0.005 8.82 Z5 -0.52 0.59 0.18 -0.86 -0.18 0.42 0.84 -2.96 &lt;0.005 8.37 Concordance 0.63 Partial AIC 4640.29 log-likelihood ratio test 81.78 on 5 df -log2(p) of ll-ratio test 51.32 n_jt success alpha_jt J X 1 1 74 True -1.095262 2 41 True -1.563535 3 36 True -1.579912 4 33 True -1.541470 5 21 True -1.895841 6 45 True -0.967645 8 13 True -2.066400 9 11 True -2.134931 10 13 True -1.858564 11 10 True -2.001975 12 10 True -1.879093 13 17 True -1.223394 15 9 True -1.703488 16 7 True -1.856321 17 5 True -2.067005 18 3 True -2.481967 19 2 True -2.806130 20 6 True -1.579369 22 5 True -1.486437 23 1 True -2.914180 24 6 True -0.859284 25 7 True -0.334542 <pre>\n<code>\n\nModel summary for event: 2\n</code>\n</pre> model lifelines.CoxPHFitter duration col 'X' event col 'j_2' strata X_copy baseline estimation breslow number of observations 9463 number of events observed 161 partial log-likelihood -987.94 time fit was run 2022-03-22 16:14:20 UTC coef exp(coef) se(coef) coef lower 95% coef upper 95% exp(coef) lower 95% exp(coef) upper 95% z p -log2(p) Z1 0.16 1.18 0.27 -0.37 0.70 0.69 2.02 0.60 0.55 0.87 Z2 -1.08 0.34 0.28 -1.63 -0.53 0.20 0.59 -3.83 &lt;0.005 12.92 Z3 -1.67 0.19 0.29 -2.24 -1.10 0.11 0.33 -5.74 &lt;0.005 26.68 Z4 -0.67 0.51 0.27 -1.19 -0.15 0.31 0.86 -2.52 0.01 6.39 Z5 -0.38 0.69 0.27 -0.90 0.15 0.41 1.16 -1.40 0.16 2.64 Concordance 0.66 Partial AIC 1985.88 log-likelihood ratio test 55.99 on 5 df -log2(p) of ll-ratio test 33.51 n_jt success alpha_jt J X 2 1 29 True -1.891318 2 11 True -2.715119 3 19 True -2.029019 4 15 True -2.143387 5 17 True -1.891328 6 11 True -2.224464 8 6 True -2.633376 9 8 True -2.231573 10 7 True -2.259989 11 11 True -1.667235 12 3 True -2.838670 13 6 True -2.067073 15 3 True -2.584596 16 4 True -2.178489 17 1 True -3.370445 18 2 True -2.628716 19 1 True -3.205144 20 2 True -2.415453 22 1 True -2.842736 23 1 True -2.658322 24 1 True -2.411368 25 2 True -1.384788 <p> </p>"},{"location":"User%20Story/#guide-for-dealing-with-problem-definition","title":"Guide for dealing with problem definition","text":""},{"location":"User%20Story/#case-study-1-not-enough-cases-in-the-tail","title":"Case study #1: Not enough cases in the tail","text":"<p>For the first example we would consider settings in which the cases are becoming less frequent in the tail end of the data </p>"},{"location":"User%20Story/#case-study-2-not-enough-cases-in-the-middle","title":"Case study #2: Not enough cases in the middle","text":"<p>In this example we examine a case where there is no events during the weekend, but only cencoreship. This use-case shows possible solution of combining those events into stucks of \"weekend\".</p>"},{"location":"examples-intro/","title":"Examples","text":"<p>In this section, we present the main functionalities of PyDTS through usage examples. We demonstrate how to:</p> <ol> <li>Use <code>EventTimesSampler</code> to simulate discrete-time survival data with competing events, including random and hazard-based censoring.  </li> <li>Perform estimation and prediction with <code>TwoStagesFitter</code> and <code>DataExpansionFitter</code>.  </li> <li>Apply evaluation metrics.  </li> <li>Add regularization.  </li> <li>Handle small sample sizes using <code>TwoStagesFitterExact</code>.  </li> <li>Conduct screening with <code>SISTwoStagesFitter</code>.  </li> <li>Carry out model selection procedures.  </li> <li>Propose data-preprocessing strategies to mitigate estimation errors that arise when the number of observed events is too small at specific times.</li> <li>Work with a simulated hospitalization length-of-stay use case.  </li> </ol> <p>Note - The figures included in the module <code>pydts.example_utils</code> are provided solely for visualization in the documentation and are not generalizable to arbitrary datasets.</p>"},{"location":"intro/","title":"Introduction","text":""},{"location":"intro/#discrete-data-survival-analysis","title":"Discrete-data survival analysis","text":"<p>Discrete-data survival analysis refers to the case where data can only take values over a discrete grid. Sometimes, events can only occur at regular, discrete points in time. For example, in the United States a change in party controlling the presidency only occurs quadrennially in the month of January [3]. In other situations events may occur at any point in time, but available data record only the particular interval of time in which each event occurs. For example, death from cancer measured by months since time of diagnosis [4], or length of stay in hospital recorded on a daily basis. It is well-known that naively using standard continuous-time models (even after correcting for ties) with discrete-time data may result in biased estimators for the discrete time models.</p>"},{"location":"intro/#competing-events","title":"Competing events","text":"<p>Competing events arise when individuals are susceptible to several types of events but can experience at most one event. For example, competing risks for hospital length of stay are discharge and in-hospital death. Occurrence of one of these events precludes us from observing the other event on this patient. Another classical example of competing risks is cause-specific mortality, such as death from heart disease, death from cancer and death from other causes [5, 6]. </p> <p>PyDTS is an open source Python package which implements tools for discrete-time survival analysis with competing risks.</p>"},{"location":"intro/#references","title":"References","text":"<p>[1]  Meir, Tomer, Gutman, Rom, and Gorfine, Malka,  \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks and Optional Penalization\", Journal of Open Source Software (2025),  doi: 10.21105/joss.08815</p> <p>[2]  Meir, Tomer and Gorfine, Malka,  \"Discrete-time Competing-Risks Regression with or without Penalization\", Biometrics (2025), doi: 10.1093/biomtc/ujaf040</p> <p>[3]  Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718</p> <p>[4]  Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881</p> <p>[5]  Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0</p> <p>[6]  Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer (2003) ISBN: 978-0-387-95399-1</p>"},{"location":"methods/","title":"Methods","text":""},{"location":"methods/#definitions","title":"Definitions","text":"<p>We let \\(T\\)  denote a discrete event time that can take on only the values \\(\\{1,2,...,d\\}\\) and \\(J\\) denote the type of event, \\(J \\in \\{1,\\ldots,M\\}\\).  Consider a \\(p \\times 1\\) vector of baseline covariates \\(Z\\). A general discrete cause-specific hazard function is of the form $$ \\lambda_j(t|Z) = \\Pr(T=t,J=j|T\\geq t, Z)  \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1,\\ldots,M  \\, . $$ A popular semi-parametric model of the above hazard function based on a transformation regression model is of the form $$ h(\\lambda_{j}(t|Z))  = \\alpha_{jt} +Z^T \\beta_j \\hspace{0.3cm} t \\in {1,2,...,d} \\hspace{0.3cm} j=1, \\ldots,M $$ such that \\(h\\) is a known function [2, and reference therein]. The total number of parameters in the model is \\(M(d+p)\\). The logit function \\(h(a)=\\log \\{ a/(1-a) \\}\\) yields  \\begin{equation}\\label{eq:logis} \\lambda_j(t|Z)=\\frac{\\exp(\\alpha_{jt}+Z^T\\beta_j)}{1+\\exp(\\alpha_{jt}+Z^T\\beta_j)} \\, . \\end{equation} It should be noted that leaving \\(\\alpha_{jt}\\) unspecified is analogous to having an unspecified baseline hazard function in the Cox proportional hazard model [3], and thus we consider the above as a semi-parametric model.</p> <p>Let \\(S(t|Z) = \\Pr(T&gt;t|Z)\\) be the overall survival given \\(Z\\). Then, the probability of experiencing event of type \\(j\\) at time \\(t\\) equals $$ \\Pr(T=t,J=j|Z)=\\lambda_j(t|Z) \\prod_{k=1}^{t-1} \\left\\lbrace 1- \\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace $$ and the cumulative incident function (CIF) of cause \\(j\\) is given by $$ F_j(t|Z) = \\Pr(T \\leq t, J=j|Z) = \\sum_{m=1}^{t} \\lambda_j(m|Z) S(m-1|Z) = \\sum_{m=1}^{t}\\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ Finally, the marginal probability of event type \\(j\\) (marginally with respect to the time of event), given \\(Z\\), equals $$ \\Pr(J=j|Z) = \\sum_{m=1}^{d} \\lambda_j(m|Z) \\prod_{k=1}^{m-1} \\left\\lbrace 1-\\sum_{j'=1}^M\\lambda_{j'}(k|Z) \\right\\rbrace \\, . $$ In the next section we provide a fast estimation technique of the parameters \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T \\, ; \\, j=1,\\ldots,M\\}\\).</p>"},{"location":"methods/#the-collapsed-log-likelihood-approach-and-the-proposed-estimators","title":"The Collapsed Log-Likelihood Approach and the Proposed Estimators","text":"<p>For simplicity of presentation, we assume two competing events, i.e., \\(M=2\\) and our goal is estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T,\\alpha_{21},\\ldots,\\alpha_{2d},\\beta_2^T\\}\\) along with the standard error of the estimators. The data at hand consist of \\(n\\) independent observations, each with \\((X_i,\\delta_i,J_i,Z_i)\\) where \\(X_i=\\min(C_i,T_i)\\), \\(C_i\\) is a right-censoring time,  \\(\\delta_i=I(X_i=T_i)\\) is the event indicator and \\(J_i\\in\\{0,1,2\\}\\), where \\(J_i=0\\) if and only if \\(\\delta_i=0\\). Assume that given the covariates, the censoring and failure time are independent and non-informative. Then, the likelihood function is proportional to  $$ L = \\prod_{i=1}^n  \\left\\lbrace\\frac{\\lambda_1(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{1i}=1)} \\left\\lbrace\\frac{\\lambda_2(X_i|Z_i)}{1-\\lambda_1(X_i|Z_i)-\\lambda_2(X_i|Z_i)}\\right\\rbrace^{I(\\delta_{2i}=1)} \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ or, equivalently, $$ L = \\prod_{i=1}^n \\left[ \\prod_{j=1}^2 \\prod_{m=1}^{X_i} \\left\\lbrace \\frac{\\lambda_j(m|Z_i)}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right\\rbrace^{\\delta_{jim}}\\right] \\prod_{k=1}^{X_i}\\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace $$ where \\(\\delta_{jim}\\) equals one if subject \\(i\\) experienced event type \\(j\\) at time \\(m\\); and 0 otherwise. Clearly \\(L\\) cannot be decomposed into separate likelihoods for each cause-specific hazard function \\(\\lambda_j\\). The log likelihood becomes $$ \\log L = \\sum_{i=1}^n \\left[ \\sum_{j=1}^2 \\sum_{m=1}^{X_i} \\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i) - \\delta_{jim}{1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)}\\right] \\right. +\\left.\\sum_{k=1}^{X_i}\\log \\lbrace 1-\\lambda_1(k|Z_i)-\\lambda_2(k|Z_i)\\rbrace \\right] $$ $$     = \\sum_{i=1}^n \\sum_{m=1}^{X_i} \\left[  \\delta_{1im} \\log \\lambda_1(m|Z_i)+\\delta_{2im} \\log \\lambda_2(m|Z_i) \\right. +\\left. \\lbrace 1-\\delta_{1im}-\\delta_{2im}\\rbrace \\log\\lbrace 1-\\lambda_1(m|Z_i)-\\lambda_2(m|Z_i)\\rbrace \\right] \\, . $$</p> <p>Instead of maximizing the \\(M(d+p)\\) parameters simultaneously based on the above log-likelihood, the collapsed log-likelihood of Lee et al. [4] can be adopted. Specifically, the data are expanded  such that for each observation \\(i\\) the expanded dataset includes \\(X_i\\) rows, one row for each time \\(t\\), \\(t \\leq X_i\\). At each time point \\(t\\) the expanded data are conditionally multinomial with one of three possible outcomes \\(\\{\\delta_{1it},\\delta_{2it},1-\\delta_{1it}-\\delta_{2it}\\}\\), as in Table 1.</p> <p>Table 1:  Original and expanded datasets with \\(M = 2\\) competing events [Lee et al. (2018)]. </p> \\(i\\) \\(X_i\\) \\(\\delta_i\\) \\(Z_i\\) \\(i\\) \\(\\tilde{X}_i\\) \\(\\delta_{1it}\\) \\(\\delta_{2it}\\) \\(1 - \\delta_{1it} - \\delta_{2it}\\) \\(Z_i\\) 1 2 1 \\(Z_1\\) 1 1 0 0 1 \\(Z_1\\) 1 2 1 0 0 \\(Z_1\\) 2 3 2 \\(Z_2\\) 2 1 0 0 1 \\(Z_2\\) 2 2 0 0 1 \\(Z_2\\) 2 3 0 1 0 \\(Z_2\\) 3 3 0 \\(Z_3\\) 3 1 0 0 1 \\(Z_3\\) 3 2 0 0 1 \\(Z_3\\) 3 3 0 0 1 \\(Z_3\\) <p>Then, for estimating \\(\\{\\alpha_{11},\\ldots,\\alpha_{1d},\\beta_1^T\\}\\), we combine \\(\\delta_{2it}\\) and \\(1-\\delta_{1it}-\\delta_{2it}\\), and the collapsed log-likelihood for cause \\(J=1\\) based on a binary regression model with \\(\\delta_{1it}\\) as the outcome is given by $$ \\log L_1 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{1im} \\log \\lambda_1(m|Z_i)+(1-\\delta_{1im})\\log \\lbrace 1-\\lambda_1(m|Z_i)\\rbrace \\right] \\, . $$ Similarly, the collapsed log-likelihood for cause \\(J=2\\) based on a binary regression model with \\(\\delta_{2it}\\) as the outcome becomes $$ \\log L_2 = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{2im} \\log \\lambda_2(m|Z_i)+(1-\\delta_{2im})\\log \\lbrace 1-\\lambda_2(m|Z_i)\\rbrace \\right] $$ and one can fit the two models, separately.  </p> <p>In general, for \\(M\\) competing events,  the estimators of \\(\\{\\alpha_{j1},\\ldots,\\alpha_{jd},\\beta_j^T\\}\\), \\(j=1,\\ldots,M\\), are the respective values that maximize $$ \\log L_j = \\sum_{i=1}^n \\sum_{m=1}^{X_i}\\left[ \\delta_{jim} \\log \\lambda_j(m|Z_i)+(1-\\delta_{jim})\\log {1-\\lambda_j(m|Z_i)} \\right] \\, . $$ Namely, each maximization \\(j\\), \\(j=1,\\ldots,M\\), consists of  maximizing \\(d + p\\) parameters simultaneously. </p>"},{"location":"methods/#proposed-estimators","title":"Proposed Estimators","text":"<p>Alternatively, we propose the following simpler and faster estimation procedure, with a negligible efficiency loss, if any. Our idea exploits the close relationship between conditional logistic regression analysis and stratified Cox regression analysis [5]. We propose to estimate each \\(\\beta_j\\) separately, and given \\(\\beta_j\\), \\(\\alpha_{jt}\\), \\(t=1\\ldots,d\\), are separately estimated. In particular, the proposed estimating procedure consists of the following two speedy steps:</p>"},{"location":"methods/#step-1","title":"Step 1.","text":"<p>Use the expanded dataset and estimate each vector \\(\\beta_j\\), \\(j \\in \\{1,\\ldots, M\\}\\), by a simple conditional logistic regression, conditioning on the event time \\(X\\), using a stratified Cox analysis.</p>"},{"location":"methods/#step-2","title":"Step 2.","text":"<p>Given the estimators \\(\\widehat{\\beta}_j\\) , \\(j \\in \\{1,\\ldots, M\\}\\), of Step 1, use the original (un-expanded) data and estimate each \\(\\alpha_{jt}\\), \\(j \\in \\{1,\\ldots,M\\}\\), \\(t=1,\\ldots,d\\), separately, by</p> \\[\\widehat{ \\alpha }_{jt} = argmin_{a} \\left\\lbrace \\frac{1}{y_t} \\sum_{i=1}^n I(X_i \\geq t) \\frac{ \\exp(a+Z_i^T \\widehat{\\beta}_j)}{1 + \\exp(a + Z_i^T \\widehat{\\beta}_j)} - \\frac{n_{tj}}{y_t} \\right\\rbrace ^2 \\] <p>where \\(y_t=\\sum_{i=1}^n I(X_i \\geq t)\\) and \\(n_{tj}=\\sum_{i=1}^n I(X_i = t, J_i=j)\\).</p> <p>The above equation consists minimizing the squared distance between the observed proportion of failures of type \\(j\\) at time \\(t\\)  (\\(n_{tj}/y_t\\)) and the expected proportion of failures given model defined above for \\(\\lambda_j\\) and \\(\\widehat{\\beta}_j\\).  The simulation results of section Simple Simulation reveals that the above two-step procedure performs well in terms of bias, and provides similar standard error of that of [3]. However, the improvement in computational time, by using our procedure, could be improved by a factor of 1.5-3.5 depending on d. Standard errors of \\(\\widehat{\\beta}_j\\), \\(j \\in \\{1,\\ldots,M\\}\\), can be derived directly from the stratified Cox analysis.</p>"},{"location":"methods/#time-dependent-covariates","title":"Time-dependent covariates","text":"<p>Similarly to the continuous-time Cox model, the simplest way to code time-dependent covariates uses intervals of time [Therneau et al. (2000)]. Then, the data is encoded by breaking the individual\u2019s time into multiple time intervals, with one row of data for each interval. Hence combining this data expansion step with the expansion demonstrated in Table 1 is straightforward.</p>"},{"location":"methods/#regularized-regression-models","title":"Regularized regression models","text":"<p>Penalized regression methods, such as lasso, adaptive lasso, and elastic net [Hastie et al. 2009], place a constraint on the size of the regression coefficients. The estimation procedure of [Meir and Gorfine (2023)] that separates the estimation of \\(\\beta_j\\) and \\(\\alpha_{jt}\\) can easily incorporate such constraints in Lagrangian form by minimizing</p> \\[ -\\log L_j^c(\\beta_j)  + \\eta_j P(\\beta_j) \\, , \\quad j=1,\\ldots,M \\, , \\] <p>where \\(P\\) is a penalty function and \\(\\eta_j \\geq 0\\) are shrinkage tuning parameters. The parameters \\(\\alpha_{jt}\\) are estimated once the regularization step is completed and \\(\\beta_j\\) are estimated.</p> <p>Clearly, any regularized Cox regression model routines can be used for estimating \\(\\beta_j\\), \\(j=1,\\ldots,M\\), based on the above equation, for example, the <code>CoxPHFitter</code> of the <code>lifelines</code> Python package [Davidson-Pilon (2019)] with penalization.</p>"},{"location":"methods/#sure-independence-screening","title":"Sure Independence Screening","text":"<p>When the number of available covariates greatly exceeds the number of observations (as common in genetic datasets, for example), i.e., the ultra-high setting, most regularized methods suffer from the curse of dimensionality, high variance, and overfitting [Hastie et al. (2009)].  Sure Independent Screening (SIS) is a marginal screening technique designed to filter out uninformative covariates.  Penalized regression methods can be applied after the marginal screening process to the remaining covariates.</p> <p>We start the SIS procedure by ranking all the covariates using a utility measure between the response and each covariate, and then retain only covariates with estimated coefficients that exceeds a threshold value.  We focus on SIS and SIS followed by lasso (SIS-L) [Fan et al. (2010); Saldana and Feng (2018)] within the proposed two-step procedure.</p> <p>We start by fitting a marginal regression for each covariate by maximizing:</p> \\[ L_j^{\\mathcal{C}}(\\beta_{jr}) \\quad \\text{for } j=1,\\ldots,M, \\quad r=1,\\ldots,p  \\] <p>where \\(\\boldsymbol{\\beta}_j = (\\beta_{j1},\\ldots,\\beta_{jp})^T\\).  Then we rank the features based on the magnitude of their marginal regression coefficients.  The selected sets of variables are given by:</p> \\[ \\widehat{\\mathcal{M}}_{j,w_n} = \\left\\{1 \\leq k \\leq p \\, : \\, |\\widehat{\\beta}_{jk}| \\geq w_n \\right\\}, \\quad j=1,\\ldots,M, \\] <p>where \\(w_n\\) is a threshold value.  We adopt the data-driven threshold of [Saldana and Feng (2018)].  Given data of the form \\(\\{X_i, \\delta_i, J_i, \\mathbf{Z}_i \\, ; \\, i = 1, \\ldots, n\\}\\), a random permutation \\(\\pi\\) of \\(\\{1,\\ldots,n\\}\\) is used to decouple \\(\\mathbf{Z}_i\\) and \\((X_i, \\delta_i, J_i)\\), so that the permuted data \\(\\{X_i, \\delta_i, J_i, \\mathbf{Z}_{\\pi(i)}\\}\\) follow a model where the covariates have no predictive power over the survival time of any event type.</p> <p>For the permuted data, we re-estimate individual regression coefficients and obtain \\(\\widehat{\\beta}^*_{jr}\\). The data-driven threshold is defined as:</p> \\[ w_n = \\max_{1 \\leq j \\leq M, \\, 1 \\leq k \\leq p} |\\widehat{\\beta}^*_{jk}|. \\] <p>For the SIS-L procedure, lasso regularization is then applied in the first step of the two-step procedure to the set of covariates selected by SIS. </p>"},{"location":"methods/#references","title":"References","text":"<p>[1]  Meir, Tomer, Gutman, Rom, and Gorfine, Malka,  \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks and Optional Penalization\", Journal of Open Source Software (2025),  doi: 10.21105/joss.08815</p> <p>[2]  Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718</p> <p>[3]  Cox, D. R. \"Regression Models and Life-Tables\" Journal of the Royal Statistical Society: Series B (Methodological) (1972) doi: 10.1111/j.2517-6161.1972.tb00899.x</p> <p>[4]  Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881</p> <p>[5]  Prentice, Ross L and Breslow, Norman E \"Retrospective studies and failure time models\" Biometrika (1978) doi: 10.1111/j.2517-6161.1972.tb00899.x</p> <p>[6]  Therneau, Terry M and Grambsch, Patricia M, \"Modeling Survival Data: Extending the Cox Model\", Springer-Verlag, (2000)</p> <p>[7]  Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H, \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\", Springer-Verlag, (2009)</p> <p>[8]  Meir, Tomer and Gorfine, Malka,  \"Discrete-time Competing-Risks Regression with or without Penalization\", Biometrics (2025), doi: 10.1093/biomtc/ujaf040</p> <p>[9]  Davidson-Pilon, Cameron, \"lifelines: Survival Analysis in Python\", Journal of Open Source Software, (2019)</p> <p>[10]  Fan, J and Feng, Y and Wu, Y, \"High-dimensional variable selection for Cox\u2019s proportional hazards model\",  Institute of Mathematical Statistics, (2010)</p> <p>[11]  Saldana, DF and Feng, Y, \"SIS: An R package for sure independence screening in ultrahigh-dimensional statistical models\",  Journal of Statistical Software, (2018)</p>"},{"location":"methodsevaluation/","title":"Evaluation Measures","text":"<p>Let</p> \\[ \\pi_{ij}(t) = \\widehat{\\Pr}(T_i=t, J_i=j \\mid Z_i) = \\widehat{\\lambda}_j (t \\mid Z_i) \\widehat{S}(t-1 \\mid Z_i) \\] <p>and</p> \\[ D_{ij} (t) = I(T_i = t, J_i = j)  \\] <p>The cause-specific incidence/dynamic area under the receiver operating characteristics curve (AUC) is defined and estimated in the spirit of Heagerty and Zheng (2005) and Blanche et al. (2015) as the probability of a random observation with observed event \\(j\\) at time \\(t\\) having a higher risk prediction for cause \\(j\\) than a randomly selected observation \\(m\\), at risk at time \\(t\\), without the observed event \\(j\\) at time \\(t\\). Namely, </p> \\[ \\mbox{AUC}_j(t) = \\Pr (\\pi_{ij}(t) &gt; \\pi_{mj}(t) \\mid D_{ij} (t) = 1, D_{mj} (t) = 0, T_m \\geq t) \\] <p>In the presence of censored data and under the assumption that the censoring is independent of the failure time and observed covariates, an inverse probability censoring weighting (IPCW) estimator of  \\(\\mbox{AUC}_j(t)\\) becomes</p> \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t) \\{I(\\pi_{ij}(t) &gt; \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n}  D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) W_{ij}(t) W_{mj}(t)}  \\] <p>And can be simplified as:</p> \\[ \\widehat{\\mbox{AUC}}_j (t) = \\frac{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t) \\{I(\\pi_{ij}(t) &gt; \\pi_{mj}(t))+0.5I(\\pi_{ij}(t)=\\pi_{mj}(t))\\}}{\\sum_{i=1}^{n}\\sum_{m=1}^{n} D_{ij}(t)(1-D_{mj}(t))I(X_m \\geq t)} \\] <p>where  </p> \\[ W_{ij}(t) = \\frac{D_{ij}(t)}{\\widehat{G}_C(T_i)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = \\frac{D_{ij}(t)}{\\widehat{G}_C(t)} + I(X_i \\geq t)\\frac{1-D_{ij}(t)}{\\widehat{G}_C(t)} = I(X_i \\geq t) / \\widehat{G}_C(t) \\] <p>and \\(\\widehat{G}_C(\\cdot)\\) is the estimated survival function of the censoring (e.g., the Kaplan-Meier estimator). Interestingly, the IPCWs  have no effect on \\(\\widehat{\\mbox{AUC}}_j (t)\\).</p> <p>An integrated cause-specific AUC can be estimated as a weighted sum by</p> \\[ \\widehat{\\mbox{AUC}}_j = \\sum_t \\widehat{\\mbox{AUC}}_j (t) w_j (t) \\] <p>and we adopt a simple  data-driven weight function of the form </p> \\[ w_j(t) = \\frac{N_j(t)}{\\sum_t N_j(t)} \\] <p>A global AUC can be defined as</p> \\[ \\widehat{\\mbox{AUC}} = \\sum_j \\widehat{\\mbox{AUC}}_j v_j \\] <p>where </p> \\[ v_j = \\frac{\\sum_{t} N_j(t)}{ \\sum_{j=1}^M \\sum_{t} N_j(t) } \\] <p>Another well-known performance measure is the Brier Score (BS). In the spirit of Blanche et al. (2015) we define</p> \\[ \\widehat{\\mbox{BS}}_{j}(t) = \\frac{1}{Y_{\\cdot}(t)} {\\sum_{i=1}^n W_{ij}(t) \\left( D_{ij}(t) - \\pi_{ij}(t)\\right)^2} \\, .  \\] <p>An integrated cause-specific BS can be estimated by the weighted sum</p> \\[ \\widehat{\\mbox{BS}}_{j} = \\sum_t \\widehat{\\mbox{BS}}_{j}(t) w_j(t)  \\] <p>and an estimated global BS is given by </p> \\[ \\widehat{\\mbox{BS}} = \\sum_j \\widehat{\\mbox{BS}}_{j} v_j \\, . \\]"},{"location":"methodsintro/","title":"Methods","text":"<p>In this section, we outline the statistical background for the tools incorporated in PyDTS. We commence with some definitions, present the collapsed log-likelihood approach and the estimation procedure of Lee et al. (2018) [4], introduce our estimation method [1]-[2], and conclude with evaluation metrics. For additional details, check out the references.</p>"},{"location":"methodsintro/#references","title":"References","text":"<p>[1]  Meir, Tomer, Gutman, Rom, and Gorfine, Malka,  \"PyDTS: A Python Package for Discrete-Time Survival Analysis with Competing Risks and Optional Penalization\", Journal of Open Source Software (2025),  doi: 10.21105/joss.08815</p> <p>[2]  Meir, Tomer and Gorfine, Malka,  \"Discrete-time Competing-Risks Regression with or without Penalization\", Biometrics (2025), doi: 10.1093/biomtc/ujaf040</p> <p>[3]  Allison, Paul D. \"Discrete-Time Methods for the Analysis of Event Histories\" Sociological Methodology (1982), doi: 10.2307/270718</p> <p>[4]  Lee, Minjung and Feuer, Eric J. and Fine, Jason P. \"On the analysis of discrete time competing risks data\" Biometrics (2018) doi: 10.1111/biom.12881</p> <p>[5]  Kalbfleisch, John D. and Prentice, Ross L. \"The Statistical Analysis of Failure Time Data\" 2nd Ed., Wiley (2011) ISBN: 978-1-118-03123-0</p> <p>[6]  Klein, John P. and Moeschberger, Melvin L. \"Survival Analysis\", Springer-Verlag (2003) ISBN: 978-0-387-95399-1</p>"},{"location":"api/cross_validation/","title":"Cross Validation","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV","title":"<code>pydts.cross_validation.TwoStagesCV()</code>","text":"<p>               Bases: <code>BaseTwoStagesCV</code></p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'CoxPHFitter'\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'CoxPHFitter'</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.models","title":"<code>models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.results","title":"<code>results = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.test_pids","title":"<code>test_pids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCV.cross_validate","title":"<code>cross_validate(full_df, n_splits=5, shuffle=True, seed=None, fit_beta_kwargs={}, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', x0=0, verbose=2, nb_workers=WORKERS, metrics=['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC'])</code>","text":"<p>This method implements K-fold cross-validation using TwoStagesFitters and full_df data.</p> <p>Parameters:</p> Name Type Description Default <code>full_df</code> <code>DataFrame</code> <p>Data to cross validate.</p> required <code>n_splits</code> <code>int</code> <p>Number of folds, defaults to 5.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Shuffle samples before splitting to folds. Defaults to True.</p> <code>True</code> <code>seed</code> <code>Union[int, None]</code> <p>Pseudo-random seed to KFold instance. Defaults to None.</p> <code>None</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here.</p> <code>{}</code> <code>covariates</code> <code>list</code> <p>list of covariates to be used in estimating the regression coefficients.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in full_df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in full_df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC']</code> <p>Returns:</p> Name Type Description <code>Results</code> <code>DataFrame</code> <p>Cross validation metrics results</p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def cross_validate(self,\n                   full_df: pd.DataFrame,\n                   n_splits: int = 5,\n                   shuffle: bool = True,\n                   seed: Union[int, None] = None,\n                   fit_beta_kwargs: dict = {},\n                   covariates=None,\n                   event_type_col: str = 'J',\n                   duration_col: str = 'X',\n                   pid_col: str = 'pid',\n                   x0: Union[np.array, int] = 0,\n                   verbose: int = 2,\n                   nb_workers: int = WORKERS,\n                   metrics=['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC']):\n\n    \"\"\"\n    This method implements K-fold cross-validation using TwoStagesFitters and full_df data.\n\n    Args:\n        full_df (pd.DataFrame): Data to cross validate.\n        n_splits (int): Number of folds, defaults to 5.\n        shuffle (bool): Shuffle samples before splitting to folds. Defaults to True.\n        seed: Pseudo-random seed to KFold instance. Defaults to None.\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here.\n        covariates (list): list of covariates to be used in estimating the regression coefficients.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in full_df).\n        pid_col (str): Sample ID column name (must be a column in full_df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        metrics (str, list): Evaluation metrics.\n\n    Returns:\n        Results (pd.DataFrame): Cross validation metrics results\n    \"\"\"\n\n    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    self.models = {}\n    self.kfold_cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)\n\n    if 'C' in full_df.columns:\n        full_df = full_df.drop(['C'], axis=1)\n    if 'T' in full_df.columns:\n        full_df = full_df.drop(['T'], axis=1)\n\n    for i_fold, (train_index, test_index) in enumerate(self.kfold_cv.split(full_df)):\n        self.test_pids[i_fold] = full_df.iloc[test_index][pid_col].values\n        train_df, test_df = full_df.iloc[train_index], full_df.iloc[test_index]\n        if self.TwoStagesFitter_type == 'Exact':\n            fold_fitter = TwoStagesFitterExact()\n        else:\n            fold_fitter = TwoStagesFitter()\n        print(f'Fitting fold {i_fold+1}/{n_splits}')\n        fold_fitter.fit(df=train_df,\n                        covariates=covariates,\n                        event_type_col=event_type_col,\n                        duration_col=duration_col,\n                        pid_col=pid_col,\n                        x0=x0,\n                        fit_beta_kwargs=fit_beta_kwargs,\n                        verbose=verbose,\n                        nb_workers=nb_workers)\n\n        #self.models[i_fold] = deepcopy(fold_fitter)\n        self.models[i_fold] = fold_fitter\n\n        pred_df = self.models[i_fold].predict_prob_events(test_df)\n\n        for metric in metrics:\n            if metric == 'IAUC':\n                self.integrated_auc[i_fold] = events_integrated_auc(pred_df, event_type_col=event_type_col,\n                                                                    duration_col=duration_col)\n            elif metric == 'GAUC':\n                self.global_auc[i_fold] = global_auc(pred_df, event_type_col=event_type_col,\n                                                              duration_col=duration_col)\n            elif metric == 'IBS':\n                self.integrated_bs[i_fold] = events_integrated_brier_score(pred_df, event_type_col=event_type_col,\n                                                                                    duration_col=duration_col)\n            elif metric == 'GBS':\n                self.global_bs[i_fold] = global_brier_score(pred_df, event_type_col=event_type_col,\n                                                                     duration_col=duration_col)\n            elif metric == 'AUC':\n                tmp_res = events_auc_at_t(pred_df, event_type_col=event_type_col,\n                                                   duration_col=duration_col)\n                tmp_res = pd.concat([tmp_res], keys=[i_fold], names=['fold'])\n                tmp_res = pd.concat([tmp_res], keys=[metric], names=['metric'])\n                self.results = pd.concat([self.results, tmp_res], axis=0)\n            elif metric == 'BS':\n                tmp_res = events_brier_score_at_t(pred_df, event_type_col=event_type_col,\n                                                           duration_col=duration_col)\n                tmp_res = pd.concat([tmp_res], keys=[i_fold], names=['fold'])\n                tmp_res = pd.concat([tmp_res], keys=[metric], names=['metric'])\n                self.results = pd.concat([self.results, tmp_res], axis=0)\n\n    return self.results\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact","title":"<code>pydts.cross_validation.TwoStagesCVExact()</code>","text":"<p>               Bases: <code>BaseTwoStagesCV</code></p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'Exact'\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'Exact'</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.models","title":"<code>models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.results","title":"<code>results = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.test_pids","title":"<code>test_pids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.TwoStagesCVExact.cross_validate","title":"<code>cross_validate(full_df, n_splits=5, shuffle=True, seed=None, fit_beta_kwargs={}, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', x0=0, verbose=2, nb_workers=WORKERS, metrics=['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC'])</code>","text":"<p>This method implements K-fold cross-validation using TwoStagesFitters and full_df data.</p> <p>Parameters:</p> Name Type Description Default <code>full_df</code> <code>DataFrame</code> <p>Data to cross validate.</p> required <code>n_splits</code> <code>int</code> <p>Number of folds, defaults to 5.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Shuffle samples before splitting to folds. Defaults to True.</p> <code>True</code> <code>seed</code> <code>Union[int, None]</code> <p>Pseudo-random seed to KFold instance. Defaults to None.</p> <code>None</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here.</p> <code>{}</code> <code>covariates</code> <code>list</code> <p>list of covariates to be used in estimating the regression coefficients.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in full_df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in full_df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC']</code> <p>Returns:</p> Name Type Description <code>Results</code> <code>DataFrame</code> <p>Cross validation metrics results</p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def cross_validate(self,\n                   full_df: pd.DataFrame,\n                   n_splits: int = 5,\n                   shuffle: bool = True,\n                   seed: Union[int, None] = None,\n                   fit_beta_kwargs: dict = {},\n                   covariates=None,\n                   event_type_col: str = 'J',\n                   duration_col: str = 'X',\n                   pid_col: str = 'pid',\n                   x0: Union[np.array, int] = 0,\n                   verbose: int = 2,\n                   nb_workers: int = WORKERS,\n                   metrics=['BS', 'IBS', 'GBS', 'AUC', 'IAUC', 'GAUC']):\n\n    \"\"\"\n    This method implements K-fold cross-validation using TwoStagesFitters and full_df data.\n\n    Args:\n        full_df (pd.DataFrame): Data to cross validate.\n        n_splits (int): Number of folds, defaults to 5.\n        shuffle (bool): Shuffle samples before splitting to folds. Defaults to True.\n        seed: Pseudo-random seed to KFold instance. Defaults to None.\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure. If different model for beta is desired, it can be defined here.\n        covariates (list): list of covariates to be used in estimating the regression coefficients.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in full_df).\n        pid_col (str): Sample ID column name (must be a column in full_df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        metrics (str, list): Evaluation metrics.\n\n    Returns:\n        Results (pd.DataFrame): Cross validation metrics results\n    \"\"\"\n\n    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    self.models = {}\n    self.kfold_cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)\n\n    if 'C' in full_df.columns:\n        full_df = full_df.drop(['C'], axis=1)\n    if 'T' in full_df.columns:\n        full_df = full_df.drop(['T'], axis=1)\n\n    for i_fold, (train_index, test_index) in enumerate(self.kfold_cv.split(full_df)):\n        self.test_pids[i_fold] = full_df.iloc[test_index][pid_col].values\n        train_df, test_df = full_df.iloc[train_index], full_df.iloc[test_index]\n        if self.TwoStagesFitter_type == 'Exact':\n            fold_fitter = TwoStagesFitterExact()\n        else:\n            fold_fitter = TwoStagesFitter()\n        print(f'Fitting fold {i_fold+1}/{n_splits}')\n        fold_fitter.fit(df=train_df,\n                        covariates=covariates,\n                        event_type_col=event_type_col,\n                        duration_col=duration_col,\n                        pid_col=pid_col,\n                        x0=x0,\n                        fit_beta_kwargs=fit_beta_kwargs,\n                        verbose=verbose,\n                        nb_workers=nb_workers)\n\n        #self.models[i_fold] = deepcopy(fold_fitter)\n        self.models[i_fold] = fold_fitter\n\n        pred_df = self.models[i_fold].predict_prob_events(test_df)\n\n        for metric in metrics:\n            if metric == 'IAUC':\n                self.integrated_auc[i_fold] = events_integrated_auc(pred_df, event_type_col=event_type_col,\n                                                                    duration_col=duration_col)\n            elif metric == 'GAUC':\n                self.global_auc[i_fold] = global_auc(pred_df, event_type_col=event_type_col,\n                                                              duration_col=duration_col)\n            elif metric == 'IBS':\n                self.integrated_bs[i_fold] = events_integrated_brier_score(pred_df, event_type_col=event_type_col,\n                                                                                    duration_col=duration_col)\n            elif metric == 'GBS':\n                self.global_bs[i_fold] = global_brier_score(pred_df, event_type_col=event_type_col,\n                                                                     duration_col=duration_col)\n            elif metric == 'AUC':\n                tmp_res = events_auc_at_t(pred_df, event_type_col=event_type_col,\n                                                   duration_col=duration_col)\n                tmp_res = pd.concat([tmp_res], keys=[i_fold], names=['fold'])\n                tmp_res = pd.concat([tmp_res], keys=[metric], names=['metric'])\n                self.results = pd.concat([self.results, tmp_res], axis=0)\n            elif metric == 'BS':\n                tmp_res = events_brier_score_at_t(pred_df, event_type_col=event_type_col,\n                                                           duration_col=duration_col)\n                tmp_res = pd.concat([tmp_res], keys=[i_fold], names=['fold'])\n                tmp_res = pd.concat([tmp_res], keys=[metric], names=['metric'])\n                self.results = pd.concat([self.results, tmp_res], axis=0)\n\n    return self.results\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV","title":"<code>pydts.cross_validation.PenaltyGridSearchCV()</code>","text":"<p>               Bases: <code>BasePenaltyGridSearchCV</code></p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'CoxPHFitter'\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'CoxPHFitter'</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.folds_grids","title":"<code>folds_grids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.test_pids","title":"<code>test_pids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCV.cross_validate","title":"<code>cross_validate(full_df, l1_ratio, penalizers, n_splits=5, shuffle=True, seed=None, event_type_col='J', duration_col='X', pid_col='pid', twostages_fit_kwargs={'nb_workers': WORKERS}, metrics=['IBS', 'GBS', 'IAUC', 'GAUC'])</code>","text":"<p>This method implements K-fold cross-validation using PenaltyGridSearch and full_df data.</p> <p>Parameters:</p> Name Type Description Default <code>full_df</code> <code>DataFrame</code> <p>Data to cross validate.</p> required <code>l1_ratio</code> <code>float</code> <p>regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>penalizers</code> <code>list</code> <p>penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>n_splits</code> <code>int</code> <p>Number of folds, defaults to 5.</p> <code>5</code> <code>shuffle</code> <code>boolean</code> <p>Shuffle samples before splitting to folds. Defaults to True.</p> <code>True</code> <code>seed</code> <code>Union[int, None]</code> <p>Pseudo-random seed to KFold instance. Defaults to None.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in full_df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in full_df).</p> <code>'pid'</code> <code>twostages_fit_kwargs</code> <code>dict</code> <p>keyword arguments to pass to each TwoStagesFitter.</p> <code>{'nb_workers': WORKERS}</code> <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['IBS', 'GBS', 'IAUC', 'GAUC']</code> <p>Returns:</p> Name Type Description <code>gauc_output_df</code> <code>DataFrame</code> <p>Global AUC k-fold mean and standard error for all possible combination of the penalizers.</p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def cross_validate(self,\n                   full_df: pd.DataFrame,\n                   l1_ratio: float,\n                   penalizers: list,\n                   n_splits: int = 5,\n                   shuffle: bool = True,\n                   seed: Union[int, None] = None,\n                   event_type_col: str = 'J',\n                   duration_col: str = 'X',\n                   pid_col: str = 'pid',\n                   twostages_fit_kwargs: dict = {'nb_workers': WORKERS},\n                   metrics=['IBS', 'GBS', 'IAUC', 'GAUC']) -&gt; pd.DataFrame:\n\n    \"\"\"\n    This method implements K-fold cross-validation using PenaltyGridSearch and full_df data.\n\n    Args:\n        full_df (pd.DataFrame): Data to cross validate.\n        l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        n_splits (int): Number of folds, defaults to 5.\n        shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True.\n        seed: Pseudo-random seed to KFold instance. Defaults to None.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in full_df).\n        pid_col (str): Sample ID column name (must be a column in full_df).\n        twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter.\n        metrics (str, list): Evaluation metrics.\n\n    Returns:\n        gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers.\n    \"\"\"\n\n    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    self.folds_grids = {}\n    self.kfold_cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)\n\n    if 'C' in full_df.columns:\n        full_df = full_df.drop(['C'], axis=1)\n    if 'T' in full_df.columns:\n        full_df = full_df.drop(['T'], axis=1)\n\n    for i_fold, (train_index, test_index) in enumerate(self.kfold_cv.split(full_df)):\n        print(f'Starting fold {i_fold+1}/{n_splits}')\n        start = time()\n        self.test_pids[i_fold] = full_df.iloc[test_index][pid_col].values\n        train_df, test_df = full_df.iloc[train_index], full_df.iloc[test_index]\n        if self.TwoStagesFitter_type == 'Exact':\n            fold_pgs = PenaltyGridSearchExact()\n        else:\n            fold_pgs = PenaltyGridSearch()\n\n        fold_pgs.evaluate(train_df=train_df,\n                          test_df=test_df,\n                          l1_ratio=l1_ratio,\n                          penalizers=penalizers,\n                          metrics=metrics,\n                          seed=seed,\n                          event_type_col=event_type_col,\n                          duration_col=duration_col,\n                          pid_col=pid_col,\n                          twostages_fit_kwargs=twostages_fit_kwargs)\n\n        self.folds_grids[i_fold] = fold_pgs\n\n        for metric in metrics:\n            if metric == 'GAUC':\n                self.global_auc[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.global_auc)\n            elif metric == 'IAUC':\n                self.integrated_auc[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.integrated_auc)\n            elif metric == 'GBS':\n                self.global_bs[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.global_bs)\n            elif metric == 'IBS':\n                self.integrated_bs[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.integrated_bs)\n\n        end = time()\n        print(f'Finished fold {i_fold+1}/{n_splits}, {int(end-start)} seconds')\n\n    if 'GAUC' in metrics:\n        res = [v for k, v in self.global_auc.items()]\n        gauc_output_df = pd.concat([pd.concat(res, axis=1).mean(axis=1),\n                                    pd.concat(res, axis=1).std(axis=1)],\n                                   keys=['Mean', 'SE'], axis=1)\n    else:\n        gauc_output_df = pd.DataFrame()\n    return gauc_output_df\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact","title":"<code>pydts.cross_validation.PenaltyGridSearchCVExact()</code>","text":"<p>               Bases: <code>BasePenaltyGridSearchCV</code></p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'Exact'\n</code></pre>"},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'Exact'</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.folds_grids","title":"<code>folds_grids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.test_pids","title":"<code>test_pids = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/cross_validation/#pydts.cross_validation.PenaltyGridSearchCVExact.cross_validate","title":"<code>cross_validate(full_df, l1_ratio, penalizers, n_splits=5, shuffle=True, seed=None, event_type_col='J', duration_col='X', pid_col='pid', twostages_fit_kwargs={'nb_workers': WORKERS}, metrics=['IBS', 'GBS', 'IAUC', 'GAUC'])</code>","text":"<p>This method implements K-fold cross-validation using PenaltyGridSearch and full_df data.</p> <p>Parameters:</p> Name Type Description Default <code>full_df</code> <code>DataFrame</code> <p>Data to cross validate.</p> required <code>l1_ratio</code> <code>float</code> <p>regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>penalizers</code> <code>list</code> <p>penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>n_splits</code> <code>int</code> <p>Number of folds, defaults to 5.</p> <code>5</code> <code>shuffle</code> <code>boolean</code> <p>Shuffle samples before splitting to folds. Defaults to True.</p> <code>True</code> <code>seed</code> <code>Union[int, None]</code> <p>Pseudo-random seed to KFold instance. Defaults to None.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in full_df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in full_df).</p> <code>'pid'</code> <code>twostages_fit_kwargs</code> <code>dict</code> <p>keyword arguments to pass to each TwoStagesFitter.</p> <code>{'nb_workers': WORKERS}</code> <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['IBS', 'GBS', 'IAUC', 'GAUC']</code> <p>Returns:</p> Name Type Description <code>gauc_output_df</code> <code>DataFrame</code> <p>Global AUC k-fold mean and standard error for all possible combination of the penalizers.</p> Source code in <code>src/pydts/cross_validation.py</code> <pre><code>def cross_validate(self,\n                   full_df: pd.DataFrame,\n                   l1_ratio: float,\n                   penalizers: list,\n                   n_splits: int = 5,\n                   shuffle: bool = True,\n                   seed: Union[int, None] = None,\n                   event_type_col: str = 'J',\n                   duration_col: str = 'X',\n                   pid_col: str = 'pid',\n                   twostages_fit_kwargs: dict = {'nb_workers': WORKERS},\n                   metrics=['IBS', 'GBS', 'IAUC', 'GAUC']) -&gt; pd.DataFrame:\n\n    \"\"\"\n    This method implements K-fold cross-validation using PenaltyGridSearch and full_df data.\n\n    Args:\n        full_df (pd.DataFrame): Data to cross validate.\n        l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        n_splits (int): Number of folds, defaults to 5.\n        shuffle (boolean): Shuffle samples before splitting to folds. Defaults to True.\n        seed: Pseudo-random seed to KFold instance. Defaults to None.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in full_df).\n        pid_col (str): Sample ID column name (must be a column in full_df).\n        twostages_fit_kwargs (dict): keyword arguments to pass to each TwoStagesFitter.\n        metrics (str, list): Evaluation metrics.\n\n    Returns:\n        gauc_output_df (pd.DataFrame): Global AUC k-fold mean and standard error for all possible combination of the penalizers.\n    \"\"\"\n\n    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    self.folds_grids = {}\n    self.kfold_cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)\n\n    if 'C' in full_df.columns:\n        full_df = full_df.drop(['C'], axis=1)\n    if 'T' in full_df.columns:\n        full_df = full_df.drop(['T'], axis=1)\n\n    for i_fold, (train_index, test_index) in enumerate(self.kfold_cv.split(full_df)):\n        print(f'Starting fold {i_fold+1}/{n_splits}')\n        start = time()\n        self.test_pids[i_fold] = full_df.iloc[test_index][pid_col].values\n        train_df, test_df = full_df.iloc[train_index], full_df.iloc[test_index]\n        if self.TwoStagesFitter_type == 'Exact':\n            fold_pgs = PenaltyGridSearchExact()\n        else:\n            fold_pgs = PenaltyGridSearch()\n\n        fold_pgs.evaluate(train_df=train_df,\n                          test_df=test_df,\n                          l1_ratio=l1_ratio,\n                          penalizers=penalizers,\n                          metrics=metrics,\n                          seed=seed,\n                          event_type_col=event_type_col,\n                          duration_col=duration_col,\n                          pid_col=pid_col,\n                          twostages_fit_kwargs=twostages_fit_kwargs)\n\n        self.folds_grids[i_fold] = fold_pgs\n\n        for metric in metrics:\n            if metric == 'GAUC':\n                self.global_auc[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.global_auc)\n            elif metric == 'IAUC':\n                self.integrated_auc[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.integrated_auc)\n            elif metric == 'GBS':\n                self.global_bs[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.global_bs)\n            elif metric == 'IBS':\n                self.integrated_bs[i_fold] = fold_pgs.convert_results_dict_to_df(fold_pgs.integrated_bs)\n\n        end = time()\n        print(f'Finished fold {i_fold+1}/{n_splits}, {int(end-start)} seconds')\n\n    if 'GAUC' in metrics:\n        res = [v for k, v in self.global_auc.items()]\n        gauc_output_df = pd.concat([pd.concat(res, axis=1).mean(axis=1),\n                                    pd.concat(res, axis=1).std(axis=1)],\n                                   keys=['Mean', 'SE'], axis=1)\n    else:\n        gauc_output_df = pd.DataFrame()\n    return gauc_output_df\n</code></pre>"},{"location":"api/data_expansion_fitter/","title":"Data Expansion Procedure of Lee et al. (2018)","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter","title":"<code>pydts.fitters.DataExpansionFitter()</code>","text":"<p>               Bases: <code>ExpansionBasedFitter</code></p> <p>This class implements the estimation procedure of Lee et al. (2018) [1]. See also the Example section.</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.models_kwargs = dict(family=sm.families.Binomial())\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.covariates","title":"<code>covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.duration_col","title":"<code>duration_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.event_models","title":"<code>event_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.event_type_col","title":"<code>event_type_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.events","title":"<code>events = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.expanded_df","title":"<code>expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.formula","title":"<code>formula = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.models_kwargs","title":"<code>models_kwargs = dict(family=(sm.families.Binomial()))</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.pid_col","title":"<code>pid_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.times","title":"<code>times = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter._expand_data","title":"<code>_expand_data(df, event_type_col, duration_col, pid_col)</code>","text":"<p>This method expands the raw data as explained in Lee et al. 2018</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to expand.</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df),                   Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> required <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Expanded df (pandas.DataFrame): the expanded dataframe.</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _expand_data(self,\n                 df: pd.DataFrame,\n                 event_type_col: str,\n                 duration_col: str,\n                 pid_col: str) -&gt; pd.DataFrame:\n    \"\"\"\n    This method expands the raw data as explained in Lee et al. 2018\n\n    Args:\n        df (pandas.DataFrame): Dataframe to expand.\n        event_type_col (str): The event type column name (must be a column in df),\n                              Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n\n    Returns:\n        Expanded df (pandas.DataFrame): the expanded dataframe.\n    \"\"\"\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    return get_expanded_df(df=df, event_type_col=event_type_col, duration_col=duration_col, pid_col=pid_col)\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter._fit_event","title":"<code>_fit_event(model_fit_kwargs={})</code>","text":"<p>This method fits a model for a GLM model for a specific event.</p> <p>Parameters:</p> Name Type Description Default <code>model_fit_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to model.fit() method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>fitted GLM model</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def _fit_event(self, model_fit_kwargs={}):\n    \"\"\"\n    This method fits a model for a GLM model for a specific event.\n\n    Args:\n        model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method.\n\n    Returns:\n        fitted GLM model\n    \"\"\"\n    model = sm.GLM.from_formula(formula=self.formula, data=self.expanded_df, **self.models_kwargs)\n    return model.fit(**model_fit_kwargs)\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter._validate_cols","title":"<code>_validate_cols(df, event_type_col, duration_col, pid_col)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_cols(self, df, event_type_col, duration_col, pid_col):\n    assert event_type_col in df.columns, f'Event type column is missing from df: {event_type_col}'\n    assert duration_col in df.columns, f'Duration column is missing from df: {duration_col}'\n    assert pid_col in df.columns, f'Observation ID column is missing from df: {pid_col}'\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter._validate_covariates_in_df","title":"<code>_validate_covariates_in_df(df)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_covariates_in_df(self, df):\n    cov_not_fitted = []\n    if isinstance(self.covariates, list):\n        cov_not_fitted = [cov for cov in self.covariates if cov not in df.columns]\n    elif isinstance(self.covariates, dict):\n        for event in self.events:\n            event_cov_not_fitted = [cov for cov in self.covariates[event] if cov not in df.columns]\n            cov_not_fitted.extend(event_cov_not_fitted)\n    assert len(cov_not_fitted) == 0, \\\n        f\"Cannot predict - required covariates are missing from df: {cov_not_fitted}\"\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter._validate_t","title":"<code>_validate_t(t, return_iter=True)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_t(self, t, return_iter=True):\n    _t = np.array([t]) if not isinstance(t, Iterable) else t\n    t_i_not_fitted = [t_i for t_i in _t if (t_i not in self.times)]\n    assert len(t_i_not_fitted) == 0, \\\n        f\"Cannot predict for times which were not included during .fit(): {t_i_not_fitted}\"\n    if return_iter:\n        return _t\n    return t\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.evaluate","title":"<code>evaluate(test_df, oracle_col='T', **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def evaluate(self, test_df: pd.DataFrame, oracle_col: str = 'T', **kwargs) -&gt; float:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.fit","title":"<code>fit(df, event_type_col='J', duration_col='X', pid_col='pid', skip_expansion=False, covariates=None, formula=None, models_kwargs=None, model_fit_kwargs={})</code>","text":"<p>This method fits a model to the discrete data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>skip_expansion</code> <code>boolean</code> <p>Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.</p> <code>False</code> <code>covariates</code> <code>(list, Optional)</code> <p>A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col.</p> <code>None</code> <code>formula</code> <code>(str, Optional)</code> <p>Model formula to be fitted. Patsy format string.</p> <code>None</code> <code>models_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to model instance initiation.</p> <code>None</code> <code>model_fit_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to model.fit() method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>event_models</code> <code>dict</code> <p>Fitted models dictionary. Keys - event names, Values - fitted models for the event.</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def fit(self,\n        df: pd.DataFrame,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid',\n        skip_expansion: bool = False,\n        covariates: Optional[list] = None,\n        formula: Optional[str] = None,\n        models_kwargs: Optional[dict] = None,\n        model_fit_kwargs: Optional[dict] = {}) -&gt; dict:\n    \"\"\"\n    This method fits a model to the discrete data.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        event_type_col (str): The event type column name (must be a column in df), Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.\n        covariates (list, Optional): A list of covariates, all must be columns in df. Defaults to all the columns of df except event_type_col, duration_col, and pid_col.\n        formula (str, Optional): Model formula to be fitted. Patsy format string.\n        models_kwargs (dict, Optional): Keyword arguments to pass to model instance initiation.\n        model_fit_kwargs (dict, Optional): Keyword arguments to pass to model.fit() method.\n\n    Returns:\n        event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event.\n    \"\"\"\n\n    if models_kwargs is not None:\n        self.models_kwargs = models_kwargs\n\n    if 'C' in df.columns:\n        raise ValueError('C is an invalid column name, to avoid errors with categorical symbol C() in formula')\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    if covariates is not None:\n        cov_not_in_df = [cov for cov in covariates if cov not in df.columns]\n        if len(cov_not_in_df) &gt; 0:\n            raise ValueError(f\"Error during fit - missing covariates from df: {cov_not_in_df}\")\n\n    self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    self.covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]] \\\n                      if covariates is None else covariates\n    self.times = sorted(df[duration_col].unique())\n\n    if not skip_expansion:\n        self.expanded_df = self._expand_data(df=df, event_type_col=event_type_col, duration_col=duration_col,\n                                             pid_col=pid_col)\n    else:\n        print('Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.')\n        self.expanded_df = df\n\n    for event in self.events:\n        cov = ' + '.join(self.covariates)\n        _formula = f'j_{event} ~ {formula}' if formula is not None else \\\n            f'j_{event} ~ {cov} + C({duration_col}) -1 '\n        self.formula = _formula\n        self.event_models[event] = self._fit_event(model_fit_kwargs=model_fit_kwargs)\n    return self.event_models\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.get_alpha_df","title":"<code>get_alpha_df()</code>","text":"<p>This function returns the Alpha coefficients and their Standard Errors for all the events.</p> <p>Returns:</p> Name Type Description <code>se_df</code> <code>DataFrame</code> <p>Alpha coefficients and Standard Errors Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_alpha_df(self):\n    \"\"\"\n    This function returns the Alpha coefficients and their Standard Errors for all the events.\n\n    Returns:\n        se_df (pandas.DataFrame): Alpha coefficients and Standard Errors Dataframe\n    \"\"\"\n\n    full_table = pd.DataFrame()\n    for event in self.events:\n        summary = self.event_models[event].summary()\n        summary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\n        summary_df.columns = summary_df.iloc[0]\n        summary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\n        summary_df.columns = pd.MultiIndex.from_product([[event], summary_df.columns])\n        full_table = pd.concat([full_table, summary_df.iloc[:-len(self.covariates)-1]], axis=1)\n    return full_table\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.get_beta_SE","title":"<code>get_beta_SE()</code>","text":"<p>This function returns the Beta coefficients and their Standard Errors for all the events.</p> <p>Returns:</p> Name Type Description <code>se_df</code> <code>DataFrame</code> <p>Beta coefficients and Standard Errors Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_beta_SE(self):\n    \"\"\"\n    This function returns the Beta coefficients and their Standard Errors for all the events.\n\n    Returns:\n        se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe\n    \"\"\"\n\n    full_table = pd.DataFrame()\n    for event in self.events:\n        summary = self.event_models[event].summary()\n        summary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\n        summary_df.columns = summary_df.iloc[0]\n        summary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\n        summary_df.columns = pd.MultiIndex.from_product([[event], summary_df.columns])\n        full_table = pd.concat([full_table, summary_df.iloc[-len(self.covariates):]], axis=1)\n    return full_table\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict","title":"<code>predict(df, **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict(self, df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_cumulative_incident_function","title":"<code>predict_cumulative_incident_function(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_cumulative_incident_function(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        if f'cif_j{event}_at_t{self.times[-2]}' not in df.columns:\n            df = self.predict_event_cumulative_incident_function(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_event_cumulative_incident_function","title":"<code>predict_event_cumulative_incident_function(df, event)</code>","text":"<p>This function adds a specific event columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_event_cumulative_incident_function(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a specific event columns of the predicted hazard function, overall survival, probabilities\n    of event occurance and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_events(df=df)\n    cols = [f'prob_j{event}_at_t{t}' for t in self.times[:-1]]\n    cif_df = df[cols].cumsum(axis=1)\n    cif_df.columns = [f'cif_j{event}_at_t{t}' for t in self.times[:-1]]\n    df = pd.concat([df, cif_df], axis=1)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_full","title":"<code>predict_full(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_full(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    return self.predict_cumulative_incident_function(df)\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_hazard_all","title":"<code>predict_hazard_all(df)</code>","text":"<p>This function calculates the hazard for all the events at all time values included in the training set for each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_all(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at all time values included in the training set for each\n    event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    df = self.predict_hazard_t(df, t=self.times[:-1])\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_hazard_jt","title":"<code>predict_hazard_jt(df, event, t, n_jobs=-1)</code>","text":"<p>This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>array</code> <p>times to calculate the hazard for</p> required <code>n_jobs</code> <code>int</code> <p>number of CPUs to use, defualt to every available CPU</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def predict_hazard_jt(self,\n                      df: pd.DataFrame,\n                      event: Union[str, int],\n                      t: Union[Iterable, int],\n                      n_jobs: int = -1) -&gt; pd.DataFrame:\n    \"\"\"\n    This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        event (Union[str, int]): event name\n        t (np.array): times to calculate the hazard for\n        n_jobs: number of CPUs to use, defualt to every available CPU\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    t = self._validate_t(t, return_iter=True)\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    _t = np.array([t_i for t_i in t if (f'hazard_j{event}_t{t_i}' not in df.columns)])\n    if len(_t) == 0:\n        return df\n\n    temp_df = df.copy()\n    model = self.event_models[event]\n    res = Parallel(n_jobs=n_jobs)(delayed(model.predict)(df[self.covariates].assign(X=c)) for c in t)\n    temp_hazard_df = pd.concat(res, axis=1)\n    temp_df[[f'hazard_j{event}_t{c_}' for c_ in t]] = temp_hazard_df.values\n    return temp_df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_hazard_t","title":"<code>predict_hazard_t(df, t)</code>","text":"<p>This function calculates the hazard for all the events at the requested time values if they were included in the training set of each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>t</code> <code>(int, array)</code> <p>times to calculate the hazard for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_t(self, df: pd.DataFrame, t: Union[int, np.array]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at the requested time values if they were included in\n    the training set of each event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        t (int, np.array): times to calculate the hazard for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    t = self._validate_t(t)\n    self._validate_covariates_in_df(df.head())\n\n    for event, model in self.event_models.items():\n        df = self.predict_hazard_jt(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_marginal_prob_all_events","title":"<code>predict_marginal_prob_all_events(df)</code>","text":"<p>This function calculates the marginal probability per event given the covariates for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_all_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability per event given the covariates for all the events.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    for event in self.events:\n        df = self.predict_marginal_prob_event_j(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_marginal_prob_event_j","title":"<code>predict_marginal_prob_event_j(df, event)</code>","text":"<p>This function calculates the marginal probability of an event given the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_event_j(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability of an event given the covariates.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    cols = [f'prob_j{event}_at_t{_t}' for _t in self.times[:-1]]\n    marginal_prob = df[cols].sum(axis=1)\n    marginal_prob.name = f'marginal_prob_j{event}'\n    return pd.concat([df, marginal_prob], axis=1)\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_overall_survival","title":"<code>predict_overall_survival(df, t=None, return_hazards=False)</code>","text":"<p>This function adds columns of the overall survival until time t. Args:     df (pandas.DataFrame): dataframe with covariates columns     t (int): time     return_hazards (bool): if to keep the hazard columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with the additional overall survival columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_overall_survival(self,\n                             df: pd.DataFrame,\n                             t: int = None,\n                             return_hazards: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the overall survival until time t.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        t (int): time\n        return_hazards (bool): if to keep the hazard columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with the additional overall survival columns\n\n    \"\"\"\n    if t is not None:\n        self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    all_hazards = self.predict_hazard_all(df)\n    _times = self.times[:-1] if t is None else [_t for _t in self.times[:-1] if _t &lt;= t]\n    overall = pd.DataFrame()\n    for t_i in _times:\n        cols = [f'hazard_j{e}_t{t_i}' for e in self.events]\n        t_i_hazard = 1 - all_hazards[cols].sum(axis=1)\n        t_i_hazard.name = f'overall_survival_t{t_i}'\n        overall = pd.concat([overall, t_i_hazard], axis=1)\n    overall = pd.concat([df, overall.cumprod(axis=1)], axis=1)\n\n    if return_hazards:\n        cols = all_hazards.columns[all_hazards.columns.str.startswith(\"hazard_\")]\n        cols = cols.difference(overall.columns)\n        if len(cols) &gt; 0:\n            overall = pd.concat([overall, all_hazards[cols]], axis=1)\n    return overall\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_prob_event_j_all","title":"<code>predict_prob_event_j_all(df, event)</code>","text":"<p>This function adds columns of a specific event occurrence probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_all(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of a specific event occurrence probabilities.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'overall_survival_t{self.times[-2]}' not in df.columns:\n        df = self.predict_overall_survival(df, return_hazards=True)\n    for t in self.times[:-1]:\n        if f'prob_j{event}_at_t{t}' not in df.columns:\n            df = self.predict_prob_event_j_at_t(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_prob_event_j_at_t","title":"<code>predict_prob_event_j_at_t(df, event, t)</code>","text":"<p>This function adds a column with probability of occurance of a specific event at a specific a time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>int</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe an additional probability column</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_at_t(self, df: pd.DataFrame, event: Union[str, int], t: int) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a column with probability of occurance of a specific event at a specific a time.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n        t (int): time\n\n    Returns:\n        df (pandas.DataFrame): dataframe an additional probability column\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{t}' not in df.columns:\n        if t == 1:\n            if f'hazard_j{event}_t{t}' not in df.columns:\n                df = self.predict_hazard_jt(df=df, event=event, t=t)\n            df[f'prob_j{event}_at_t{t}'] = df[f'hazard_j{event}_t{t}']\n            return df\n        elif not f'overall_survival_t{t - 1}' in df.columns:\n            df = self.predict_overall_survival(df, t=t, return_hazards=True)\n        elif not f'hazard_j{event}_t{t}' in df.columns:\n            df = self.predict_hazard_t(df, t=np.array([_t for _t in self.times[:-1] if _t &lt;= t]))\n        df[f'prob_j{event}_at_t{t}'] = df[f'overall_survival_t{t - 1}'] * df[f'hazard_j{event}_t{t}']\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.predict_prob_events","title":"<code>predict_prob_events(df)</code>","text":"<p>This function adds columns of all the events occurance probabilities. Args:     df (pandas.DataFrame): dataframe with covariates columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of all the events occurance probabilities.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/data_expansion_fitter/#pydts.fitters.DataExpansionFitter.print_summary","title":"<code>print_summary(summary_func='summary', summary_kwargs={})</code>","text":"<p>This method prints the summary of the fitted models for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>summary_func</code> <code>(str, Optional)</code> <p>print summary method of the fitted model type (\"summary\", \"print_summary\").</p> <code>'summary'</code> <code>summary_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to the model summary function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def print_summary(self,\n                  summary_func: str = \"summary\",\n                  summary_kwargs: dict = {}) -&gt; None:\n    \"\"\"\n    This method prints the summary of the fitted models for all the events.\n\n    Args:\n        summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\").\n        summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function.\n\n    Returns:\n        None\n    \"\"\"\n    for event, model in self.event_models.items():\n        _summary_func = getattr(model, summary_func, None)\n        if _summary_func is not None:\n            print(f'\\n\\nModel summary for event: {event}')\n            print(_summary_func(**summary_kwargs))\n        else:\n            print(f'Not {summary_func} function in event {event} model')\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation","text":""},{"location":"api/evaluation/#pydts.evaluation","title":"<code>pydts.evaluation</code>","text":""},{"location":"api/evaluation/#pydts.evaluation.slicer","title":"<code>slicer = pd.IndexSlice</code>  <code>module-attribute</code>","text":""},{"location":"api/evaluation/#pydts.evaluation.event_specific_auc_at_t","title":"<code>event_specific_auc_at_t(pred_df, event, t, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the event specific AUC at time t.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the integrated AUC for.</p> required <code>t</code> <code>int</code> <p>time to calculate the AUC for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Series</code> <p>event specific AUC for all times included in duration_col of pred_df.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_auc_at_t(pred_df: pd.DataFrame,\n                            event: int,\n                            t: int,\n                            event_type_col: str = 'J',\n                            duration_col: str = 'X') -&gt; float:\n    \"\"\"\n    This function implements the calculation of the event specific AUC at time t.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the integrated AUC for.\n        t (int): time to calculate the AUC for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        result (pd.Series): event specific AUC for all times included in duration_col of pred_df.\n    \"\"\"\n\n    event_observed_at_t_df = pred_df[(pred_df[event_type_col] == event) &amp; (pred_df[duration_col] == t)]\n    no_event_at_t_df = pred_df[pred_df[duration_col] &gt;= t]\n    no_event_at_t_df = no_event_at_t_df[~((no_event_at_t_df[event_type_col] == event) &amp;\n                                          (no_event_at_t_df[duration_col] == t))]\n    total_t = (len(event_observed_at_t_df)*len(no_event_at_t_df))\n    if total_t == 0:\n        print(f'AUC could not be calculated for event {event} at time {t} - no test pairs of with and without observed event {event} at time {t}')\n        return np.nan\n    correct_order = 0\n    for i_idx, i_row in event_observed_at_t_df.iterrows():\n        pi_ij = i_row.loc[f'prob_j{event}_at_t{t}']\n        pi_mj = no_event_at_t_df.loc[:, f'prob_j{event}_at_t{t}']\n        correct_order += ((pi_ij &gt; pi_mj).sum()+0.5*(pi_ij == pi_mj).sum())\n    return correct_order / total_t\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_auc_at_t_all","title":"<code>event_specific_auc_at_t_all(pred_df, event, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the AUC for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Series</code> <p>event specific AUC for all times included in duration_col of pred_df.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_auc_at_t_all(pred_df: pd.DataFrame,\n                                event: int,\n                                event_type_col: str = 'J',\n                                duration_col: str = 'X') -&gt; pd.Series:\n    \"\"\"\n    This function implements the calculation of the event specific AUC at time t for all times included in duration_col of pred_df.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the AUC for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        result (pd.Series): event specific AUC for all times included in duration_col of pred_df.\n    \"\"\"\n\n    res = {}\n    for t in sorted(pred_df[duration_col].unique())[:-1]:\n        res[t] = event_specific_auc_at_t(pred_df=pred_df, event=event, t=t,\n                                         event_type_col=event_type_col, duration_col=duration_col)\n    return pd.Series(res, name=event)\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_brier_score_at_t","title":"<code>event_specific_brier_score_at_t(pred_df, event, t, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the event specific Brier Score at time t.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the Brier Score for.</p> required <code>t</code> <code>int</code> <p>time to calculate the Brier Score for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Series</code> <p>event specific Brier Score at time t.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_brier_score_at_t(pred_df: pd.DataFrame,\n                                    event: int,\n                                    t: int,\n                                    event_type_col: str = 'J',\n                                    duration_col: str = 'X') -&gt; float:\n    \"\"\"\n    This function implements the calculation of the event specific Brier Score at time t.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate Brier Score for. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the Brier Score for.\n        t (int): time to calculate the Brier Score for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        result (pd.Series): event specific Brier Score at time t.\n    \"\"\"\n\n    pi_ij = pred_df.loc[:, f'prob_j{event}_at_t{t}']\n    D_ij = ((pred_df.loc[:, event_type_col] == event) &amp; (pred_df.loc[:, duration_col] == t)).astype(int)\n    censoring_kmf = KaplanMeierFitter()\n    censoring_kmf.fit(durations=pred_df[duration_col], event_observed=(pred_df[event_type_col] == 0))\n    in_risk_set_at_t = (pred_df.loc[:, duration_col] &gt;= t).astype(int)\n    W_ij = (in_risk_set_at_t / censoring_kmf.predict(times=t))\n    BS_jt = ((W_ij*((D_ij - pi_ij)**2)).sum() / in_risk_set_at_t.sum())\n    return BS_jt\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_brier_score_at_t_all","title":"<code>event_specific_brier_score_at_t_all(pred_df, event, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the Brier Score for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Series</code> <p>event specific Brier Score for all times included in duration_col of pred_df.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_brier_score_at_t_all(pred_df: pd.DataFrame,\n                                        event: int,\n                                        event_type_col: str = 'J',\n                                        duration_col: str = 'X') -&gt; pd.Series:\n    \"\"\"\n    This function implements the calculation of the event specific Brier Score at time t for all times included in duration_col of pred_df.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the Brier Score for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        result (pd.Series): event specific Brier Score for all times included in duration_col of pred_df.\n    \"\"\"\n\n    res = {}\n    for t in sorted(pred_df[duration_col].unique())[:-1]:\n        res[t] = event_specific_brier_score_at_t(pred_df=pred_df, event=event, t=t,\n                                                 event_type_col=event_type_col, duration_col=duration_col)\n    return pd.Series(res, name=event)\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_integrated_auc","title":"<code>event_specific_integrated_auc(pred_df, event, event_type_col='J', duration_col='X', weights=None)</code>","text":"<p>This function implements the calculation of the event specific integrated auc.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the integrated AUC for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>weights</code> <code>Series</code> <p>Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>float</code> <p>integrated AUC results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_integrated_auc(pred_df: pd.DataFrame,\n                                  event: int,\n                                  event_type_col: str = 'J',\n                                  duration_col: str = 'X',\n                                  weights: Union[pd.Series, None] = None) -&gt; float:\n    \"\"\"\n    This function implements the calculation of the event specific integrated auc.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the integrated AUC for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times.\n\n    Returns:\n        result (float): integrated AUC results.\n    \"\"\"\n\n    auc_at_t = event_specific_auc_at_t_all(pred_df=pred_df, event=event,\n                                           event_type_col=event_type_col,\n                                           duration_col=duration_col)\n\n    if auc_at_t.isnull().any():\n        print(f'There are NaN values in AUC(t) during Integrated AUC calculation for event {event}. Times: {auc_at_t[auc_at_t.isnull()].index}\\n')\n\n    if weights is None:\n        if auc_at_t.isnull().any():\n            print(f'Please check there are events of type {event} at each time in pred_df or provide a weights vector with weight 0 for the problematic times.')\n            return np.nan\n        weights = event_specific_weights(pred_df=pred_df, event=event,\n                                         event_type_col=event_type_col,\n                                         duration_col=duration_col)\n\n    result = auc_at_t.dot(weights.sort_index())\n    return result\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_integrated_brier_score","title":"<code>event_specific_integrated_brier_score(pred_df, event, event_type_col='J', duration_col='X', weights=None)</code>","text":"<p>This function implements the calculation of the event specific integrated Brier Score.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the integrated Brier Score for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>weights</code> <code>Series</code> <p>Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>result</code> <code>float</code> <p>integrated Brier Score results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_integrated_brier_score(pred_df: pd.DataFrame,\n                                          event: int,\n                                          event_type_col: str = 'J',\n                                          duration_col: str = 'X',\n                                          weights: Union[pd.Series, None] = None) -&gt; float:\n    \"\"\"\n    This function implements the calculation of the event specific integrated Brier Score.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the integrated Brier Score for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        weights (pd.Series): Optional. Weights vector with time as index and weight as value. Length must be the number of possible event times.\n\n    Returns:\n        result (float): integrated Brier Score results.\n    \"\"\"\n\n    brier_score_at_t = event_specific_brier_score_at_t_all(pred_df=pred_df, event=event,\n                                                           event_type_col=event_type_col,\n                                                           duration_col=duration_col)\n\n    if brier_score_at_t.isnull().any():\n        print(f'There are NaN values in BS(t) during Integrated Brier Score calculation for event {event}. Times: {brier_score_at_t[brier_score_at_t.isnull()].index}\\n')\n\n    if weights is None:\n        weights = event_specific_weights(pred_df=pred_df, event=event,\n                                         event_type_col=event_type_col,\n                                         duration_col=duration_col)\n\n    result = brier_score_at_t.dot(weights.sort_index())\n    return result\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.event_specific_weights","title":"<code>event_specific_weights(pred_df, event, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the event specific time-weights.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()</p> required <code>event</code> <code>int</code> <p>Event-type to calculate the weights for.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Series</code> <p>event specific weights.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def event_specific_weights(pred_df: pd.DataFrame,\n                           event: int,\n                           event_type_col: str = 'J',\n                           duration_col: str = 'X') -&gt; pd.Series:\n    \"\"\"\n    This function implements the calculation of the event specific time-weights.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for the event. See TwoStagesFitter.predict_prob_events()\n        event (int): Event-type to calculate the weights for.\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        result (pd.Series): event specific weights.\n    \"\"\"\n\n    event_df = pred_df[pred_df[event_type_col] == event]\n    if len(event_df) == 0:\n        print(f'Could not calculate weights for event {event} - no test events')\n        return np.nan\n    weights = event_df.groupby(duration_col).size() / event_df.groupby(duration_col).size().sum()\n    times = sorted(pred_df[duration_col].unique())[:-1]\n    return weights.reindex(times).fillna(0)\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.events_auc_at_t","title":"<code>events_auc_at_t(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the events AUC at t.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>event_auc_at_t_df</code> <code>DataFrame</code> <p>events AUC at t results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def events_auc_at_t(pred_df: pd.DataFrame,\n                    event_type_col: str = 'J',\n                    duration_col: str ='X') -&gt; pd.DataFrame:\n    \"\"\"\n    This function implements the calculation of the events AUC at t.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate AUC. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        event_auc_at_t_df (pd.DataFrame): events AUC at t results.\n    \"\"\"\n\n    events = [e for e in pred_df[event_type_col].unique() if e != 0]\n    event_auc_at_t_df = pd.DataFrame()\n    for event in sorted(events):\n        event_auc_at_t_df = pd.concat([event_auc_at_t_df,\n                                       event_specific_auc_at_t_all(pred_df=pred_df,\n                                                                   event=event,\n                                                                   event_type_col=event_type_col,\n                                                                   duration_col=duration_col)],\n                                       axis=1)\n    return event_auc_at_t_df.T\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.events_brier_score_at_t","title":"<code>events_brier_score_at_t(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the events Brier score at t.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>event_brier_score_at_t_df</code> <code>DataFrame</code> <p>events Brier score at t results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def events_brier_score_at_t(pred_df: pd.DataFrame,\n                            event_type_col: str = 'J',\n                            duration_col: str ='X') -&gt; pd.DataFrame:\n    \"\"\"\n    This function implements the calculation of the events Brier score at t.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        event_brier_score_at_t_df (pd.DataFrame): events Brier score at t results.\n    \"\"\"\n\n    events = [e for e in pred_df[event_type_col].unique() if e != 0]\n    event_brier_score_at_t_df = pd.DataFrame()\n    for event in sorted(events):\n        event_brier_score_at_t_df = pd.concat([event_brier_score_at_t_df,\n                                               event_specific_brier_score_at_t_all(pred_df=pred_df,\n                                                                                   event=event,\n                                                                                   event_type_col=event_type_col,\n                                                                                   duration_col=duration_col)],\n                                              axis=1)\n    return event_brier_score_at_t_df.T\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.events_integrated_auc","title":"<code>events_integrated_auc(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the integrated AUC to all events.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>integrated_auc</code> <code>dict</code> <p>integrated AUC results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def events_integrated_auc(pred_df: pd.DataFrame, event_type_col: str = 'J',\n                          duration_col: str ='X') -&gt; dict:\n    \"\"\"\n    This function implements the calculation of the integrated AUC to all events.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        integrated_auc (dict): integrated AUC results.\n    \"\"\"\n\n    events = [e for e in pred_df[event_type_col].unique() if e != 0]\n    integrated_auc = {}\n    for event in sorted(events):\n        integrated_auc[event] = event_specific_integrated_auc(pred_df=pred_df,\n                                                              event=event,\n                                                              event_type_col=event_type_col,\n                                                              duration_col=duration_col)\n    return integrated_auc\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.events_integrated_brier_score","title":"<code>events_integrated_brier_score(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the integrated Brier Score to all events.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>integrated_brier_score</code> <code>dict</code> <p>integrated Brier Score results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def events_integrated_brier_score(pred_df: pd.DataFrame, event_type_col: str = 'J',\n                                  duration_col: str ='X') -&gt; dict:\n    \"\"\"\n    This function implements the calculation of the integrated Brier Score to all events.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate integrated Brier Score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        integrated_brier_score (dict): integrated Brier Score results.\n    \"\"\"\n\n    events = [e for e in pred_df[event_type_col].unique() if e != 0]\n    integrated_brier_score = {}\n    for event in sorted(events):\n        integrated_brier_score[event] = event_specific_integrated_brier_score(pred_df=pred_df,\n                                                                              event=event,\n                                                                              event_type_col=event_type_col,\n                                                                              duration_col=duration_col)\n    return integrated_brier_score\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.global_auc","title":"<code>global_auc(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the global AUC.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>global_auc</code> <code>float</code> <p>global AUC results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def global_auc(pred_df: pd.DataFrame,\n               event_type_col: str = 'J',\n               duration_col: str = 'X') -&gt; float:\n    \"\"\"\n    This function implements the calculation of the global AUC.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate prediction error. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        global_auc (float): global AUC results.\n    \"\"\"\n\n    e_j_ser = pred_df[pred_df[event_type_col] != 0].groupby('J').size().sort_index()\n    total_e = e_j_ser.sum()\n    global_auc = 0\n    for event, e_j in e_j_ser.items():\n        global_auc += (e_j / total_e) * event_specific_integrated_auc(\n            pred_df=pred_df, event=event, event_type_col=event_type_col,\n            duration_col=duration_col)\n    return global_auc\n</code></pre>"},{"location":"api/evaluation/#pydts.evaluation.global_brier_score","title":"<code>global_brier_score(pred_df, event_type_col='J', duration_col='X')</code>","text":"<p>This function implements the calculation of the global Brier Score.</p> <p>Parameters:</p> Name Type Description Default <code>pred_df</code> <code>DataFrame</code> <p>Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in pred_df).</p> <code>'X'</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <p>Returns:</p> Name Type Description <code>global_auc</code> <code>float</code> <p>global Brier Score results.</p> Source code in <code>src/pydts/evaluation.py</code> <pre><code>def global_brier_score(pred_df: pd.DataFrame,\n                       event_type_col: str = 'J',\n                       duration_col: str = 'X') -&gt; float:\n    \"\"\"\n    This function implements the calculation of the global Brier Score.\n\n    Args:\n        pred_df (pd.DataFrame): Data to calculate Brier score. Must contain the observed duration and event-type, and the probability of event at time t prediction results for all events. See TwoStagesFitter.predict_prob_events()\n        duration_col (str): Last follow up time column name (must be a column in pred_df).\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n\n    Returns:\n        global_auc (float): global Brier Score results.\n    \"\"\"\n\n    e_j_ser = pred_df[pred_df[event_type_col] != 0].groupby('J').size().sort_index()\n    total_e = e_j_ser.sum()\n    global_bs = 0\n    for event, e_j in e_j_ser.items():\n        global_bs += (e_j / total_e) * event_specific_integrated_brier_score(\n            pred_df=pred_df, event=event, event_type_col=event_type_col,\n            duration_col=duration_col)\n    return global_bs\n</code></pre>"},{"location":"api/event_times_sampler/","title":"Event Times Sampler","text":""},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler","title":"<code>pydts.data_generation.EventTimesSampler(d_times, j_event_types)</code>","text":"<p>               Bases: <code>object</code></p> <p>This class implements sampling procedure for discrete event times and censoring times for given observations.</p> <p>Parameters:</p> Name Type Description Default <code>d_times</code> <code>int</code> <p>number of possible event times</p> required <code>j_event_types</code> <code>int</code> <p>number of possible event types</p> required Source code in <code>src/pydts/data_generation.py</code> <pre><code>def __init__(self, d_times: int, j_event_types: int):\n    \"\"\"\n    This class implements sampling procedure for discrete event times and censoring times for given observations.\n\n    Args:\n        d_times (int): number of possible event times\n        j_event_types (int): number of possible event types\n    \"\"\"\n\n    self.d_times = d_times\n    self.times = range(1, self.d_times + 2)  # d + 1 for administrative censoring\n    self.j_event_types = j_event_types\n    self.events = range(1, self.j_event_types + 1)\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.d_times","title":"<code>d_times = d_times</code>  <code>instance-attribute</code>","text":""},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.events","title":"<code>events = range(1, self.j_event_types + 1)</code>  <code>instance-attribute</code>","text":""},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.j_event_types","title":"<code>j_event_types = j_event_types</code>  <code>instance-attribute</code>","text":""},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.times","title":"<code>times = range(1, self.d_times + 2)</code>  <code>instance-attribute</code>","text":""},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler._validate_prob_dfs_list","title":"<code>_validate_prob_dfs_list(dfs_list, numerical_error_tolerance=0.001)</code>","text":"Source code in <code>src/pydts/data_generation.py</code> <pre><code>def _validate_prob_dfs_list(self, dfs_list: list, numerical_error_tolerance: float = 0.001) -&gt; list:\n    for df in dfs_list:\n        if (((df &lt; (0-numerical_error_tolerance)).any().any()) or ((df &gt; (1+numerical_error_tolerance)).any().any())):\n            raise ValueError(\"The chosen sampling parameters result in invalid probabilities for event j at time t\")\n        # Only fixes numerical errors smaller than the tolerance size\n        df.clip(0, 1, inplace=True)\n    return dfs_list\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calc_prob_t_given_j","title":"<code>calc_prob_t_given_j(prob_j_at_t, total_prob_j, numerical_error_tolerance=0.001)</code>","text":"<p>Calculates the conditional probability for event occurrance at time t given J_i=j.</p> <p>Parameters:</p> Name Type Description Default <code>prob_j_at_t</code> <code>list</code> <p>A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.</p> required <code>total_prob_j</code> <code>list</code> <p>A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.</p> required <code>numerical_error_tolerance</code> <code>float</code> <p>Tolerate numerical errors of probabilities up to this value.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>conditional_prob</code> <code>list</code> <p>A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def calc_prob_t_given_j(self, prob_j_at_t, total_prob_j, numerical_error_tolerance=0.001):\n    \"\"\"\n    Calculates the conditional probability for event occurrance at time t given J_i=j.\n\n    Args:\n        prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.\n        total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.\n        numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value.\n\n    Returns:\n        conditional_prob (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations.\n    \"\"\"\n    conditional_prob = [prob.div(sumj, axis=0) for prob, sumj in zip(prob_j_at_t, total_prob_j)]\n    conditional_prob = self._validate_prob_dfs_list(conditional_prob, numerical_error_tolerance)\n    return conditional_prob\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_hazards","title":"<code>calculate_hazards(observations_df, hazard_coefs, events=None)</code>","text":"<p>Calculates the hazard function for the observations given the hazard coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with observations covariates.</p> required <code>coefficients</code> <code>dict</code> <p>time coefficients and covariates coefficients for each event type.</p> required <p>Returns:</p> Name Type Description <code>hazards_dfs</code> <code>list</code> <p>A list of dataframes, one for each event type, with the hazard function at time t to each of the observations.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def calculate_hazards(self, observations_df: pd.DataFrame, hazard_coefs: dict, events: list = None) -&gt; list:\n    \"\"\"\n    Calculates the hazard function for the observations given the hazard coefficients.\n\n    Args:\n        observations_df (pd.DataFrame): Dataframe with observations covariates.\n        coefficients (dict): time coefficients and covariates coefficients for each event type.\n\n    Returns:\n        hazards_dfs (list): A list of dataframes, one for each event type, with the hazard function at time t to each of the observations.\n    \"\"\"\n    events = events if events is not None else self.events\n    a_t = {}\n    for event in events:\n        if callable(hazard_coefs['alpha'][event]):\n            a_t[event] = {t: hazard_coefs['alpha'][event](t) for t in range(1, self.d_times + 1)}\n        else:\n            a_t[event] = {t: hazard_coefs['alpha'][event][t-1] for t in range(1, self.d_times + 1)}\n    b = pd.concat([observations_df.dot(hazard_coefs['beta'][j]) for j in events], axis=1, keys=events)\n    hazards_dfs = [pd.concat([expit((a_t[j][t] + b[j]).astype(float)) for t in range(1, self.d_times + 1)],\n                              axis=1, keys=(range(1, self.d_times + 1))) for j in events]\n    return hazards_dfs\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_overall_survival","title":"<code>calculate_overall_survival(hazards, numerical_error_tolerance=0.001)</code>","text":"<p>Calculates the overall survival function given the hazards.</p> <p>Parameters:</p> Name Type Description Default <code>hazards</code> <code>list</code> <p>A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function).</p> required <code>numerical_error_tolerance</code> <code>float</code> <p>Tolerate numerical errors of probabilities up to this value.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>overall_survival</code> <code>Dataframe</code> <p>The overall survival functions.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def calculate_overall_survival(self, hazards: list, numerical_error_tolerance: float = 0.001) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates the overall survival function given the hazards.\n\n    Args:\n        hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function).\n        numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value.\n\n    Returns:\n        overall_survival (pd.Dataframe): The overall survival functions.\n    \"\"\"\n    if (((sum(hazards)) &gt; (1 + numerical_error_tolerance)).sum().sum() &gt; 0):\n        raise ValueError(\"The chosen sampling parameters result in negative values of the overall survival function\")\n    sum_hazards = sum(hazards).clip(0, 1)\n    overall_survival = pd.concat([pd.Series(1, index=hazards[0].index),\n                                  (1 - sum_hazards).cumprod(axis=1).iloc[:, :-1]], axis=1)\n    overall_survival.columns += 1\n    return overall_survival\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_prob_event_at_t","title":"<code>calculate_prob_event_at_t(hazards, overall_survival, numerical_error_tolerance=0.001)</code>","text":"<p>Calculates the probability for event j at time t.</p> <p>Parameters:</p> Name Type Description Default <code>hazards</code> <code>list</code> <p>A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function)</p> required <code>overall_survival</code> <code>Dataframe</code> <p>The overall survival functions</p> required <code>numerical_error_tolerance</code> <code>float</code> <p>Tolerate numerical errors of probabilities up to this value.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>prob_event_at_t</code> <code>list</code> <p>A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def calculate_prob_event_at_t(self, hazards: list, overall_survival: pd.DataFrame,\n                              numerical_error_tolerance: float = 0.001) -&gt; list:\n    \"\"\"\n    Calculates the probability for event j at time t.\n\n    Args:\n        hazards (list): A list of hazards dataframes for each event type (as returned from EventTimesSampler.calculate_hazards function)\n        overall_survival (pd.Dataframe): The overall survival functions\n        numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value.\n\n    Returns:\n        prob_event_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.\n    \"\"\"\n    prob_event_at_t = [hazard * overall_survival for hazard in hazards]\n    prob_event_at_t = self._validate_prob_dfs_list(prob_event_at_t, numerical_error_tolerance)\n    return prob_event_at_t\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.calculate_prob_event_j","title":"<code>calculate_prob_event_j(prob_j_at_t, numerical_error_tolerance=0.001)</code>","text":"<p>Calculates the total probability for event j.</p> <p>Parameters:</p> Name Type Description Default <code>prob_j_at_t</code> <code>list</code> <p>A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.</p> required <code>numerical_error_tolerance</code> <code>float</code> <p>Tolerate numerical errors of probabilities up to this value.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>total_prob_j</code> <code>list</code> <p>A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def calculate_prob_event_j(self, prob_j_at_t: list, numerical_error_tolerance: float = 0.001) -&gt; list:\n    \"\"\"\n    Calculates the total probability for event j.\n\n    Args:\n        prob_j_at_t (list): A list of dataframes, one for each event type, with the probability of event occurrance at time t to each of the observations.\n        numerical_error_tolerance (float): Tolerate numerical errors of probabilities up to this value.\n\n    Returns:\n        total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.\n    \"\"\"\n    total_prob_j = [prob.sum(axis=1) for prob in prob_j_at_t]\n    total_prob_j = self._validate_prob_dfs_list(total_prob_j, numerical_error_tolerance)\n    return total_prob_j\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_event_times","title":"<code>sample_event_times(observations_df, hazard_coefs, covariates=None, events=None, seed=None)</code>","text":"<p>Sample event type and event occurance times.</p> <p>Parameters:</p> Name Type Description Default <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with observations covariates.</p> required <code>covariates</code> <code>list</code> <p>list of covariates name, must be a subset of observations_df.columns</p> <code>None</code> <code>coefficients</code> <code>dict</code> <p>time coefficients and covariates coefficients for each event type.</p> required <code>seed</code> <code>(int, None)</code> <p>numpy seed number for pseudo random sampling.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with additional columns for sampled event time (T) and event type (J).</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def sample_event_times(self, observations_df: pd.DataFrame,\n                             hazard_coefs: dict,\n                             covariates: Union[list, None] = None,\n                             events: Union[list, None] = None,\n                             seed: Union[int, None] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Sample event type and event occurance times.\n\n    Args:\n        observations_df (pd.DataFrame): Dataframe with observations covariates.\n        covariates (list): list of covariates name, must be a subset of observations_df.columns\n        coefficients (dict): time coefficients and covariates coefficients for each event type.\n        seed (int, None): numpy seed number for pseudo random sampling.\n\n    Returns:\n        observations_df (pd.DataFrame): Dataframe with additional columns for sampled event time (T) and event type (J).\n    \"\"\"\n    np.random.seed(seed)\n    if covariates is None:\n        covariates = [c for c in observations_df.columns if c not in ['X', 'T', 'C', 'J']]\n    events = events if events is not None else self.events\n    cov_df = observations_df[covariates]\n    hazards = self.calculate_hazards(cov_df, hazard_coefs, events=events)\n    overall_survival = self.calculate_overall_survival(hazards)\n    probs_j_at_t = self.calculate_prob_event_at_t(hazards, overall_survival)\n    total_prob_j = self.calculate_prob_event_j(probs_j_at_t)\n    probs_t_given_j = self.calc_prob_t_given_j(probs_j_at_t, total_prob_j)\n    sampled_jt = self.sample_jt(total_prob_j, probs_t_given_j)\n    if 'J' in observations_df.columns:\n        observations_df.drop('J', axis=1, inplace=True)\n    if 'T' in observations_df.columns:\n        observations_df.drop('T', axis=1, inplace=True)\n    observations_df = pd.concat([observations_df, sampled_jt], axis=1)\n    return observations_df\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_hazard_lof_censoring","title":"<code>sample_hazard_lof_censoring(observations_df, censoring_hazard_coefs, seed=None, covariates=None)</code>","text":"<p>Samples loss of follow-up censoring time from hazard coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with observations covariates.</p> required <code>censoring_hazard_coefs</code> <code>dict</code> <p>time coefficients and covariates coefficients for the censoring hazard.</p> required <code>seed</code> <code>int</code> <p>pseudo random seed number for numpy.random.seed()</p> <code>None</code> <code>covariates</code> <code>list</code> <p>list of covariates names, must be a subset of observations_df.columns</p> <code>None</code> <p>Returns:</p> Name Type Description <code>observations_df</code> <code>DataFrame</code> <p>Upadted dataframe including sampled censoring time.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def sample_hazard_lof_censoring(self, observations_df: pd.DataFrame, censoring_hazard_coefs: dict,\n                                seed: Union[int, None] = None,\n                                covariates: Union[list, None] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Samples loss of follow-up censoring time from hazard coefficients.\n\n    Args:\n        observations_df (pd.DataFrame): Dataframe with observations covariates.\n        censoring_hazard_coefs (dict): time coefficients and covariates coefficients for the censoring hazard.\n        seed (int): pseudo random seed number for numpy.random.seed()\n        covariates (list): list of covariates names, must be a subset of observations_df.columns\n\n    Returns:\n        observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time.\n    \"\"\"\n    if covariates is None:\n        covariates = [c for c in observations_df.columns if c not in ['X', 'T', 'C', 'J']]\n    cov_df = observations_df[covariates]\n    tmp_ets = EventTimesSampler(d_times=self.d_times, j_event_types=1)\n    sampled_df = tmp_ets.sample_event_times(cov_df, censoring_hazard_coefs, seed=seed, covariates=covariates,\n                                            events=[0])\n\n    # No follow-up censoring, C=d+2 such that T wins when building X column:\n    #sampled_df.loc[sampled_df['J'] == 0, 'T'] = self.d_times + 2\n    sampled_df = sampled_df[['T']]\n    sampled_df.columns = ['C']\n    if 'C' in observations_df.columns:\n        observations_df.drop('C', axis=1, inplace=True)\n    observations_df = pd.concat([observations_df, sampled_df], axis=1)\n    return observations_df\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_independent_lof_censoring","title":"<code>sample_independent_lof_censoring(observations_df, prob_lof_at_t, seed=None)</code>","text":"<p>Samples loss of follow-up censoring time from probabilities independent of covariates.</p> <p>Parameters:</p> Name Type Description Default <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with observations covariates.</p> required <code>prob_lof_at_t</code> <code>array</code> <p>Array of probabilities for sampling each of the possible times.</p> required <code>seed</code> <code>int</code> <p>pseudo random seed number for numpy.random.seed()</p> <code>None</code> <p>Returns:</p> Name Type Description <code>observations_df</code> <code>DataFrame</code> <p>Upadted dataframe including sampled censoring time.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def sample_independent_lof_censoring(self, observations_df: pd.DataFrame,\n                                     prob_lof_at_t: np.array, seed: Union[int, None] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Samples loss of follow-up censoring time from probabilities independent of covariates.\n\n    Args:\n        observations_df (pd.DataFrame): Dataframe with observations covariates.\n        prob_lof_at_t (np.array): Array of probabilities for sampling each of the possible times.\n        seed (int): pseudo random seed number for numpy.random.seed()\n\n    Returns:\n        observations_df (pd.DataFrame): Upadted dataframe including sampled censoring time.\n    \"\"\"\n    np.random.seed(seed)\n    administrative_censoring_prob = (1 - sum(prob_lof_at_t))\n    assert (administrative_censoring_prob &gt;= 0), \"Check the sum of prob_lof_at_t argument.\"\n    assert (administrative_censoring_prob &lt;= 1), \"Check the sum of prob_lof_at_t argument.\"\n\n    prob_lof_at_t = np.append(prob_lof_at_t, administrative_censoring_prob)\n    sampled_df = pd.DataFrame(np.random.choice(a=self.times, size=len(observations_df), p=prob_lof_at_t),\n                              index=observations_df.index, columns=['C'])\n    # No follow-up censoring, C=d+2 such that T wins when building X column:\n    #sampled_df.loc[sampled_df['C'] == self.times[-1], 'C'] = self.d_times + 2\n    if 'C' in observations_df.columns:\n        observations_df.drop('C', axis=1, inplace=True)\n    observations_df = pd.concat([observations_df, sampled_df], axis=1)\n    return observations_df\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.sample_jt","title":"<code>sample_jt(total_prob_j, probs_t_given_j, numerical_error_tolerance=0.001)</code>","text":"<p>Sample event type and event time for each observation.</p> <p>Parameters:</p> Name Type Description Default <code>total_prob_j</code> <code>list</code> <p>A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.</p> required <code>probs_t_given_j</code> <code>list</code> <p>A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations.</p> required <p>Returns:</p> Name Type Description <code>sampled_df</code> <code>DataFrame</code> <p>A dataframe with sampled event time and event type for each observation.</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def sample_jt(self, total_prob_j: list, probs_t_given_j: list, numerical_error_tolerance: float = 0.001) -&gt; pd.DataFrame:\n    \"\"\"\n    Sample event type and event time for each observation.\n\n    Args:\n        total_prob_j (list): A list of dataframes, one for each event type, with the total probability of event occurrance to each of the observations.\n        probs_t_given_j (list): A list of dataframes, one for each event type, with the conditional probability of event occurrance at t given event type j to each of the observations.\n\n    Returns:\n        sampled_df (pd.DataFrame): A dataframe with sampled event time and event type for each observation.\n    \"\"\"\n\n    total_prob_j = self._validate_prob_dfs_list(total_prob_j, numerical_error_tolerance)\n    probs_t_given_j = self._validate_prob_dfs_list(probs_t_given_j, numerical_error_tolerance)\n\n    # Add administrative censoring (no event occured until Tmax) probability as J=0\n    temp_sums = pd.concat([1 - sum(total_prob_j), *total_prob_j], axis=1, keys=[0, *self.events])\n    if (((temp_sums &lt; (0 - numerical_error_tolerance)).any().any()) or \\\n            ((temp_sums &gt; (1 + numerical_error_tolerance)).any().any())):\n        raise ValueError(\"The chosen sampling parameters result in invalid probabilities\")\n    # Only fixes numerical errors smaller than the tolerance size\n    temp_sums.clip(0, 1, inplace=True)\n\n    # Efficient way to sample j for each observation with different event probabilities\n    sampled_df = (temp_sums.cumsum(1) &gt; np.random.rand(temp_sums.shape[0])[:, None]).idxmax(axis=1).to_frame('J')\n\n    temp_ts = []\n    for j in self.events:\n        # Get the index of the observations with J_i = j\n        rel_j = sampled_df.query(\"J==@j\").index\n\n        # Get probs dataframe from the dfs list\n        prob_df = probs_t_given_j[j - 1]  # the prob j to sample from\n\n        # Sample time of occurrence given J_i = j\n        temp_ts.append((prob_df.loc[rel_j].cumsum(1) &gt;= np.random.rand(rel_j.shape[0])[:, None]).idxmax(axis=1))\n\n    # Add Tmax+1 for observations with J_i = 0\n    temp_ts.append(pd.Series(self.d_times + 1, index=sampled_df.query('J==0').index))\n    sampled_df[\"T\"] = pd.concat(temp_ts).sort_index()\n    return sampled_df\n</code></pre>"},{"location":"api/event_times_sampler/#pydts.data_generation.EventTimesSampler.update_event_or_lof","title":"<code>update_event_or_lof(observations_df)</code>","text":"<p>Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'. Event type 'J' will be changed to 0 for observation with 'C' &lt; 'T'.</p> <p>Parameters:</p> Name Type Description Default <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with observations after sampling event times 'T' and censoring time 'C'.</p> required <p>Returns:</p> Name Type Description <code>observations_df</code> <code>DataFrame</code> <p>Dataframe with updated time column 'X' and event type column 'J'</p> Source code in <code>src/pydts/data_generation.py</code> <pre><code>def update_event_or_lof(self, observations_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Updates time column 'X' to be the minimum between event time column 'T' and censoring time column 'C'.\n    Event type 'J' will be changed to 0 for observation with 'C' &lt; 'T'.\n\n    Args:\n        observations_df (pd.DataFrame): Dataframe with observations after sampling event times 'T' and censoring time 'C'.\n\n    Returns:\n        observations_df (pd.DataFrame): Dataframe with updated time column 'X' and event type column 'J'\n    \"\"\"\n    assert ('T' in observations_df.columns), \"Trying to update event or censoring before sampling event times\"\n    assert ('C' in observations_df.columns), \"Trying to update event or censoring before sampling censoring time\"\n    observations_df['X'] = observations_df[['T', 'C']].min(axis=1)\n    observations_df.loc[observations_df.loc[(observations_df['C'] &lt; observations_df['T'])].index, 'J'] = 0\n    return observations_df\n</code></pre>"},{"location":"api/model_selection/","title":"Model Selection","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch","title":"<code>pydts.model_selection.PenaltyGridSearch()</code>","text":"<p>               Bases: <code>BasePenaltyGridSearch</code></p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'CoxPHFitter'\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'CoxPHFitter'</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.l1_ratio","title":"<code>l1_ratio = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.meta_models","title":"<code>meta_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.penalizers","title":"<code>penalizers = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.seed","title":"<code>seed = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.test_df","title":"<code>test_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.train_df","title":"<code>train_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch._get_model_fit_kwargs","title":"<code>_get_model_fit_kwargs(penalizer, l1_ratio)</code>","text":"Source code in <code>src/pydts/model_selection.py</code> <pre><code>def _get_model_fit_kwargs(self, penalizer, l1_ratio):\n    if self.TwoStagesFitter_type == 'Exact':\n        fit_beta_kwargs = {\n            'model_fit_kwargs': {\n                'alpha': penalizer,\n                'L1_wt': l1_ratio\n            }\n        }\n    else:\n        fit_beta_kwargs = {\n            'model_kwargs': {\n                'penalizer': penalizer,\n                'l1_ratio': l1_ratio\n            },\n        }\n    return fit_beta_kwargs\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.convert_results_dict_to_df","title":"<code>convert_results_dict_to_df(results_dict)</code>","text":"<p>This function converts a results dictionary to a pd.DataFrame format.</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <p>one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Results in a pd.DataFrame format.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def convert_results_dict_to_df(self, results_dict):\n    \"\"\"\n    This function converts a results dictionary to a pd.DataFrame format.\n\n    Args:\n        results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs.\n\n    Returns:\n        df (pd.DataFrame): Results in a pd.DataFrame format.\n    \"\"\"\n    df = pd.DataFrame(results_dict.values(), index=pd.MultiIndex.from_tuples(results_dict.keys()))\n    return df\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.evaluate","title":"<code>evaluate(train_df, test_df, l1_ratio, penalizers, metrics=['IBS', 'GBS', 'IAUC', 'GAUC'], seed=None, event_type_col='J', duration_col='X', pid_col='pid', twostages_fit_kwargs={})</code>","text":"<p>This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers.</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>training data for fitting the model.</p> required <code>test_df</code> <code>DataFrame</code> <p>testing data for evaluating the estimated model's performance.</p> required <code>l1_ratio</code> <code>float</code> <p>regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>penalizers</code> <code>list</code> <p>penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['IBS', 'GBS', 'IAUC', 'GAUC']</code> <code>seed</code> <code>int</code> <p>pseudo random seed number for numpy.random.seed()</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>twostages_fit_kwargs</code> <code>dict</code> <p>keyword arguments to pass to the TwoStagesFitter.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tuple</code> <p>Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def evaluate(self,\n             train_df: pd.DataFrame,\n             test_df: pd.DataFrame,\n             l1_ratio: float,\n             penalizers: list,\n             metrics: Union[list, str] = ['IBS', 'GBS', 'IAUC', 'GAUC'],\n             seed: Union[None, int] = None,\n             event_type_col: str = 'J',\n             duration_col: str = 'X',\n             pid_col: str = 'pid',\n             twostages_fit_kwargs: dict = {}) -&gt; tuple:\n\n    \"\"\"\n    This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers.\n\n    Args:\n        train_df (pd.DataFrame): training data for fitting the model.\n        test_df (pd.DataFrame): testing data for evaluating the estimated model's performance.\n        l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        metrics (str, list): Evaluation metrics.\n        seed (int): pseudo random seed number for numpy.random.seed()\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter.\n\n    Returns:\n        output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics.\n\n    \"\"\"\n\n    self.l1_ratio = l1_ratio\n    self.penalizers = penalizers\n    self.seed = seed\n    np.random.seed(seed)\n\n    for idp, penalizer in enumerate(penalizers):\n\n        fit_beta_kwargs = self._get_model_fit_kwargs(penalizer, l1_ratio)\n\n        if self.TwoStagesFitter_type == 'Exact':\n            self.meta_models[penalizer] = TwoStagesFitterExact()\n        else:\n            self.meta_models[penalizer] = TwoStagesFitter()\n        print(f\"Started estimating the coefficients for penalizer {penalizer} ({idp+1}/{len(penalizers)})\")\n        start = time()\n        self.meta_models[penalizer].fit(df=train_df, fit_beta_kwargs=fit_beta_kwargs,\n                                        pid_col=pid_col, event_type_col=event_type_col, duration_col=duration_col,\n                                        **twostages_fit_kwargs)\n        end = time()\n        print(f\"Finished estimating the coefficients for penalizer {penalizer} ({idp+1}/{len(penalizers)}), {int(end - start)} seconds\")\n\n    events = [j for j in sorted(train_df[event_type_col].unique()) if j != 0]\n    grid = [penalizers for e in events]\n    penalizers_combinations = list(product(*grid))\n\n    for idc, combination in enumerate(penalizers_combinations):\n        mixed_two_stages = self.get_mixed_two_stages_fitter(combination)\n\n        pred_df = mixed_two_stages.predict_prob_events(test_df)\n\n        for metric in metrics:\n            if metric == 'IAUC':\n                self.integrated_auc[combination] = events_integrated_auc(pred_df, event_type_col=event_type_col,\n                                                                                  duration_col=duration_col)\n            elif metric == 'GAUC':\n                self.global_auc[combination] = global_auc(pred_df, event_type_col=event_type_col,\n                                                                   duration_col=duration_col)\n            elif metric == 'IBS':\n                self.integrated_bs[combination] = events_integrated_brier_score(pred_df,\n                                                                                event_type_col=event_type_col,\n                                                                                duration_col=duration_col)\n            elif metric == 'GBS':\n                self.global_bs[combination] = global_brier_score(pred_df, event_type_col=event_type_col,\n                                                                          duration_col=duration_col)\n\n    output = self.convert_results_dict_to_df(self.global_auc).idxmax().values[0] if 'GAUC' in metrics else []\n    return output\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearch.get_mixed_two_stages_fitter","title":"<code>get_mixed_two_stages_fitter(penalizers_combination)</code>","text":"<p>This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination.</p> <p>Parameters:</p> Name Type Description Default <code>penalizers_combination</code> <code>list</code> <p>List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method.</p> required <p>Returns:</p> Name Type Description <code>mixed_two_stages</code> <code>TwoStagesFitter</code> <p>TwoStagesFitter for the required penalty combination.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def get_mixed_two_stages_fitter(self, penalizers_combination: list) -&gt; TwoStagesFitter:\n    \"\"\"\n    This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination.\n\n    Args:\n        penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method.\n\n    Returns:\n        mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination.\n    \"\"\"\n    _validate_estimated_value = [p for p in penalizers_combination if p not in list(self.meta_models.keys())]\n    assert len(_validate_estimated_value) == 0, \\\n           f\"Values {_validate_estimated_value} were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\"\n\n    events = self.meta_models[penalizers_combination[0]].events\n    event_type_col = self.meta_models[penalizers_combination[0]].event_type_col\n    if self.TwoStagesFitter_type == 'Exact':\n        mixed_two_stages = TwoStagesFitterExact()\n    else:\n        mixed_two_stages = TwoStagesFitter()\n\n    for ide, event in enumerate(sorted(events)):\n        if ide == 0:\n            mixed_two_stages.covariates = self.meta_models[penalizers_combination[ide]].covariates\n            mixed_two_stages.duration_col = self.meta_models[penalizers_combination[ide]].duration_col\n            mixed_two_stages.event_type_col = self.meta_models[penalizers_combination[ide]].event_type_col\n            mixed_two_stages.events = self.meta_models[penalizers_combination[ide]].events\n            mixed_two_stages.pid_col = self.meta_models[penalizers_combination[ide]].pid_col\n            mixed_two_stages.times = self.meta_models[penalizers_combination[ide]].times\n\n        mixed_two_stages.beta_models[event] = self.meta_models[penalizers_combination[ide]].beta_models[event]\n        mixed_two_stages.event_models[event] = []\n        mixed_two_stages.event_models[event].append(self.meta_models[penalizers_combination[ide]].beta_models[event])\n\n        event_alpha = self.meta_models[penalizers_combination[ide]].alpha_df.copy()\n        event_alpha = event_alpha[event_alpha[event_type_col] == event]\n        mixed_two_stages.alpha_df = pd.concat([mixed_two_stages.alpha_df, event_alpha])\n        mixed_two_stages.event_models[event].append(event_alpha)\n\n    return mixed_two_stages\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact","title":"<code>pydts.model_selection.PenaltyGridSearchExact()</code>","text":"<p>               Bases: <code>BasePenaltyGridSearch</code></p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'Exact'\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'Exact'</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.global_auc","title":"<code>global_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.global_bs","title":"<code>global_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.integrated_auc","title":"<code>integrated_auc = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.integrated_bs","title":"<code>integrated_bs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.l1_ratio","title":"<code>l1_ratio = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.meta_models","title":"<code>meta_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.penalizers","title":"<code>penalizers = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.seed","title":"<code>seed = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.test_df","title":"<code>test_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.train_df","title":"<code>train_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact._get_model_fit_kwargs","title":"<code>_get_model_fit_kwargs(penalizer, l1_ratio)</code>","text":"Source code in <code>src/pydts/model_selection.py</code> <pre><code>def _get_model_fit_kwargs(self, penalizer, l1_ratio):\n    if self.TwoStagesFitter_type == 'Exact':\n        fit_beta_kwargs = {\n            'model_fit_kwargs': {\n                'alpha': penalizer,\n                'L1_wt': l1_ratio\n            }\n        }\n    else:\n        fit_beta_kwargs = {\n            'model_kwargs': {\n                'penalizer': penalizer,\n                'l1_ratio': l1_ratio\n            },\n        }\n    return fit_beta_kwargs\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.convert_results_dict_to_df","title":"<code>convert_results_dict_to_df(results_dict)</code>","text":"<p>This function converts a results dictionary to a pd.DataFrame format.</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <p>one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Results in a pd.DataFrame format.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def convert_results_dict_to_df(self, results_dict):\n    \"\"\"\n    This function converts a results dictionary to a pd.DataFrame format.\n\n    Args:\n        results_dict: one of the class attributes: global_auc, integrated_auc, global_bs, integrated_bs.\n\n    Returns:\n        df (pd.DataFrame): Results in a pd.DataFrame format.\n    \"\"\"\n    df = pd.DataFrame(results_dict.values(), index=pd.MultiIndex.from_tuples(results_dict.keys()))\n    return df\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.evaluate","title":"<code>evaluate(train_df, test_df, l1_ratio, penalizers, metrics=['IBS', 'GBS', 'IAUC', 'GAUC'], seed=None, event_type_col='J', duration_col='X', pid_col='pid', twostages_fit_kwargs={})</code>","text":"<p>This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers.</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>training data for fitting the model.</p> required <code>test_df</code> <code>DataFrame</code> <p>testing data for evaluating the estimated model's performance.</p> required <code>l1_ratio</code> <code>float</code> <p>regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>penalizers</code> <code>list</code> <p>penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).</p> required <code>metrics</code> <code>(str, list)</code> <p>Evaluation metrics.</p> <code>['IBS', 'GBS', 'IAUC', 'GAUC']</code> <code>seed</code> <code>int</code> <p>pseudo random seed number for numpy.random.seed()</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>twostages_fit_kwargs</code> <code>dict</code> <p>keyword arguments to pass to the TwoStagesFitter.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tuple</code> <p>Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def evaluate(self,\n             train_df: pd.DataFrame,\n             test_df: pd.DataFrame,\n             l1_ratio: float,\n             penalizers: list,\n             metrics: Union[list, str] = ['IBS', 'GBS', 'IAUC', 'GAUC'],\n             seed: Union[None, int] = None,\n             event_type_col: str = 'J',\n             duration_col: str = 'X',\n             pid_col: str = 'pid',\n             twostages_fit_kwargs: dict = {}) -&gt; tuple:\n\n    \"\"\"\n    This function implements model estimation using train_df and evaluation of the metrics using test_df to all the possible combinations of penalizers.\n\n    Args:\n        train_df (pd.DataFrame): training data for fitting the model.\n        test_df (pd.DataFrame): testing data for evaluating the estimated model's performance.\n        l1_ratio (float): regularization ratio for the CoxPHFitter (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        penalizers (list): penalizer options for each event (see lifelines.fitters.coxph_fitter.CoxPHFitter documentation).\n        metrics (str, list): Evaluation metrics.\n        seed (int): pseudo random seed number for numpy.random.seed()\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        twostages_fit_kwargs (dict): keyword arguments to pass to the TwoStagesFitter.\n\n    Returns:\n        output (Tuple): Penalizers with best performance in terms of Global-AUC, if 'GAUC' is in metrics.\n\n    \"\"\"\n\n    self.l1_ratio = l1_ratio\n    self.penalizers = penalizers\n    self.seed = seed\n    np.random.seed(seed)\n\n    for idp, penalizer in enumerate(penalizers):\n\n        fit_beta_kwargs = self._get_model_fit_kwargs(penalizer, l1_ratio)\n\n        if self.TwoStagesFitter_type == 'Exact':\n            self.meta_models[penalizer] = TwoStagesFitterExact()\n        else:\n            self.meta_models[penalizer] = TwoStagesFitter()\n        print(f\"Started estimating the coefficients for penalizer {penalizer} ({idp+1}/{len(penalizers)})\")\n        start = time()\n        self.meta_models[penalizer].fit(df=train_df, fit_beta_kwargs=fit_beta_kwargs,\n                                        pid_col=pid_col, event_type_col=event_type_col, duration_col=duration_col,\n                                        **twostages_fit_kwargs)\n        end = time()\n        print(f\"Finished estimating the coefficients for penalizer {penalizer} ({idp+1}/{len(penalizers)}), {int(end - start)} seconds\")\n\n    events = [j for j in sorted(train_df[event_type_col].unique()) if j != 0]\n    grid = [penalizers for e in events]\n    penalizers_combinations = list(product(*grid))\n\n    for idc, combination in enumerate(penalizers_combinations):\n        mixed_two_stages = self.get_mixed_two_stages_fitter(combination)\n\n        pred_df = mixed_two_stages.predict_prob_events(test_df)\n\n        for metric in metrics:\n            if metric == 'IAUC':\n                self.integrated_auc[combination] = events_integrated_auc(pred_df, event_type_col=event_type_col,\n                                                                                  duration_col=duration_col)\n            elif metric == 'GAUC':\n                self.global_auc[combination] = global_auc(pred_df, event_type_col=event_type_col,\n                                                                   duration_col=duration_col)\n            elif metric == 'IBS':\n                self.integrated_bs[combination] = events_integrated_brier_score(pred_df,\n                                                                                event_type_col=event_type_col,\n                                                                                duration_col=duration_col)\n            elif metric == 'GBS':\n                self.global_bs[combination] = global_brier_score(pred_df, event_type_col=event_type_col,\n                                                                          duration_col=duration_col)\n\n    output = self.convert_results_dict_to_df(self.global_auc).idxmax().values[0] if 'GAUC' in metrics else []\n    return output\n</code></pre>"},{"location":"api/model_selection/#pydts.model_selection.PenaltyGridSearchExact.get_mixed_two_stages_fitter","title":"<code>get_mixed_two_stages_fitter(penalizers_combination)</code>","text":"<p>This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination.</p> <p>Parameters:</p> Name Type Description Default <code>penalizers_combination</code> <code>list</code> <p>List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method.</p> required <p>Returns:</p> Name Type Description <code>mixed_two_stages</code> <code>TwoStagesFitter</code> <p>TwoStagesFitter for the required penalty combination.</p> Source code in <code>src/pydts/model_selection.py</code> <pre><code>def get_mixed_two_stages_fitter(self, penalizers_combination: list) -&gt; TwoStagesFitter:\n    \"\"\"\n    This function creates a mixed TwoStagesFitter from the estimated meta models for a specific penalizers combination.\n\n    Args:\n        penalizers_combination (list): List with length equals to the number of competing events. The penalizers value to each of the events. Each of the values must be one of the values that was previously passed to the evaluate() method.\n\n    Returns:\n        mixed_two_stages (pydts.fitters.TwoStagesFitter): TwoStagesFitter for the required penalty combination.\n    \"\"\"\n    _validate_estimated_value = [p for p in penalizers_combination if p not in list(self.meta_models.keys())]\n    assert len(_validate_estimated_value) == 0, \\\n           f\"Values {_validate_estimated_value} were note estimated. All the penalizers in penalizers_combination must be estimated using evaluate() before a mixed combination can be generated.\"\n\n    events = self.meta_models[penalizers_combination[0]].events\n    event_type_col = self.meta_models[penalizers_combination[0]].event_type_col\n    if self.TwoStagesFitter_type == 'Exact':\n        mixed_two_stages = TwoStagesFitterExact()\n    else:\n        mixed_two_stages = TwoStagesFitter()\n\n    for ide, event in enumerate(sorted(events)):\n        if ide == 0:\n            mixed_two_stages.covariates = self.meta_models[penalizers_combination[ide]].covariates\n            mixed_two_stages.duration_col = self.meta_models[penalizers_combination[ide]].duration_col\n            mixed_two_stages.event_type_col = self.meta_models[penalizers_combination[ide]].event_type_col\n            mixed_two_stages.events = self.meta_models[penalizers_combination[ide]].events\n            mixed_two_stages.pid_col = self.meta_models[penalizers_combination[ide]].pid_col\n            mixed_two_stages.times = self.meta_models[penalizers_combination[ide]].times\n\n        mixed_two_stages.beta_models[event] = self.meta_models[penalizers_combination[ide]].beta_models[event]\n        mixed_two_stages.event_models[event] = []\n        mixed_two_stages.event_models[event].append(self.meta_models[penalizers_combination[ide]].beta_models[event])\n\n        event_alpha = self.meta_models[penalizers_combination[ide]].alpha_df.copy()\n        event_alpha = event_alpha[event_alpha[event_type_col] == event]\n        mixed_two_stages.alpha_df = pd.concat([mixed_two_stages.alpha_df, event_alpha])\n        mixed_two_stages.event_models[event].append(event_alpha)\n\n    return mixed_two_stages\n</code></pre>"},{"location":"api/screening/","title":"Sure Independent Screening","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter","title":"<code>pydts.screening.SISTwoStagesFitter()</code>","text":"<p>               Bases: <code>BaseSISTwoStages</code></p> Source code in <code>src/pydts/screening.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'CoxPHFitter'\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'CoxPHFitter'</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.chosen_covariates","title":"<code>chosen_covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.chosen_covariates_j","title":"<code>chosen_covariates_j = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.covariates","title":"<code>covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.df","title":"<code>df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.duration_col","title":"<code>duration_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.event_type_col","title":"<code>event_type_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.events","title":"<code>events = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.expanded_df","title":"<code>expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.final_model","title":"<code>final_model = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.marginal_estimates_df","title":"<code>marginal_estimates_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.null_model_df","title":"<code>null_model_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.permuted_df","title":"<code>permuted_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.permuted_expanded_df","title":"<code>permuted_expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.pid_col","title":"<code>pid_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.threshold","title":"<code>threshold = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.times","title":"<code>times = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter._get_params_cols_from_res_df","title":"<code>_get_params_cols_from_res_df(res_df)</code>","text":"Source code in <code>src/pydts/screening.py</code> <pre><code>def _get_params_cols_from_res_df(self, res_df):\n    if self.TwoStagesFitter_type == 'Exact':\n        _params_cols = [c for c in res_df.columns if '   coef   ' in c]\n    else:\n        _params_cols = [c for c in res_df.columns if 'params' in c]\n    return _params_cols\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.fit","title":"<code>fit(df, threshold=None, quantile=1, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS, seed=None, fit_final_model=True)</code>","text":"<p>This method performs the principled sure independence screening (PSIS) process of Zhao et al. (2012) for discrete-time data with data-driven threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>threshold</code> <code>float</code> <p>a user defined threshold. Defaults to None, i.e. data-driven threshold</p> <code>None</code> <code>quantile</code> <code>float</code> <p>the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Only in use when threshold = None. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.</p> <code>1</code> <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>seed</code> <code>int</code> <p>pseudo random state.</p> <code>None</code> <code>fit_final_model</code> <code>boolean</code> <p>True if to fit and return the TwoStagesFitter with the selected covariates.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>final_model</code> <code>TwoStagesFitter</code> <p>estimated model with the chosen covariates after PSIS.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def fit(self,\n        df: pd.DataFrame,\n        threshold: float = None,\n        quantile: float = 1,\n        covariates: List = None,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid',\n        x0: Union[np.array, int] = 0,\n        fit_beta_kwargs: dict = {},\n        verbose: int = 2,\n        nb_workers: int = WORKERS,\n        seed: int = None,\n        fit_final_model: bool = True):\n\n    \"\"\"\n    This method performs the principled sure independence screening (PSIS) process of Zhao et al. (2012) for discrete-time data with data-driven threshold.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        threshold (float): a user defined threshold. Defaults to None, i.e. data-driven threshold\n        quantile (float): the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Only in use when threshold = None. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        seed (int): pseudo random state.\n        fit_final_model (boolean): True if to fit and return the TwoStagesFitter with the selected covariates.\n\n    Returns:\n        final_model (TwoStagesFitter): estimated model with the chosen covariates after PSIS.\n    \"\"\"\n\n    self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n    self.covariates = covariates\n    self.event_type_col = event_type_col\n    self.duration_col = duration_col\n    self.pid_col = pid_col\n    self.times = sorted(df[duration_col].unique())\n\n    if threshold is not None:\n        self.threshold = threshold\n    else:\n        self.threshold = self.get_data_driven_threshold(df=df,\n                                                        covariates=covariates,\n                                                        quantile=quantile,\n                                                        event_type_col=event_type_col,\n                                                        duration_col=duration_col,\n                                                        pid_col=pid_col,\n                                                        x0=x0,\n                                                        fit_beta_kwargs=fit_beta_kwargs,\n                                                        verbose=verbose,\n                                                        nb_workers=nb_workers,\n                                                        seed=seed)\n    self.df = df\n    self.expanded_df = get_expanded_df(df=df,\n                                       event_type_col=event_type_col,\n                                       duration_col=duration_col,\n                                       pid_col=pid_col)\n\n    self.marginal_estimates_df = self.get_marginal_estimates(expanded_df=self.expanded_df,\n                                                             covariates=covariates,\n                                                             event_type_col=event_type_col,\n                                                             duration_col=duration_col,\n                                                             pid_col=pid_col,\n                                                             verbose=verbose,\n                                                             x0=x0,\n                                                             fit_beta_kwargs=fit_beta_kwargs,\n                                                             nb_workers=nb_workers)\n\n    chosen_covariates = []\n    chosen_covariates_j = {}\n\n    _params_cols = self._get_params_cols_from_res_df(self.marginal_estimates_df)\n\n    for c in _params_cols:\n        if self.TwoStagesFitter_type == 'Exact':\n            event = int(c[0])\n        else:\n            event = int(c[1:].split('_')[0])\n        chosen_covariates_j[event] = self.marginal_estimates_df[self.marginal_estimates_df[c].abs() &gt;= self.threshold].index.tolist()\n        chosen_covariates.extend(chosen_covariates_j[event])\n\n    self.chosen_covariates = sorted(np.unique(chosen_covariates))\n    self.chosen_covariates_j = chosen_covariates_j\n\n    if fit_final_model:\n        if self.TwoStagesFitter_type == 'Exact':\n            self.final_model = TwoStagesFitterExact()\n        else:\n            self.final_model = TwoStagesFitter()\n        self.final_model.fit(df=df,\n                             covariates=self.chosen_covariates_j,\n                             event_type_col=event_type_col,\n                             duration_col=duration_col,\n                             pid_col=pid_col,\n                             x0=x0,\n                             fit_beta_kwargs=fit_beta_kwargs,\n                             verbose=verbose,\n                             nb_workers=nb_workers)\n\n    return self.final_model\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.fit_marginal_model","title":"<code>fit_marginal_model(expanded_df, covariate, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=1)</code>","text":"<p>This method fits a marginal model to data using a single covariate. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).</p> <p>Parameters:</p> Name Type Description Default <code>expanded_df</code> <code>DataFrame</code> <p>expanded training data for fitting the model</p> required <code>covariate</code> <code>str</code> <p>a single covariate to be used in estimating the regression coefficients</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>Estimated parameter and standard errors. TwoStagesFitter.get_beta_SE() output.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def fit_marginal_model(self,\n                       expanded_df,\n                       covariate: str,\n                       event_type_col: str = 'J',\n                       duration_col: str = 'X',\n                       pid_col: str = 'pid',\n                       x0: Union[np.array, int] = 0,\n                       fit_beta_kwargs: dict = {},\n                       verbose: int = 2,\n                       nb_workers: int = 1):\n    \"\"\"\n    This method fits a marginal model to data using a single covariate. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).\n\n    Args:\n        expanded_df (pd.DataFrame): expanded training data for fitting the model\n        covariate (str): a single covariate to be used in estimating the regression coefficients\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        result (pd.DataFrame): Estimated parameter and standard errors. TwoStagesFitter.get_beta_SE() output.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(expanded_df[event_type_col].unique()) if c != 0]\n\n    if self.TwoStagesFitter_type == 'Exact':\n        marginal_model = MarginalTwoStagesFitterExact()\n    else:\n        marginal_model = MarginalTwoStagesFitter()\n\n    marginal_model.fit(\n        expanded_df=expanded_df[[pid_col, covariate, event_type_col, duration_col, 'j_0'] +\n                                [f'j_{e}' for e in self.events]],\n        covariates=[covariate],\n        event_type_col=event_type_col,\n        duration_col=duration_col,\n        pid_col=pid_col,\n        x0=x0,\n        fit_beta_kwargs=fit_beta_kwargs,\n        verbose=verbose,\n        nb_workers=nb_workers)\n\n    result = marginal_model.get_beta_SE()\n    del marginal_model\n    return result\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.get_data_driven_threshold","title":"<code>get_data_driven_threshold(df, covariates=None, quantile=1, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS, seed=None)</code>","text":"<p>This method calculates a data-driven threshold for each risk. It fits marginal models to the permuted data and returns the required quantile of the absolute values of the coefficients estimated from the null model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>quantile</code> <code>float</code> <p>represents the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.</p> <code>1</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>seed</code> <code>int</code> <p>pseudo random state.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>threshold</code> <code>Series</code> <p>Estimated thresholds.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def get_data_driven_threshold(self,\n                              df,\n                              covariates: List = None,\n                              quantile: float = 1,\n                              event_type_col: str = 'J',\n                              duration_col: str = 'X',\n                              pid_col: str = 'pid',\n                              x0: Union[np.array, int] = 0,\n                              fit_beta_kwargs: dict = {},\n                              verbose: int = 2,\n                              nb_workers: int = WORKERS,\n                              seed: int = None):\n\n    \"\"\"\n    This method calculates a data-driven threshold for each risk. It fits marginal models to the permuted data and returns the required quantile of the absolute values of the coefficients estimated from the null model.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        quantile (float): represents the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        seed (int): pseudo random state.\n\n    Returns:\n        threshold (pd.Series): Estimated thresholds.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n\n    self.permute_df(df=df, event_type_col=event_type_col,\n                    duration_col=duration_col, pid_col=pid_col, seed=seed)\n    self.null_model_df = self.get_marginal_estimates(expanded_df=self.permuted_expanded_df,\n                                                     covariates=covariates,\n                                                     event_type_col=event_type_col,\n                                                     duration_col=duration_col,\n                                                     pid_col=pid_col,\n                                                     verbose=verbose,\n                                                     x0=x0,\n                                                     fit_beta_kwargs=fit_beta_kwargs,\n                                                     nb_workers=nb_workers)\n\n    _params_cols = self._get_params_cols_from_res_df(self.null_model_df)\n    self.threshold = np.quantile(self.null_model_df[_params_cols].abs().values, q=quantile)\n    return self.threshold\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.get_marginal_estimates","title":"<code>get_marginal_estimates(expanded_df, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', verbose=2, x0=0, fit_beta_kwargs={}, nb_workers=WORKERS)</code>","text":"<p>This method fits a marginal model to data to each of the covariates. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).</p> <p>Parameters:</p> Name Type Description Default <code>expanded_df</code> <code>DataFrame</code> <p>expanded training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>Estimated parameters and standard errors of the marginal models. A concatenation of all the TwoStagesFitter.get_beta_SE() outputs.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def get_marginal_estimates(self,\n                           expanded_df,\n                           covariates: Union[List, dict] = None,\n                           event_type_col: str = 'J',\n                           duration_col: str = 'X',\n                           pid_col: str = 'pid',\n                           verbose: int = 2,\n                           x0: Union[np.array, int] = 0,\n                           fit_beta_kwargs: dict = {},\n                           nb_workers: int = WORKERS):\n\n    \"\"\"\n    This method fits a marginal model to data to each of the covariates. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).\n\n    Args:\n        expanded_df (pd.DataFrame): expanded training data for fitting the model\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        verbose (int, Optional): The verbosity level of pandaallel\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        results_df (pd.DataFrame): Estimated parameters and standard errors of the marginal models. A concatenation of all the TwoStagesFitter.get_beta_SE() outputs.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(expanded_df[event_type_col].unique()) if c != 0]\n\n    if covariates is None:\n        covariates = [col for col in expanded_df if col not in ([event_type_col, duration_col, pid_col, 'j_0'] +\n                      [f'j_{e}' for e in self.events])]\n\n    parallel = Parallel(n_jobs=nb_workers, verbose=verbose)\n    results_df = pd.DataFrame()\n    if isinstance(covariates, list):\n        _results = parallel(delayed(self.fit_marginal_model)(expanded_df, cov,\n                                                             event_type_col, duration_col, pid_col,\n                                                             x0, fit_beta_kwargs, verbose, nb_workers)\n                                                             for cov in covariates)\n        results_df = pd.concat(_results)\n    elif isinstance(covariates, dict):\n        raise ValueError(\"Please provide a list of covariates for the marginal testing, including the union of all possible options across all risks.\")\n    # elif isinstance(covariates, dict):\n    #     for event in self.events:\n    #         _results = parallel(delayed(self.fit_marginal_model)(expanded_df, cov,\n    #                                                              event_type_col, duration_col, pid_col,\n    #                                                              x0, fit_beta_kwargs, verbose, nb_workers)\n    #                                                              for cov in covariates[event])\n    #         event_results_df = pd.concat(_results)\n    #         results_df = pd.concat([results_df, event_results_df], axis=1)\n\n    return results_df.astype(float)\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitter.permute_df","title":"<code>permute_df(df, event_type_col='J', duration_col='X', pid_col='pid', seed=None)</code>","text":"<p>This method applies random permutation on the event-time and event-type columns of the training data such that the covariates are decoupled from the outcome; the permuted data follow the null model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>seed</code> <code>(int, Optional)</code> <p>pseudo random state.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>permuted_df</code> <code>DataFrame</code> <p>null model data.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def permute_df(self,\n               df,\n               event_type_col: str = 'J',\n               duration_col: str = 'X',\n               pid_col: str = 'pid',\n               seed: int = None):\n\n    \"\"\"\n    This method applies random permutation on the event-time and event-type columns of the training data such that the covariates are decoupled from the outcome; the permuted data follow the null model.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        seed (int, Optional): pseudo random state.\n\n    Returns:\n        permuted_df (pd.DataFrame): null model data.\n    \"\"\"\n\n    permuted_df = df.copy()\n    np.random.seed(seed)\n    permuted_index = np.random.permutation(permuted_df.index)\n    permuted_df.loc[:, duration_col] = df.loc[permuted_index, duration_col].values\n    permuted_df.loc[:, event_type_col] = df.loc[permuted_index, event_type_col].values\n    self.permuted_df = permuted_df\n    self.permuted_expanded_df = get_expanded_df(df=self.permuted_df,\n                                                event_type_col=event_type_col,\n                                                duration_col=duration_col,\n                                                pid_col=pid_col)\n    return permuted_df\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact","title":"<code>pydts.screening.SISTwoStagesFitterExact()</code>","text":"<p>               Bases: <code>BaseSISTwoStages</code></p> Source code in <code>src/pydts/screening.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.TwoStagesFitter_type = 'Exact'\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.TwoStagesFitter_type","title":"<code>TwoStagesFitter_type = 'Exact'</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.chosen_covariates","title":"<code>chosen_covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.chosen_covariates_j","title":"<code>chosen_covariates_j = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.covariates","title":"<code>covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.df","title":"<code>df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.duration_col","title":"<code>duration_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.event_type_col","title":"<code>event_type_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.events","title":"<code>events = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.expanded_df","title":"<code>expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.final_model","title":"<code>final_model = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.marginal_estimates_df","title":"<code>marginal_estimates_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.null_model_df","title":"<code>null_model_df = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.permuted_df","title":"<code>permuted_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.permuted_expanded_df","title":"<code>permuted_expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.pid_col","title":"<code>pid_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.threshold","title":"<code>threshold = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.times","title":"<code>times = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact._get_params_cols_from_res_df","title":"<code>_get_params_cols_from_res_df(res_df)</code>","text":"Source code in <code>src/pydts/screening.py</code> <pre><code>def _get_params_cols_from_res_df(self, res_df):\n    if self.TwoStagesFitter_type == 'Exact':\n        _params_cols = [c for c in res_df.columns if '   coef   ' in c]\n    else:\n        _params_cols = [c for c in res_df.columns if 'params' in c]\n    return _params_cols\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.fit","title":"<code>fit(df, threshold=None, quantile=1, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS, seed=None, fit_final_model=True)</code>","text":"<p>This method performs the principled sure independence screening (PSIS) process of Zhao et al. (2012) for discrete-time data with data-driven threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>threshold</code> <code>float</code> <p>a user defined threshold. Defaults to None, i.e. data-driven threshold</p> <code>None</code> <code>quantile</code> <code>float</code> <p>the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Only in use when threshold = None. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.</p> <code>1</code> <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>seed</code> <code>int</code> <p>pseudo random state.</p> <code>None</code> <code>fit_final_model</code> <code>boolean</code> <p>True if to fit and return the TwoStagesFitter with the selected covariates.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>final_model</code> <code>TwoStagesFitter</code> <p>estimated model with the chosen covariates after PSIS.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def fit(self,\n        df: pd.DataFrame,\n        threshold: float = None,\n        quantile: float = 1,\n        covariates: List = None,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid',\n        x0: Union[np.array, int] = 0,\n        fit_beta_kwargs: dict = {},\n        verbose: int = 2,\n        nb_workers: int = WORKERS,\n        seed: int = None,\n        fit_final_model: bool = True):\n\n    \"\"\"\n    This method performs the principled sure independence screening (PSIS) process of Zhao et al. (2012) for discrete-time data with data-driven threshold.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        threshold (float): a user defined threshold. Defaults to None, i.e. data-driven threshold\n        quantile (float): the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Only in use when threshold = None. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        seed (int): pseudo random state.\n        fit_final_model (boolean): True if to fit and return the TwoStagesFitter with the selected covariates.\n\n    Returns:\n        final_model (TwoStagesFitter): estimated model with the chosen covariates after PSIS.\n    \"\"\"\n\n    self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n    self.covariates = covariates\n    self.event_type_col = event_type_col\n    self.duration_col = duration_col\n    self.pid_col = pid_col\n    self.times = sorted(df[duration_col].unique())\n\n    if threshold is not None:\n        self.threshold = threshold\n    else:\n        self.threshold = self.get_data_driven_threshold(df=df,\n                                                        covariates=covariates,\n                                                        quantile=quantile,\n                                                        event_type_col=event_type_col,\n                                                        duration_col=duration_col,\n                                                        pid_col=pid_col,\n                                                        x0=x0,\n                                                        fit_beta_kwargs=fit_beta_kwargs,\n                                                        verbose=verbose,\n                                                        nb_workers=nb_workers,\n                                                        seed=seed)\n    self.df = df\n    self.expanded_df = get_expanded_df(df=df,\n                                       event_type_col=event_type_col,\n                                       duration_col=duration_col,\n                                       pid_col=pid_col)\n\n    self.marginal_estimates_df = self.get_marginal_estimates(expanded_df=self.expanded_df,\n                                                             covariates=covariates,\n                                                             event_type_col=event_type_col,\n                                                             duration_col=duration_col,\n                                                             pid_col=pid_col,\n                                                             verbose=verbose,\n                                                             x0=x0,\n                                                             fit_beta_kwargs=fit_beta_kwargs,\n                                                             nb_workers=nb_workers)\n\n    chosen_covariates = []\n    chosen_covariates_j = {}\n\n    _params_cols = self._get_params_cols_from_res_df(self.marginal_estimates_df)\n\n    for c in _params_cols:\n        if self.TwoStagesFitter_type == 'Exact':\n            event = int(c[0])\n        else:\n            event = int(c[1:].split('_')[0])\n        chosen_covariates_j[event] = self.marginal_estimates_df[self.marginal_estimates_df[c].abs() &gt;= self.threshold].index.tolist()\n        chosen_covariates.extend(chosen_covariates_j[event])\n\n    self.chosen_covariates = sorted(np.unique(chosen_covariates))\n    self.chosen_covariates_j = chosen_covariates_j\n\n    if fit_final_model:\n        if self.TwoStagesFitter_type == 'Exact':\n            self.final_model = TwoStagesFitterExact()\n        else:\n            self.final_model = TwoStagesFitter()\n        self.final_model.fit(df=df,\n                             covariates=self.chosen_covariates_j,\n                             event_type_col=event_type_col,\n                             duration_col=duration_col,\n                             pid_col=pid_col,\n                             x0=x0,\n                             fit_beta_kwargs=fit_beta_kwargs,\n                             verbose=verbose,\n                             nb_workers=nb_workers)\n\n    return self.final_model\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.fit_marginal_model","title":"<code>fit_marginal_model(expanded_df, covariate, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=1)</code>","text":"<p>This method fits a marginal model to data using a single covariate. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).</p> <p>Parameters:</p> Name Type Description Default <code>expanded_df</code> <code>DataFrame</code> <p>expanded training data for fitting the model</p> required <code>covariate</code> <code>str</code> <p>a single covariate to be used in estimating the regression coefficients</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>Estimated parameter and standard errors. TwoStagesFitter.get_beta_SE() output.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def fit_marginal_model(self,\n                       expanded_df,\n                       covariate: str,\n                       event_type_col: str = 'J',\n                       duration_col: str = 'X',\n                       pid_col: str = 'pid',\n                       x0: Union[np.array, int] = 0,\n                       fit_beta_kwargs: dict = {},\n                       verbose: int = 2,\n                       nb_workers: int = 1):\n    \"\"\"\n    This method fits a marginal model to data using a single covariate. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).\n\n    Args:\n        expanded_df (pd.DataFrame): expanded training data for fitting the model\n        covariate (str): a single covariate to be used in estimating the regression coefficients\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        result (pd.DataFrame): Estimated parameter and standard errors. TwoStagesFitter.get_beta_SE() output.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(expanded_df[event_type_col].unique()) if c != 0]\n\n    if self.TwoStagesFitter_type == 'Exact':\n        marginal_model = MarginalTwoStagesFitterExact()\n    else:\n        marginal_model = MarginalTwoStagesFitter()\n\n    marginal_model.fit(\n        expanded_df=expanded_df[[pid_col, covariate, event_type_col, duration_col, 'j_0'] +\n                                [f'j_{e}' for e in self.events]],\n        covariates=[covariate],\n        event_type_col=event_type_col,\n        duration_col=duration_col,\n        pid_col=pid_col,\n        x0=x0,\n        fit_beta_kwargs=fit_beta_kwargs,\n        verbose=verbose,\n        nb_workers=nb_workers)\n\n    result = marginal_model.get_beta_SE()\n    del marginal_model\n    return result\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.get_data_driven_threshold","title":"<code>get_data_driven_threshold(df, covariates=None, quantile=1, event_type_col='J', duration_col='X', pid_col='pid', x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS, seed=None)</code>","text":"<p>This method calculates a data-driven threshold for each risk. It fits marginal models to the permuted data and returns the required quantile of the absolute values of the coefficients estimated from the null model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>quantile</code> <code>float</code> <p>represents the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.</p> <code>1</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <code>seed</code> <code>int</code> <p>pseudo random state.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>threshold</code> <code>Series</code> <p>Estimated thresholds.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def get_data_driven_threshold(self,\n                              df,\n                              covariates: List = None,\n                              quantile: float = 1,\n                              event_type_col: str = 'J',\n                              duration_col: str = 'X',\n                              pid_col: str = 'pid',\n                              x0: Union[np.array, int] = 0,\n                              fit_beta_kwargs: dict = {},\n                              verbose: int = 2,\n                              nb_workers: int = WORKERS,\n                              seed: int = None):\n\n    \"\"\"\n    This method calculates a data-driven threshold for each risk. It fits marginal models to the permuted data and returns the required quantile of the absolute values of the coefficients estimated from the null model.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        quantile (float): represents the quantile of the absolute values of the coefficients from the null model that determines the data-driven threshold. Defaults to 1, which corresponds to the maximum absolute value of the null model's coefficients.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n        seed (int): pseudo random state.\n\n    Returns:\n        threshold (pd.Series): Estimated thresholds.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n\n    self.permute_df(df=df, event_type_col=event_type_col,\n                    duration_col=duration_col, pid_col=pid_col, seed=seed)\n    self.null_model_df = self.get_marginal_estimates(expanded_df=self.permuted_expanded_df,\n                                                     covariates=covariates,\n                                                     event_type_col=event_type_col,\n                                                     duration_col=duration_col,\n                                                     pid_col=pid_col,\n                                                     verbose=verbose,\n                                                     x0=x0,\n                                                     fit_beta_kwargs=fit_beta_kwargs,\n                                                     nb_workers=nb_workers)\n\n    _params_cols = self._get_params_cols_from_res_df(self.null_model_df)\n    self.threshold = np.quantile(self.null_model_df[_params_cols].abs().values, q=quantile)\n    return self.threshold\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.get_marginal_estimates","title":"<code>get_marginal_estimates(expanded_df, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', verbose=2, x0=0, fit_beta_kwargs={}, nb_workers=WORKERS)</code>","text":"<p>This method fits a marginal model to data to each of the covariates. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).</p> <p>Parameters:</p> Name Type Description Default <code>expanded_df</code> <code>DataFrame</code> <p>expanded training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to estimate the marginal regression coefficient for.</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>Estimated parameters and standard errors of the marginal models. A concatenation of all the TwoStagesFitter.get_beta_SE() outputs.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def get_marginal_estimates(self,\n                           expanded_df,\n                           covariates: Union[List, dict] = None,\n                           event_type_col: str = 'J',\n                           duration_col: str = 'X',\n                           pid_col: str = 'pid',\n                           verbose: int = 2,\n                           x0: Union[np.array, int] = 0,\n                           fit_beta_kwargs: dict = {},\n                           nb_workers: int = WORKERS):\n\n    \"\"\"\n    This method fits a marginal model to data to each of the covariates. Note that the expanded discrete-time data is expected as an input (see the Methods section of PyDTS documentation and pydts.utils.get_expanded_df).\n\n    Args:\n        expanded_df (pd.DataFrame): expanded training data for fitting the model\n        covariates (list): list of covariates to estimate the marginal regression coefficient for.\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        verbose (int, Optional): The verbosity level of pandaallel\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        results_df (pd.DataFrame): Estimated parameters and standard errors of the marginal models. A concatenation of all the TwoStagesFitter.get_beta_SE() outputs.\n    \"\"\"\n\n    if self.events is None:\n        self.events = [c for c in sorted(expanded_df[event_type_col].unique()) if c != 0]\n\n    if covariates is None:\n        covariates = [col for col in expanded_df if col not in ([event_type_col, duration_col, pid_col, 'j_0'] +\n                      [f'j_{e}' for e in self.events])]\n\n    parallel = Parallel(n_jobs=nb_workers, verbose=verbose)\n    results_df = pd.DataFrame()\n    if isinstance(covariates, list):\n        _results = parallel(delayed(self.fit_marginal_model)(expanded_df, cov,\n                                                             event_type_col, duration_col, pid_col,\n                                                             x0, fit_beta_kwargs, verbose, nb_workers)\n                                                             for cov in covariates)\n        results_df = pd.concat(_results)\n    elif isinstance(covariates, dict):\n        raise ValueError(\"Please provide a list of covariates for the marginal testing, including the union of all possible options across all risks.\")\n    # elif isinstance(covariates, dict):\n    #     for event in self.events:\n    #         _results = parallel(delayed(self.fit_marginal_model)(expanded_df, cov,\n    #                                                              event_type_col, duration_col, pid_col,\n    #                                                              x0, fit_beta_kwargs, verbose, nb_workers)\n    #                                                              for cov in covariates[event])\n    #         event_results_df = pd.concat(_results)\n    #         results_df = pd.concat([results_df, event_results_df], axis=1)\n\n    return results_df.astype(float)\n</code></pre>"},{"location":"api/screening/#pydts.screening.SISTwoStagesFitterExact.permute_df","title":"<code>permute_df(df, event_type_col='J', duration_col='X', pid_col='pid', seed=None)</code>","text":"<p>This method applies random permutation on the event-time and event-type columns of the training data such that the covariates are decoupled from the outcome; the permuted data follow the null model.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>seed</code> <code>(int, Optional)</code> <p>pseudo random state.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>permuted_df</code> <code>DataFrame</code> <p>null model data.</p> Source code in <code>src/pydts/screening.py</code> <pre><code>def permute_df(self,\n               df,\n               event_type_col: str = 'J',\n               duration_col: str = 'X',\n               pid_col: str = 'pid',\n               seed: int = None):\n\n    \"\"\"\n    This method applies random permutation on the event-time and event-type columns of the training data such that the covariates are decoupled from the outcome; the permuted data follow the null model.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        seed (int, Optional): pseudo random state.\n\n    Returns:\n        permuted_df (pd.DataFrame): null model data.\n    \"\"\"\n\n    permuted_df = df.copy()\n    np.random.seed(seed)\n    permuted_index = np.random.permutation(permuted_df.index)\n    permuted_df.loc[:, duration_col] = df.loc[permuted_index, duration_col].values\n    permuted_df.loc[:, event_type_col] = df.loc[permuted_index, event_type_col].values\n    self.permuted_df = permuted_df\n    self.permuted_expanded_df = get_expanded_df(df=self.permuted_df,\n                                                event_type_col=event_type_col,\n                                                duration_col=duration_col,\n                                                pid_col=pid_col)\n    return permuted_df\n</code></pre>"},{"location":"api/two_stages_fitter/","title":"The Two Stages Procedure of Meir and Gorfine (2023) - Efron","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter","title":"<code>pydts.fitters.TwoStagesFitter()</code>","text":"<p>               Bases: <code>ExpansionBasedFitter</code></p> <p>This class implements the approach of Meir et al. (2022):</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.alpha_df = pd.DataFrame()\n    self.beta_models = {}\n    self.beta_models_params_attr = 'params_'\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.alpha_df","title":"<code>alpha_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.beta_models","title":"<code>beta_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.beta_models_params_attr","title":"<code>beta_models_params_attr = 'params_'</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.covariates","title":"<code>covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.duration_col","title":"<code>duration_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.event_models","title":"<code>event_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.event_type_col","title":"<code>event_type_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.events","title":"<code>events = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.expanded_df","title":"<code>expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.formula","title":"<code>formula = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.pid_col","title":"<code>pid_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.times","title":"<code>times = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._alpha_jt","title":"<code>_alpha_jt(x, df, y_t, beta_j, n_jt, t, event)</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _alpha_jt(self, x, df, y_t, beta_j, n_jt, t, event):\n    # Alpha_jt optimization objective\n    partial_df = df[df[self.duration_col] &gt;= t]\n    if isinstance(self.covariates, list):\n        expit_add = np.dot(partial_df[self.covariates], beta_j)\n    elif isinstance(self.covariates, dict):\n        expit_add = np.dot(partial_df[self.covariates[event]], beta_j)\n    else:\n        raise ValueError\n    return ((1 / y_t) * np.sum(expit(x + expit_add)) - (n_jt / y_t)) ** 2\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._expand_data","title":"<code>_expand_data(df, event_type_col, duration_col, pid_col)</code>","text":"<p>This method expands the raw data as explained in Lee et al. 2018</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to expand.</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df),                   Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> required <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Expanded df (pandas.DataFrame): the expanded dataframe.</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _expand_data(self,\n                 df: pd.DataFrame,\n                 event_type_col: str,\n                 duration_col: str,\n                 pid_col: str) -&gt; pd.DataFrame:\n    \"\"\"\n    This method expands the raw data as explained in Lee et al. 2018\n\n    Args:\n        df (pandas.DataFrame): Dataframe to expand.\n        event_type_col (str): The event type column name (must be a column in df),\n                              Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n\n    Returns:\n        Expanded df (pandas.DataFrame): the expanded dataframe.\n    \"\"\"\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    return get_expanded_df(df=df, event_type_col=event_type_col, duration_col=duration_col, pid_col=pid_col)\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._fit_beta","title":"<code>_fit_beta(expanded_df, events, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={})</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _fit_beta(self, expanded_df, events, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={}):\n    # Model fitting for conditional estimation of Beta_j for all events\n    _model_kwargs_per_event = np.any([event in model_kwargs.keys() for event in events])\n    _model_fit_kwargs_per_event = np.any([event in model_fit_kwargs.keys() for event in events])\n    beta_models = {}\n    for event in events:\n        _model_kwargs = model_kwargs[event] if _model_kwargs_per_event else model_kwargs\n        _model_fit_kwargs = model_fit_kwargs[event] if _model_fit_kwargs_per_event else model_fit_kwargs\n        beta_models[event] = self._fit_event_beta(expanded_df=expanded_df, event=event,\n                                                  model=model, model_kwargs=_model_kwargs,\n                                                  model_fit_kwargs=_model_fit_kwargs)\n    return beta_models\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._fit_event_beta","title":"<code>_fit_event_beta(expanded_df, event, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={})</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _fit_event_beta(self, expanded_df, event, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={}):\n    # Model fitting for conditional estimation of Beta_j for specific event\n    if isinstance(self.covariates, list):\n        strata_df = expanded_df[self.covariates + [f'j_{event}', self.duration_col]].copy()\n    elif isinstance(self.covariates, dict):\n        strata_df = expanded_df[self.covariates[event] + [f'j_{event}', self.duration_col]].copy()\n    else:\n        raise TypeError\n    strata_df.loc[:, f'{self.duration_col}_copy'] = np.ones_like(expanded_df[self.duration_col])\n\n    beta_j_model = model(**model_kwargs)\n    if isinstance(self.covariates, list):\n        beta_j_model.fit(df=strata_df[self.covariates + [f'{self.duration_col}', f'{self.duration_col}_copy', f'j_{event}']],\n                         duration_col=f'{self.duration_col}_copy', event_col=f'j_{event}', strata=self.duration_col,\n                         **model_fit_kwargs, batch_mode=False)\n    elif isinstance(self.covariates, dict):\n        beta_j_model.fit(df=strata_df[self.covariates[event] + [f'{self.duration_col}', f'{self.duration_col}_copy', f'j_{event}']],\n                         duration_col=f'{self.duration_col}_copy', event_col=f'j_{event}', strata=self.duration_col,\n                         **model_fit_kwargs, batch_mode=False)\n    return beta_j_model\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._hazard_inverse_transformation","title":"<code>_hazard_inverse_transformation(a)</code>","text":"<p>This function defines the inverse transformation of the hazard function such that \\(\\lambda_j (t | Z) = h^{-1} ( \u0007lpha_{jt} + Z^{T} \beta_{j} )\\)</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[int, array, Series, DataFrame]</code> required <p>Returns:</p> Name Type Description <code>i</code> <code>Union[int, array, Series, DataFrame]</code> <p>the inverse function applied on a. $ h^{-1} (a) $</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def _hazard_inverse_transformation(self, a: Union[int, np.array, pd.Series, pd.DataFrame]) -&gt; \\\n        Union[int, np.array, pd.Series, pd.DataFrame]:\n    \"\"\"\n    This function defines the inverse transformation of the hazard function such that $\\lambda_j (t | Z) = h^{-1} ( \\alpha_{jt} + Z^{T} \\beta_{j} )$\n\n    Args:\n        a (Union[int, np.array, pd.Series, pd.DataFrame]):\n\n    Returns:\n        i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a) $\n    \"\"\"\n    i = expit(a)\n    return i\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._hazard_transformation","title":"<code>_hazard_transformation(a)</code>","text":"<p>This function defines the transformation of the hazard function such that $ h ( \\lambda_j (t | Z) ) = \u0007lpha_{jt} + Z^{T} \beta_{j} $</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[int, array, Series, DataFrame]</code> required <p>Returns:</p> Name Type Description <code>i</code> <code>Union[int, array, Series, DataFrame]</code> <p>the inverse function applied on a. $ h^{-1} (a)$</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def _hazard_transformation(self, a: Union[int, np.array, pd.Series, pd.DataFrame]) -&gt; \\\n        Union[int, np.array, pd.Series, pd.DataFrame]:\n    \"\"\"\n    This function defines the transformation of the hazard function such that $ h ( \\lambda_j (t | Z) ) = \\alpha_{jt} + Z^{T} \\beta_{j} $\n\n    Args:\n        a (Union[int, np.array, pd.Series, pd.DataFrame]):\n\n    Returns:\n        i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a)$\n    \"\"\"\n\n    i = logit(a)\n    return i\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._validate_cols","title":"<code>_validate_cols(df, event_type_col, duration_col, pid_col)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_cols(self, df, event_type_col, duration_col, pid_col):\n    assert event_type_col in df.columns, f'Event type column is missing from df: {event_type_col}'\n    assert duration_col in df.columns, f'Duration column is missing from df: {duration_col}'\n    assert pid_col in df.columns, f'Observation ID column is missing from df: {pid_col}'\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._validate_covariates_in_df","title":"<code>_validate_covariates_in_df(df)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_covariates_in_df(self, df):\n    cov_not_fitted = []\n    if isinstance(self.covariates, list):\n        cov_not_fitted = [cov for cov in self.covariates if cov not in df.columns]\n    elif isinstance(self.covariates, dict):\n        for event in self.events:\n            event_cov_not_fitted = [cov for cov in self.covariates[event] if cov not in df.columns]\n            cov_not_fitted.extend(event_cov_not_fitted)\n    assert len(cov_not_fitted) == 0, \\\n        f\"Cannot predict - required covariates are missing from df: {cov_not_fitted}\"\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter._validate_t","title":"<code>_validate_t(t, return_iter=True)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_t(self, t, return_iter=True):\n    _t = np.array([t]) if not isinstance(t, Iterable) else t\n    t_i_not_fitted = [t_i for t_i in _t if (t_i not in self.times)]\n    assert len(t_i_not_fitted) == 0, \\\n        f\"Cannot predict for times which were not included during .fit(): {t_i_not_fitted}\"\n    if return_iter:\n        return _t\n    return t\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.evaluate","title":"<code>evaluate(test_df, oracle_col='T', **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def evaluate(self, test_df: pd.DataFrame, oracle_col: str = 'T', **kwargs) -&gt; float:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.fit","title":"<code>fit(df, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', skip_expansion=False, x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS)</code>","text":"<p>This method fits a model to the discrete data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to be used in estimating the regression coefficients</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>skip_expansion</code> <code>boolean</code> <p>Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.</p> <code>False</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <p>Returns:</p> Name Type Description <code>event_models</code> <code>dict</code> <p>Fitted models dictionary. Keys - event names, Values - fitted models for the event.</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def fit(self,\n        df: pd.DataFrame,\n        covariates: Union[list, dict] = None,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid',\n        skip_expansion: bool = False,\n        x0: Union[np.array, int] = 0,\n        fit_beta_kwargs: dict = {},\n        verbose: int = 2,\n        nb_workers: int = WORKERS) -&gt; dict:\n    \"\"\"\n    This method fits a model to the discrete data.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        covariates (list): list of covariates to be used in estimating the regression coefficients\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event.\n    \"\"\"\n\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if (covariates is not None):\n        cov_not_in_df = []\n        if isinstance(covariates, list):\n            cov_not_in_df = [cov for cov in covariates if cov not in df.columns]\n        elif isinstance(covariates, dict):\n            for event in self.events:\n                event_cov_not_in_df = [cov for cov in covariates[event] if cov not in df.columns]\n                cov_not_in_df.extend(event_cov_not_in_df)\n        if len(cov_not_in_df) &gt; 0:\n            raise ValueError(f\"Error during fit - missing covariates from df: {cov_not_in_df}\")\n\n    #pandarallel.initialize(verbose=verbose, nb_workers=nb_workers)\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n    self.covariates = covariates\n    self.event_type_col = event_type_col\n    self.duration_col = duration_col\n    self.pid_col = pid_col\n    self.times = sorted(df[duration_col].unique())\n\n    if not skip_expansion:\n        expanded_df = self._expand_data(df=df, event_type_col=event_type_col, duration_col=duration_col,\n                                             pid_col=pid_col)\n    else:\n        print('Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.')\n        expanded_df = df\n\n    self.beta_models = self._fit_beta(expanded_df, self.events, **fit_beta_kwargs)\n\n    y_t = (df[duration_col]\n           .value_counts()\n           .sort_index(ascending=False)  # each event count for its occurring time and the times before\n           .cumsum()\n           .sort_index()\n           )\n    n_jt = df.groupby([event_type_col, duration_col]).size().to_frame().reset_index()\n    n_jt.columns = [event_type_col, duration_col, 'n_jt']\n\n    for event in self.events:\n\n        n_et = n_jt[n_jt[event_type_col] == event].copy()\n\n        if isinstance(self.beta_models[event], CoxPHFitter):\n            self.beta_models_params_attr = 'params_'\n            _res = Parallel(n_jobs=nb_workers)(delayed(minimize)(self._alpha_jt, x0=x0,\n                                                                 args=(df, y_t.loc[row[duration_col]],\n                                                                       getattr(self.beta_models[event],\n                                                                               self.beta_models_params_attr),\n                                                                       row['n_jt'],\n                                                                       row[duration_col], event),\n                                                                 method='BFGS',\n                                                                 options={'gtol': 1e-7, 'eps': 1.5e-08,\n                                                                          'maxiter': 200})\n                                                                 for _, row in n_et.iterrows())\n            n_et['success'] = Parallel(n_jobs=nb_workers)(delayed(lambda row: row.success)(val)\n                                                          for val in _res)\n            n_et['alpha_jt'] = Parallel(n_jobs=nb_workers)(delayed(lambda row: row.x[0])(val)\n                                                           for val in _res)\n\n        elif isinstance(self.beta_models[event], ConditionalResultsWrapper) or \\\n                isinstance(self.beta_models[event], RegularizedResultsWrapper):\n            self.beta_models_params_attr = 'params'\n            for idx, row in n_et.iterrows():\n                _res = minimize(self._alpha_jt,\n                                x0=x0,\n                                args=(df,\n                                      y_t.loc[row[duration_col]],\n                                      getattr(self.beta_models[event], self.beta_models_params_attr),\n                                      row['n_jt'],\n                                      row[duration_col],\n                                      event),\n                                method='BFGS',\n                                options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200})\n                n_et.loc[idx, 'success'] = _res.success\n                n_et.loc[idx, 'alpha_jt'] = _res.x[0]\n        else:\n            raise ValueError\n\n        # n_et['opt_res'] = n_et.parallel_apply(lambda row: minimize(self._alpha_jt, x0=x0,\n        #                         args=(df, y_t.loc[row[duration_col]], event_beta_params, row['n_jt'],\n        #                         row[duration_col], event), method='BFGS',\n        #                         options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200}), axis=1)\n        # n_et['success'] = n_et['opt_res'].parallel_apply(lambda val: val.success)\n        # n_et['alpha_jt'] = n_et['opt_res'].parallel_apply(lambda val: val.x[0])\n\n        assert_fit(n_et, self.times[:-1], event_type_col=event_type_col, duration_col=duration_col)  # todo move basic input validation before any optimization\n        self.event_models[event] = [self.beta_models[event], n_et]\n        self.alpha_df = pd.concat([self.alpha_df, n_et], ignore_index=True)\n    return self.event_models\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.get_alpha_df","title":"<code>get_alpha_df()</code>","text":"<p>This function returns the Alpha coefficients for all the events.</p> <p>Returns:</p> Name Type Description <code>alpha_df</code> <code>DataFrame</code> <p>Alpha coefficients Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_alpha_df(self):\n    \"\"\"\n    This function returns the Alpha coefficients for all the events.\n\n    Returns:\n        alpha_df (pandas.DataFrame): Alpha coefficients Dataframe\n    \"\"\"\n\n    alpha_df = pd.DataFrame()\n    for event, model in self.event_models.items():\n        model_alpha_df = model[1].set_index([self.event_type_col, self.duration_col])\n        model_alpha_df.columns = pd.MultiIndex.from_product([[event], model_alpha_df.columns])\n        alpha_df = pd.concat([alpha_df, model_alpha_df], axis=1)\n\n    return alpha_df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.get_beta_SE","title":"<code>get_beta_SE()</code>","text":"<p>This function returns the Beta coefficients and their Standard Errors for all the events.</p> <p>Returns:</p> Name Type Description <code>se_df</code> <code>DataFrame</code> <p>Beta coefficients and Standard Errors Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_beta_SE(self):\n    \"\"\"\n    This function returns the Beta coefficients and their Standard Errors for all the events.\n\n    Returns:\n        se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe\n    \"\"\"\n    se_df = pd.DataFrame()\n    for event, model in self.beta_models.items():\n        mdf = pd.concat([model.params_, model.standard_errors_], axis=1)\n        mdf.columns = [f'j{event}_params', f'j{event}_SE']\n        se_df = pd.concat([se_df, mdf], axis=1)\n    return se_df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_all_events_alpha","title":"<code>plot_all_events_alpha(ax=None, scatter_kwargs={}, colors=COLORS, show=True, title=None, xlabel='t', ylabel='$\\\\alpha_{jt}$', fontsize=18, ticklabelsize=15)</code>","text":"<p>This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>scatter_kwargs</code> <code>(dict, Optional)</code> <p>keywords to pass to the scatter function</p> <code>{}</code> <code>colors</code> <code>(list, Optional)</code> <p>colors names</p> <code>COLORS</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'t'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\alpha_{jt}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_all_events_alpha(self, ax: plt.Axes = None, scatter_kwargs: dict = {}, colors: list = COLORS,\n                          show: bool = True, title: Union[str, None] = None, xlabel: str = 't',\n                          ylabel: str = r'$\\alpha_{jt}$', fontsize: int = 18, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events.\n\n    Args:\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        scatter_kwargs (dict, Optional): keywords to pass to the scatter function\n        colors (list, Optional): colors names\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    title = r'$\\alpha_{jt}$' + f' for all events' if title is None else title\n    for idx, (event, model) in enumerate(self.event_models.items()):\n        label = f'{event}'\n        color = colors[idx % len(colors)]\n        self.plot_event_alpha(event=event, ax=ax, scatter_kwargs=scatter_kwargs, show=False, title=title,\n                              ylabel=ylabel, xlabel=xlabel, fontsize=fontsize, label=label, color=color,\n                              ticklabelsize=ticklabelsize)\n    ax.legend()\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_all_events_beta","title":"<code>plot_all_events_beta(ax=None, colors=COLORS, show=True, title=None, xlabel='Value', ylabel='$\\\\beta_{j}$', fontsize=18, ticklabelsize=15)</code>","text":"<p>This function plots the $ beta_{j} $ coefficients and standard errors of all the events.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>colors</code> <code>(list, Optional)</code> <p>colors names</p> <code>COLORS</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'Value'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\beta_{j}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <code>ticklabelsize</code> <code>(int, Optional)</code> <p>axes xticklabels, yticklabels fontsize</p> <code>15</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_all_events_beta(self, ax: plt.Axes = None, colors: list = COLORS, show: bool = True,\n                         title: Union[str, None] = None, xlabel: str = 'Value',  ylabel: str = r'$\\beta_{j}$',\n                         fontsize: int = 18, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots the $ beta_{j} $ coefficients and standard errors of all the events.\n\n    Args:\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        colors (list, Optional): colors names\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n        ticklabelsize (int, Optional): axes xticklabels, yticklabels fontsize\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    title = r'$\\beta_{j}$' + f' for all events' if title is None else title\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    se_df = self.get_beta_SE()\n\n    for idx, col in enumerate(se_df.columns):\n        if idx % 2 == 1:\n            continue\n        y = np.arange((idx//2)*len(se_df), (1+(idx//2))*len(se_df))\n        ax.errorbar(x=se_df.iloc[:, idx].values, y=y,\n                   color=colors[idx % len(colors)], xerr=se_df.iloc[:, idx+1].values, label=f'{col}',\n                   markersize=6, ls='', marker='o')\n\n    yt = list(se_df.index) * (len(se_df.columns) // 2)\n    ax.set_yticks(np.arange(0, len(yt)))\n    ax.set_yticklabels(yt)\n    ax.set_title(title, fontsize=fontsize)\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    ax.grid()\n    plt.gca().invert_yaxis()\n    ax.legend()\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.plot_event_alpha","title":"<code>plot_event_alpha(event, ax=None, scatter_kwargs={}, show=True, title=None, xlabel='t', ylabel='$\\\\alpha_{jt}$', fontsize=18, color=None, label=None, ticklabelsize=15)</code>","text":"<p>This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>scatter_kwargs</code> <code>(dict, Optional)</code> <p>keywords to pass to the scatter function</p> <code>{}</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'t'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\alpha_{jt}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <code>color</code> <code>(str, Optional)</code> <p>color name to use</p> <code>None</code> <code>label</code> <code>(str, Optional)</code> <p>label name</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_event_alpha(self, event: Union[str, int], ax: plt.Axes = None, scatter_kwargs: dict = {},\n                     show=True, title=None, xlabel='t', ylabel=r'$\\alpha_{jt}$', fontsize=18,\n                     color: str = None, label: str = None, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event.\n\n    Args:\n        event (Union[str, int]): event name\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        scatter_kwargs (dict, Optional): keywords to pass to the scatter function\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n        color (str, Optional): color name to use\n        label (str, Optional): label name\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n\n    assert event in self.events, f\"Cannot plot event {event} alpha - it was not included during .fit()\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    title = r'$\\alpha_{jt}$' + f' for event {event}' if title is None else title\n    label = f'{event}' if label is None else label\n    color = 'tab:blue' if color is None else color\n    alpha_df = self.event_models[event][1]\n    ax.scatter(alpha_df[self.duration_col].values, alpha_df['alpha_jt'].values, label=label,\n               color=color, **scatter_kwargs)\n    ax.set_title(title, fontsize=fontsize)\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict","title":"<code>predict(df, **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict(self, df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_cumulative_incident_function","title":"<code>predict_cumulative_incident_function(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_cumulative_incident_function(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        if f'cif_j{event}_at_t{self.times[-2]}' not in df.columns:\n            df = self.predict_event_cumulative_incident_function(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_event_cumulative_incident_function","title":"<code>predict_event_cumulative_incident_function(df, event)</code>","text":"<p>This function adds a specific event columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_event_cumulative_incident_function(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a specific event columns of the predicted hazard function, overall survival, probabilities\n    of event occurance and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_events(df=df)\n    cols = [f'prob_j{event}_at_t{t}' for t in self.times[:-1]]\n    cif_df = df[cols].cumsum(axis=1)\n    cif_df.columns = [f'cif_j{event}_at_t{t}' for t in self.times[:-1]]\n    df = pd.concat([df, cif_df], axis=1)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_full","title":"<code>predict_full(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_full(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    return self.predict_cumulative_incident_function(df)\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_hazard_all","title":"<code>predict_hazard_all(df)</code>","text":"<p>This function calculates the hazard for all the events at all time values included in the training set for each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_all(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at all time values included in the training set for each\n    event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    df = self.predict_hazard_t(df, t=self.times[:-1])\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_hazard_jt","title":"<code>predict_hazard_jt(df, event, t)</code>","text":"<p>This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>Union[Iterable, int]</code> <p>times to calculate the hazard for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def predict_hazard_jt(self,\n                      df: pd.DataFrame,\n                      event: Union[str, int],\n                      t: Union[Iterable, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        event (Union[str, int]): event name\n        t (Union[Iterable, int]): times to calculate the hazard for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    t = self._validate_t(t, return_iter=True)\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n\n    model = self.event_models[event]\n    alpha_df = model[1].set_index(self.duration_col)['alpha_jt'].copy()\n\n    _t = np.array([t_i for t_i in t if (f'hazard_j{event}_t{t_i}' not in df.columns)])\n    if len(_t) == 0:\n        return df\n    temp_df = df.copy()\n    if isinstance(self.covariates, list):\n        beta_j_x = temp_df[self.covariates].dot(getattr(model[0], self.beta_models_params_attr))\n    elif isinstance(self.covariates, dict):\n        beta_j_x = temp_df[self.covariates[event]].dot(getattr(model[0], self.beta_models_params_attr))\n    temp_df[[f'hazard_j{event}_t{c}' for c in _t]] = pd.concat(\n        [self._hazard_inverse_transformation(alpha_df[c] + beta_j_x) for c in _t], axis=1).values\n    return temp_df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_hazard_t","title":"<code>predict_hazard_t(df, t)</code>","text":"<p>This function calculates the hazard for all the events at the requested time values if they were included in the training set of each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>t</code> <code>(int, array)</code> <p>times to calculate the hazard for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_t(self, df: pd.DataFrame, t: Union[int, np.array]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at the requested time values if they were included in\n    the training set of each event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        t (int, np.array): times to calculate the hazard for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    t = self._validate_t(t)\n    self._validate_covariates_in_df(df.head())\n\n    for event, model in self.event_models.items():\n        df = self.predict_hazard_jt(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_marginal_prob_all_events","title":"<code>predict_marginal_prob_all_events(df)</code>","text":"<p>This function calculates the marginal probability per event given the covariates for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_all_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability per event given the covariates for all the events.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    for event in self.events:\n        df = self.predict_marginal_prob_event_j(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_marginal_prob_event_j","title":"<code>predict_marginal_prob_event_j(df, event)</code>","text":"<p>This function calculates the marginal probability of an event given the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_event_j(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability of an event given the covariates.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    cols = [f'prob_j{event}_at_t{_t}' for _t in self.times[:-1]]\n    marginal_prob = df[cols].sum(axis=1)\n    marginal_prob.name = f'marginal_prob_j{event}'\n    return pd.concat([df, marginal_prob], axis=1)\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_overall_survival","title":"<code>predict_overall_survival(df, t=None, return_hazards=False)</code>","text":"<p>This function adds columns of the overall survival until time t. Args:     df (pandas.DataFrame): dataframe with covariates columns     t (int): time     return_hazards (bool): if to keep the hazard columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with the additional overall survival columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_overall_survival(self,\n                             df: pd.DataFrame,\n                             t: int = None,\n                             return_hazards: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the overall survival until time t.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        t (int): time\n        return_hazards (bool): if to keep the hazard columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with the additional overall survival columns\n\n    \"\"\"\n    if t is not None:\n        self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    all_hazards = self.predict_hazard_all(df)\n    _times = self.times[:-1] if t is None else [_t for _t in self.times[:-1] if _t &lt;= t]\n    overall = pd.DataFrame()\n    for t_i in _times:\n        cols = [f'hazard_j{e}_t{t_i}' for e in self.events]\n        t_i_hazard = 1 - all_hazards[cols].sum(axis=1)\n        t_i_hazard.name = f'overall_survival_t{t_i}'\n        overall = pd.concat([overall, t_i_hazard], axis=1)\n    overall = pd.concat([df, overall.cumprod(axis=1)], axis=1)\n\n    if return_hazards:\n        cols = all_hazards.columns[all_hazards.columns.str.startswith(\"hazard_\")]\n        cols = cols.difference(overall.columns)\n        if len(cols) &gt; 0:\n            overall = pd.concat([overall, all_hazards[cols]], axis=1)\n    return overall\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_prob_event_j_all","title":"<code>predict_prob_event_j_all(df, event)</code>","text":"<p>This function adds columns of a specific event occurrence probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_all(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of a specific event occurrence probabilities.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'overall_survival_t{self.times[-2]}' not in df.columns:\n        df = self.predict_overall_survival(df, return_hazards=True)\n    for t in self.times[:-1]:\n        if f'prob_j{event}_at_t{t}' not in df.columns:\n            df = self.predict_prob_event_j_at_t(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_prob_event_j_at_t","title":"<code>predict_prob_event_j_at_t(df, event, t)</code>","text":"<p>This function adds a column with probability of occurance of a specific event at a specific a time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>int</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe an additional probability column</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_at_t(self, df: pd.DataFrame, event: Union[str, int], t: int) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a column with probability of occurance of a specific event at a specific a time.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n        t (int): time\n\n    Returns:\n        df (pandas.DataFrame): dataframe an additional probability column\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{t}' not in df.columns:\n        if t == 1:\n            if f'hazard_j{event}_t{t}' not in df.columns:\n                df = self.predict_hazard_jt(df=df, event=event, t=t)\n            df[f'prob_j{event}_at_t{t}'] = df[f'hazard_j{event}_t{t}']\n            return df\n        elif not f'overall_survival_t{t - 1}' in df.columns:\n            df = self.predict_overall_survival(df, t=t, return_hazards=True)\n        elif not f'hazard_j{event}_t{t}' in df.columns:\n            df = self.predict_hazard_t(df, t=np.array([_t for _t in self.times[:-1] if _t &lt;= t]))\n        df[f'prob_j{event}_at_t{t}'] = df[f'overall_survival_t{t - 1}'] * df[f'hazard_j{event}_t{t}']\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.predict_prob_events","title":"<code>predict_prob_events(df)</code>","text":"<p>This function adds columns of all the events occurance probabilities. Args:     df (pandas.DataFrame): dataframe with covariates columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of all the events occurance probabilities.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter/#pydts.fitters.TwoStagesFitter.print_summary","title":"<code>print_summary(summary_func='print_summary', summary_kwargs={})</code>","text":"<p>This method prints the summary of the fitted models for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>summary_func</code> <code>(str, Optional)</code> <p>print summary method of the fitted model type (\"summary\", \"print_summary\").</p> <code>'print_summary'</code> <code>summary_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to the model summary function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def print_summary(self,\n                  summary_func: str = \"print_summary\",\n                  summary_kwargs: dict = {}) -&gt; None:\n    \"\"\"\n    This method prints the summary of the fitted models for all the events.\n\n    Args:\n        summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\").\n        summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        from IPython.display import display\n        display(self.get_beta_SE())\n\n        for event, model in self.event_models.items():\n            print(f'\\n\\nModel summary for event: {event}')\n            display(model[1].set_index([self.event_type_col, self.duration_col]))\n    except:\n        print(self.get_beta_SE())\n\n        for event, model in self.event_models.items():\n            print(f'\\n\\nModel summary for event: {event}')\n            print(model[1].set_index([self.event_type_col, self.duration_col]))\n</code></pre>"},{"location":"api/two_stages_fitter_exact/","title":"The Two Stages Procedure of Meir and Gorfine (2023) - Exact","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact","title":"<code>pydts.fitters.TwoStagesFitterExact()</code>","text":"<p>               Bases: <code>TwoStagesFitter</code></p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.beta_models_params_attr = 'params'\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.WORKERS","title":"<code>WORKERS = psutil.cpu_count(logical=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.alpha_df","title":"<code>alpha_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.beta_models","title":"<code>beta_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.beta_models_params_attr","title":"<code>beta_models_params_attr = 'params'</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.covariates","title":"<code>covariates = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.duration_col","title":"<code>duration_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.event_models","title":"<code>event_models = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.event_type_col","title":"<code>event_type_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.events","title":"<code>events = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.expanded_df","title":"<code>expanded_df = pd.DataFrame()</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.formula","title":"<code>formula = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.pid_col","title":"<code>pid_col = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.times","title":"<code>times = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._alpha_jt","title":"<code>_alpha_jt(x, df, y_t, beta_j, n_jt, t, event)</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _alpha_jt(self, x, df, y_t, beta_j, n_jt, t, event):\n    # Alpha_jt optimization objective\n    partial_df = df[df[self.duration_col] &gt;= t]\n    if isinstance(self.covariates, list):\n        expit_add = np.dot(partial_df[self.covariates], beta_j)\n    elif isinstance(self.covariates, dict):\n        expit_add = np.dot(partial_df[self.covariates[event]], beta_j)\n    else:\n        raise ValueError\n    return ((1 / y_t) * np.sum(expit(x + expit_add)) - (n_jt / y_t)) ** 2\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._expand_data","title":"<code>_expand_data(df, event_type_col, duration_col, pid_col)</code>","text":"<p>This method expands the raw data as explained in Lee et al. 2018</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to expand.</p> required <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df),                   Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> required <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> required <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Expanded df (pandas.DataFrame): the expanded dataframe.</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _expand_data(self,\n                 df: pd.DataFrame,\n                 event_type_col: str,\n                 duration_col: str,\n                 pid_col: str) -&gt; pd.DataFrame:\n    \"\"\"\n    This method expands the raw data as explained in Lee et al. 2018\n\n    Args:\n        df (pandas.DataFrame): Dataframe to expand.\n        event_type_col (str): The event type column name (must be a column in df),\n                              Right censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n\n    Returns:\n        Expanded df (pandas.DataFrame): the expanded dataframe.\n    \"\"\"\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    return get_expanded_df(df=df, event_type_col=event_type_col, duration_col=duration_col, pid_col=pid_col)\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._fit_beta","title":"<code>_fit_beta(expanded_df, events, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={})</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _fit_beta(self, expanded_df, events, model=CoxPHFitter, model_kwargs={}, model_fit_kwargs={}):\n    # Model fitting for conditional estimation of Beta_j for all events\n    _model_kwargs_per_event = np.any([event in model_kwargs.keys() for event in events])\n    _model_fit_kwargs_per_event = np.any([event in model_fit_kwargs.keys() for event in events])\n    beta_models = {}\n    for event in events:\n        _model_kwargs = model_kwargs[event] if _model_kwargs_per_event else model_kwargs\n        _model_fit_kwargs = model_fit_kwargs[event] if _model_fit_kwargs_per_event else model_fit_kwargs\n        beta_models[event] = self._fit_event_beta(expanded_df=expanded_df, event=event,\n                                                  model=model, model_kwargs=_model_kwargs,\n                                                  model_fit_kwargs=_model_fit_kwargs)\n    return beta_models\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._fit_event_beta","title":"<code>_fit_event_beta(expanded_df, event, model=ConditionalLogit, model_kwargs={}, model_fit_kwargs={})</code>","text":"Source code in <code>src/pydts/fitters.py</code> <pre><code>def _fit_event_beta(self, expanded_df, event, model=ConditionalLogit, model_kwargs={}, model_fit_kwargs={}):\n    # Model fitting for conditional estimation of Beta_j for specific event\n    if isinstance(self.covariates, dict):\n        _covs = self.covariates[event]\n    else:\n        _covs = self.covariates\n\n    expanded_df = expanded_df.apply(pd.to_numeric)\n    beta_j_model = ConditionalLogit(endog=expanded_df[f'j_{event}'],\n                                    exog=expanded_df[_covs],\n                                    groups=expanded_df[self.duration_col],\n                                    **model_kwargs)\n\n    if ('alpha' in model_fit_kwargs.keys()):\n        # Use 0 &lt;= L1_wt &lt;= 1 parameter to switch between L2 (L1_wt = 0) and L1 (L1_wt = 1) or elastic net.\n        # alpha is the the penalty weight.\n        beta_j_model = beta_j_model.fit_regularized(**model_fit_kwargs)\n    else:\n        beta_j_model = beta_j_model.fit(**model_fit_kwargs)\n\n    return beta_j_model\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._hazard_inverse_transformation","title":"<code>_hazard_inverse_transformation(a)</code>","text":"<p>This function defines the inverse transformation of the hazard function such that \\(\\lambda_j (t | Z) = h^{-1} ( \u0007lpha_{jt} + Z^{T} \beta_{j} )\\)</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[int, array, Series, DataFrame]</code> required <p>Returns:</p> Name Type Description <code>i</code> <code>Union[int, array, Series, DataFrame]</code> <p>the inverse function applied on a. $ h^{-1} (a) $</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def _hazard_inverse_transformation(self, a: Union[int, np.array, pd.Series, pd.DataFrame]) -&gt; \\\n        Union[int, np.array, pd.Series, pd.DataFrame]:\n    \"\"\"\n    This function defines the inverse transformation of the hazard function such that $\\lambda_j (t | Z) = h^{-1} ( \\alpha_{jt} + Z^{T} \\beta_{j} )$\n\n    Args:\n        a (Union[int, np.array, pd.Series, pd.DataFrame]):\n\n    Returns:\n        i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a) $\n    \"\"\"\n    i = expit(a)\n    return i\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._hazard_transformation","title":"<code>_hazard_transformation(a)</code>","text":"<p>This function defines the transformation of the hazard function such that $ h ( \\lambda_j (t | Z) ) = \u0007lpha_{jt} + Z^{T} \beta_{j} $</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[int, array, Series, DataFrame]</code> required <p>Returns:</p> Name Type Description <code>i</code> <code>Union[int, array, Series, DataFrame]</code> <p>the inverse function applied on a. $ h^{-1} (a)$</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def _hazard_transformation(self, a: Union[int, np.array, pd.Series, pd.DataFrame]) -&gt; \\\n        Union[int, np.array, pd.Series, pd.DataFrame]:\n    \"\"\"\n    This function defines the transformation of the hazard function such that $ h ( \\lambda_j (t | Z) ) = \\alpha_{jt} + Z^{T} \\beta_{j} $\n\n    Args:\n        a (Union[int, np.array, pd.Series, pd.DataFrame]):\n\n    Returns:\n        i (Union[int, np.array, pd.Series, pd.DataFrame]): the inverse function applied on a. $ h^{-1} (a)$\n    \"\"\"\n\n    i = logit(a)\n    return i\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._validate_cols","title":"<code>_validate_cols(df, event_type_col, duration_col, pid_col)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_cols(self, df, event_type_col, duration_col, pid_col):\n    assert event_type_col in df.columns, f'Event type column is missing from df: {event_type_col}'\n    assert duration_col in df.columns, f'Duration column is missing from df: {duration_col}'\n    assert pid_col in df.columns, f'Observation ID column is missing from df: {pid_col}'\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._validate_covariates_in_df","title":"<code>_validate_covariates_in_df(df)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_covariates_in_df(self, df):\n    cov_not_fitted = []\n    if isinstance(self.covariates, list):\n        cov_not_fitted = [cov for cov in self.covariates if cov not in df.columns]\n    elif isinstance(self.covariates, dict):\n        for event in self.events:\n            event_cov_not_fitted = [cov for cov in self.covariates[event] if cov not in df.columns]\n            cov_not_fitted.extend(event_cov_not_fitted)\n    assert len(cov_not_fitted) == 0, \\\n        f\"Cannot predict - required covariates are missing from df: {cov_not_fitted}\"\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact._validate_t","title":"<code>_validate_t(t, return_iter=True)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def _validate_t(self, t, return_iter=True):\n    _t = np.array([t]) if not isinstance(t, Iterable) else t\n    t_i_not_fitted = [t_i for t_i in _t if (t_i not in self.times)]\n    assert len(t_i_not_fitted) == 0, \\\n        f\"Cannot predict for times which were not included during .fit(): {t_i_not_fitted}\"\n    if return_iter:\n        return _t\n    return t\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.evaluate","title":"<code>evaluate(test_df, oracle_col='T', **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def evaluate(self, test_df: pd.DataFrame, oracle_col: str = 'T', **kwargs) -&gt; float:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.fit","title":"<code>fit(df, covariates=None, event_type_col='J', duration_col='X', pid_col='pid', skip_expansion=False, x0=0, fit_beta_kwargs={}, verbose=2, nb_workers=WORKERS)</code>","text":"<p>This method fits a model to the discrete data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>training data for fitting the model</p> required <code>covariates</code> <code>list</code> <p>list of covariates to be used in estimating the regression coefficients</p> <code>None</code> <code>event_type_col</code> <code>str</code> <p>The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Last follow up time column name (must be a column in df).</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Sample ID column name (must be a column in df).</p> <code>'pid'</code> <code>skip_expansion</code> <code>boolean</code> <p>Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.</p> <code>False</code> <code>x0</code> <code>(Union[array, int], Optional)</code> <p>initial guess to pass to scipy.optimize.minimize function</p> <code>0</code> <code>fit_beta_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass on to the estimation procedure.</p> <code>{}</code> <code>verbose</code> <code>(int, Optional)</code> <p>The verbosity level of pandaallel</p> <code>2</code> <code>nb_workers</code> <code>(int, Optional)</code> <p>The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.</p> <code>WORKERS</code> <p>Returns:</p> Name Type Description <code>event_models</code> <code>dict</code> <p>Fitted models dictionary. Keys - event names, Values - fitted models for the event.</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def fit(self,\n        df: pd.DataFrame,\n        covariates: Union[list, dict] = None,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid',\n        skip_expansion: bool = False,\n        x0: Union[np.array, int] = 0,\n        fit_beta_kwargs: dict = {},\n        verbose: int = 2,\n        nb_workers: int = WORKERS) -&gt; dict:\n    \"\"\"\n    This method fits a model to the discrete data.\n\n    Args:\n        df (pd.DataFrame): training data for fitting the model\n        covariates (list): list of covariates to be used in estimating the regression coefficients\n        event_type_col (str): The event type column name (must be a column in df), Right-censored sample (i) is indicated by event value 0, df.loc[i, event_type_col] = 0.\n        duration_col (str): Last follow up time column name (must be a column in df).\n        pid_col (str): Sample ID column name (must be a column in df).\n        skip_expansion (boolean): Skips the dataframe expansion step. Use this option only if the provided dataframe (df) is already correctly expanded. When set to True, the df is expected to be in the format produced by the pydts.utils.get_expanded_df() method, as if it were applied to the unexpanded data.\n        x0 (Union[numpy.array, int], Optional): initial guess to pass to scipy.optimize.minimize function\n        fit_beta_kwargs (dict, Optional): Keyword arguments to pass on to the estimation procedure.\n        verbose (int, Optional): The verbosity level of pandaallel\n        nb_workers (int, Optional): The number of workers to pandaallel. If not sepcified, defaults to the number of workers available.\n\n    Returns:\n        event_models (dict): Fitted models dictionary. Keys - event names, Values - fitted models for the event.\n    \"\"\"\n\n    self._validate_cols(df, event_type_col, duration_col, pid_col)\n    self.events = [c for c in sorted(df[event_type_col].unique()) if c != 0]\n    if (covariates is not None):\n        cov_not_in_df = []\n        if isinstance(covariates, list):\n            cov_not_in_df = [cov for cov in covariates if cov not in df.columns]\n        elif isinstance(covariates, dict):\n            for event in self.events:\n                event_cov_not_in_df = [cov for cov in covariates[event] if cov not in df.columns]\n                cov_not_in_df.extend(event_cov_not_in_df)\n        if len(cov_not_in_df) &gt; 0:\n            raise ValueError(f\"Error during fit - missing covariates from df: {cov_not_in_df}\")\n\n    #pandarallel.initialize(verbose=verbose, nb_workers=nb_workers)\n    if covariates is None:\n        covariates = [col for col in df if col not in [event_type_col, duration_col, pid_col]]\n    self.covariates = covariates\n    self.event_type_col = event_type_col\n    self.duration_col = duration_col\n    self.pid_col = pid_col\n    self.times = sorted(df[duration_col].unique())\n\n    if not skip_expansion:\n        expanded_df = self._expand_data(df=df, event_type_col=event_type_col, duration_col=duration_col,\n                                             pid_col=pid_col)\n    else:\n        print('Skipping data expansion step, only use this option if the provided dataframe (df) is already correctly expanded.')\n        expanded_df = df\n\n    self.beta_models = self._fit_beta(expanded_df, self.events, **fit_beta_kwargs)\n\n    y_t = (df[duration_col]\n           .value_counts()\n           .sort_index(ascending=False)  # each event count for its occurring time and the times before\n           .cumsum()\n           .sort_index()\n           )\n    n_jt = df.groupby([event_type_col, duration_col]).size().to_frame().reset_index()\n    n_jt.columns = [event_type_col, duration_col, 'n_jt']\n\n    for event in self.events:\n\n        n_et = n_jt[n_jt[event_type_col] == event].copy()\n\n        if isinstance(self.beta_models[event], CoxPHFitter):\n            self.beta_models_params_attr = 'params_'\n            _res = Parallel(n_jobs=nb_workers)(delayed(minimize)(self._alpha_jt, x0=x0,\n                                                                 args=(df, y_t.loc[row[duration_col]],\n                                                                       getattr(self.beta_models[event],\n                                                                               self.beta_models_params_attr),\n                                                                       row['n_jt'],\n                                                                       row[duration_col], event),\n                                                                 method='BFGS',\n                                                                 options={'gtol': 1e-7, 'eps': 1.5e-08,\n                                                                          'maxiter': 200})\n                                                                 for _, row in n_et.iterrows())\n            n_et['success'] = Parallel(n_jobs=nb_workers)(delayed(lambda row: row.success)(val)\n                                                          for val in _res)\n            n_et['alpha_jt'] = Parallel(n_jobs=nb_workers)(delayed(lambda row: row.x[0])(val)\n                                                           for val in _res)\n\n        elif isinstance(self.beta_models[event], ConditionalResultsWrapper) or \\\n                isinstance(self.beta_models[event], RegularizedResultsWrapper):\n            self.beta_models_params_attr = 'params'\n            for idx, row in n_et.iterrows():\n                _res = minimize(self._alpha_jt,\n                                x0=x0,\n                                args=(df,\n                                      y_t.loc[row[duration_col]],\n                                      getattr(self.beta_models[event], self.beta_models_params_attr),\n                                      row['n_jt'],\n                                      row[duration_col],\n                                      event),\n                                method='BFGS',\n                                options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200})\n                n_et.loc[idx, 'success'] = _res.success\n                n_et.loc[idx, 'alpha_jt'] = _res.x[0]\n        else:\n            raise ValueError\n\n        # n_et['opt_res'] = n_et.parallel_apply(lambda row: minimize(self._alpha_jt, x0=x0,\n        #                         args=(df, y_t.loc[row[duration_col]], event_beta_params, row['n_jt'],\n        #                         row[duration_col], event), method='BFGS',\n        #                         options={'gtol': 1e-7, 'eps': 1.5e-08, 'maxiter': 200}), axis=1)\n        # n_et['success'] = n_et['opt_res'].parallel_apply(lambda val: val.success)\n        # n_et['alpha_jt'] = n_et['opt_res'].parallel_apply(lambda val: val.x[0])\n\n        assert_fit(n_et, self.times[:-1], event_type_col=event_type_col, duration_col=duration_col)  # todo move basic input validation before any optimization\n        self.event_models[event] = [self.beta_models[event], n_et]\n        self.alpha_df = pd.concat([self.alpha_df, n_et], ignore_index=True)\n    return self.event_models\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.get_alpha_df","title":"<code>get_alpha_df()</code>","text":"<p>This function returns the Alpha coefficients for all the events.</p> <p>Returns:</p> Name Type Description <code>alpha_df</code> <code>DataFrame</code> <p>Alpha coefficients Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_alpha_df(self):\n    \"\"\"\n    This function returns the Alpha coefficients for all the events.\n\n    Returns:\n        alpha_df (pandas.DataFrame): Alpha coefficients Dataframe\n    \"\"\"\n\n    alpha_df = pd.DataFrame()\n    for event, model in self.event_models.items():\n        model_alpha_df = model[1].set_index([self.event_type_col, self.duration_col])\n        model_alpha_df.columns = pd.MultiIndex.from_product([[event], model_alpha_df.columns])\n        alpha_df = pd.concat([alpha_df, model_alpha_df], axis=1)\n\n    return alpha_df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.get_beta_SE","title":"<code>get_beta_SE()</code>","text":"<p>This function returns the Beta coefficients and their Standard Errors for all the events.</p> <p>Returns:</p> Name Type Description <code>se_df</code> <code>DataFrame</code> <p>Beta coefficients and Standard Errors Dataframe</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def get_beta_SE(self):\n    \"\"\"\n    This function returns the Beta coefficients and their Standard Errors for all the events.\n\n    Returns:\n        se_df (pandas.DataFrame): Beta coefficients and Standard Errors Dataframe\n    \"\"\"\n\n    full_table = pd.DataFrame()\n    for event in self.events:\n        if isinstance(self.beta_models[event], RegularizedResultsWrapper):\n            _p = self.beta_models[event].params.copy()\n            _p.name = 'coef'\n            full_table = pd.concat([full_table,\n                                    pd.concat([_p], keys=[event], axis=1)],\n                                   axis=1)\n        else:\n            summary = self.beta_models[event].summary()\n            summary_df = pd.DataFrame([x.split(',') for x in summary.tables[1].as_csv().split('\\n')])\n            summary_df.columns = summary_df.iloc[0]\n            summary_df = summary_df.iloc[1:].set_index(summary_df.columns[0])\n            summary_df.columns = pd.MultiIndex.from_product([[event], summary_df.columns])\n            full_table = pd.concat([full_table, summary_df.iloc[-len(self.covariates):]], axis=1)\n    return full_table\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.plot_all_events_alpha","title":"<code>plot_all_events_alpha(ax=None, scatter_kwargs={}, colors=COLORS, show=True, title=None, xlabel='t', ylabel='$\\\\alpha_{jt}$', fontsize=18, ticklabelsize=15)</code>","text":"<p>This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>scatter_kwargs</code> <code>(dict, Optional)</code> <p>keywords to pass to the scatter function</p> <code>{}</code> <code>colors</code> <code>(list, Optional)</code> <p>colors names</p> <code>COLORS</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'t'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\alpha_{jt}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_all_events_alpha(self, ax: plt.Axes = None, scatter_kwargs: dict = {}, colors: list = COLORS,\n                          show: bool = True, title: Union[str, None] = None, xlabel: str = 't',\n                          ylabel: str = r'$\\alpha_{jt}$', fontsize: int = 18, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots a scatter plot of the $ alpha_{jt} $ coefficients of all the events.\n\n    Args:\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        scatter_kwargs (dict, Optional): keywords to pass to the scatter function\n        colors (list, Optional): colors names\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    title = r'$\\alpha_{jt}$' + f' for all events' if title is None else title\n    for idx, (event, model) in enumerate(self.event_models.items()):\n        label = f'{event}'\n        color = colors[idx % len(colors)]\n        self.plot_event_alpha(event=event, ax=ax, scatter_kwargs=scatter_kwargs, show=False, title=title,\n                              ylabel=ylabel, xlabel=xlabel, fontsize=fontsize, label=label, color=color,\n                              ticklabelsize=ticklabelsize)\n    ax.legend()\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.plot_all_events_beta","title":"<code>plot_all_events_beta(ax=None, colors=COLORS, show=True, title=None, xlabel='Value', ylabel='$\\\\beta_{j}$', fontsize=18, ticklabelsize=15)</code>","text":"<p>This function plots the $ beta_{j} $ coefficients and standard errors of all the events.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>colors</code> <code>(list, Optional)</code> <p>colors names</p> <code>COLORS</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'Value'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\beta_{j}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <code>ticklabelsize</code> <code>(int, Optional)</code> <p>axes xticklabels, yticklabels fontsize</p> <code>15</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_all_events_beta(self, ax: plt.Axes = None, colors: list = COLORS, show: bool = True,\n                         title: Union[str, None] = None, xlabel: str = 'Value',  ylabel: str = r'$\\beta_{j}$',\n                         fontsize: int = 18, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots the $ beta_{j} $ coefficients and standard errors of all the events.\n\n    Args:\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        colors (list, Optional): colors names\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n        ticklabelsize (int, Optional): axes xticklabels, yticklabels fontsize\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    title = r'$\\beta_{j}$' + f' for all events' if title is None else title\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    se_df = self.get_beta_SE()\n\n    for idx, col in enumerate(se_df.columns):\n        if idx % 2 == 1:\n            continue\n        y = np.arange((idx//2)*len(se_df), (1+(idx//2))*len(se_df))\n        ax.errorbar(x=se_df.iloc[:, idx].values, y=y,\n                   color=colors[idx % len(colors)], xerr=se_df.iloc[:, idx+1].values, label=f'{col}',\n                   markersize=6, ls='', marker='o')\n\n    yt = list(se_df.index) * (len(se_df.columns) // 2)\n    ax.set_yticks(np.arange(0, len(yt)))\n    ax.set_yticklabels(yt)\n    ax.set_title(title, fontsize=fontsize)\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    ax.grid()\n    plt.gca().invert_yaxis()\n    ax.legend()\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.plot_event_alpha","title":"<code>plot_event_alpha(event, ax=None, scatter_kwargs={}, show=True, title=None, xlabel='t', ylabel='$\\\\alpha_{jt}$', fontsize=18, color=None, label=None, ticklabelsize=15)</code>","text":"<p>This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>ax</code> <code>(Axes, Optional)</code> <p>ax to use</p> <code>None</code> <code>scatter_kwargs</code> <code>(dict, Optional)</code> <p>keywords to pass to the scatter function</p> <code>{}</code> <code>show</code> <code>(bool, Optional)</code> <p>if to use plt.show()</p> <code>True</code> <code>title</code> <code>(str, Optional)</code> <p>axes title</p> <code>None</code> <code>xlabel</code> <code>(str, Optional)</code> <p>axes xlabel</p> <code>'t'</code> <code>ylabel</code> <code>(str, Optional)</code> <p>axes ylabel</p> <code>'$\\\\alpha_{jt}$'</code> <code>fontsize</code> <code>(int, Optional)</code> <p>axes title, xlabel, ylabel fontsize</p> <code>18</code> <code>color</code> <code>(str, Optional)</code> <p>color name to use</p> <code>None</code> <code>label</code> <code>(str, Optional)</code> <p>label name</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>output figure</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def plot_event_alpha(self, event: Union[str, int], ax: plt.Axes = None, scatter_kwargs: dict = {},\n                     show=True, title=None, xlabel='t', ylabel=r'$\\alpha_{jt}$', fontsize=18,\n                     color: str = None, label: str = None, ticklabelsize: int = 15) -&gt; plt.Axes:\n    \"\"\"\n    This function plots a scatter plot of the $ alpha_{jt} $ coefficients of a specific event.\n\n    Args:\n        event (Union[str, int]): event name\n        ax (matplotlib.pyplot.Axes, Optional): ax to use\n        scatter_kwargs (dict, Optional): keywords to pass to the scatter function\n        show (bool, Optional): if to use plt.show()\n        title (str, Optional): axes title\n        xlabel (str, Optional): axes xlabel\n        ylabel (str, Optional): axes ylabel\n        fontsize (int, Optional): axes title, xlabel, ylabel fontsize\n        color (str, Optional): color name to use\n        label (str, Optional): label name\n\n    Returns:\n        ax (matplotlib.pyplot.Axes): output figure\n    \"\"\"\n\n    assert event in self.events, f\"Cannot plot event {event} alpha - it was not included during .fit()\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ax.tick_params(axis='both', which='major', labelsize=ticklabelsize)\n    ax.tick_params(axis='both', which='minor', labelsize=ticklabelsize)\n    title = r'$\\alpha_{jt}$' + f' for event {event}' if title is None else title\n    label = f'{event}' if label is None else label\n    color = 'tab:blue' if color is None else color\n    alpha_df = self.event_models[event][1]\n    ax.scatter(alpha_df[self.duration_col].values, alpha_df['alpha_jt'].values, label=label,\n               color=color, **scatter_kwargs)\n    ax.set_title(title, fontsize=fontsize)\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    if show:\n        plt.show()\n    return ax\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict","title":"<code>predict(df, **kwargs)</code>","text":"Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict(self, df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_cumulative_incident_function","title":"<code>predict_cumulative_incident_function(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_cumulative_incident_function(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        if f'cif_j{event}_at_t{self.times[-2]}' not in df.columns:\n            df = self.predict_event_cumulative_incident_function(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_event_cumulative_incident_function","title":"<code>predict_event_cumulative_incident_function(df, event)</code>","text":"<p>This function adds a specific event columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_event_cumulative_incident_function(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a specific event columns of the predicted hazard function, overall survival, probabilities\n    of event occurance and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_events(df=df)\n    cols = [f'prob_j{event}_at_t{t}' for t in self.times[:-1]]\n    cif_df = df[cols].cumsum(axis=1)\n    cif_df.columns = [f'cif_j{event}_at_t{t}' for t in self.times[:-1]]\n    df = pd.concat([df, cif_df], axis=1)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_full","title":"<code>predict_full(df)</code>","text":"<p>This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance and cumulative incident function (CIF) to the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_full(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the predicted hazard function, overall survival, probabilities of event occurance\n    and cumulative incident function (CIF) to the given dataframe.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n\n    \"\"\"\n    return self.predict_cumulative_incident_function(df)\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_hazard_all","title":"<code>predict_hazard_all(df)</code>","text":"<p>This function calculates the hazard for all the events at all time values included in the training set for each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_all(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at all time values included in the training set for each\n    event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    df = self.predict_hazard_t(df, t=self.times[:-1])\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_hazard_jt","title":"<code>predict_hazard_jt(df, event, t)</code>","text":"<p>This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>Union[Iterable, int]</code> <p>times to calculate the hazard for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def predict_hazard_jt(self,\n                      df: pd.DataFrame,\n                      event: Union[str, int],\n                      t: Union[Iterable, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This method calculates the hazard for the given event at the given time values if they were included in the training set of the event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        event (Union[str, int]): event name\n        t (Union[Iterable, int]): times to calculate the hazard for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    t = self._validate_t(t, return_iter=True)\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n\n    model = self.event_models[event]\n    alpha_df = model[1].set_index(self.duration_col)['alpha_jt'].copy()\n\n    _t = np.array([t_i for t_i in t if (f'hazard_j{event}_t{t_i}' not in df.columns)])\n    if len(_t) == 0:\n        return df\n    temp_df = df.copy()\n    if isinstance(self.covariates, list):\n        beta_j_x = temp_df[self.covariates].dot(getattr(model[0], self.beta_models_params_attr))\n    elif isinstance(self.covariates, dict):\n        beta_j_x = temp_df[self.covariates[event]].dot(getattr(model[0], self.beta_models_params_attr))\n    temp_df[[f'hazard_j{event}_t{c}' for c in _t]] = pd.concat(\n        [self._hazard_inverse_transformation(alpha_df[c] + beta_j_x) for c in _t], axis=1).values\n    return temp_df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_hazard_t","title":"<code>predict_hazard_t(df, t)</code>","text":"<p>This function calculates the hazard for all the events at the requested time values if they were included in the training set of each event.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>samples to predict for</p> required <code>t</code> <code>(int, array)</code> <p>times to calculate the hazard for</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>samples with the prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_hazard_t(self, df: pd.DataFrame, t: Union[int, np.array]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the hazard for all the events at the requested time values if they were included in\n    the training set of each event.\n\n    Args:\n        df (pd.DataFrame): samples to predict for\n        t (int, np.array): times to calculate the hazard for\n\n    Returns:\n        df (pd.DataFrame): samples with the prediction columns\n    \"\"\"\n    t = self._validate_t(t)\n    self._validate_covariates_in_df(df.head())\n\n    for event, model in self.event_models.items():\n        df = self.predict_hazard_jt(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_marginal_prob_all_events","title":"<code>predict_marginal_prob_all_events(df)</code>","text":"<p>This function calculates the marginal probability per event given the covariates for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_all_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability per event given the covariates for all the events.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n    for event in self.events:\n        df = self.predict_marginal_prob_event_j(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_marginal_prob_event_j","title":"<code>predict_marginal_prob_event_j(df, event)</code>","text":"<p>This function calculates the marginal probability of an event given the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns included</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with additional prediction columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_marginal_prob_event_j(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function calculates the marginal probability of an event given the covariates.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns included\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with additional prediction columns\n    \"\"\"\n\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{self.times[-2]}' not in df.columns:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    cols = [f'prob_j{event}_at_t{_t}' for _t in self.times[:-1]]\n    marginal_prob = df[cols].sum(axis=1)\n    marginal_prob.name = f'marginal_prob_j{event}'\n    return pd.concat([df, marginal_prob], axis=1)\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_overall_survival","title":"<code>predict_overall_survival(df, t=None, return_hazards=False)</code>","text":"<p>This function adds columns of the overall survival until time t. Args:     df (pandas.DataFrame): dataframe with covariates columns     t (int): time     return_hazards (bool): if to keep the hazard columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with the additional overall survival columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_overall_survival(self,\n                             df: pd.DataFrame,\n                             t: int = None,\n                             return_hazards: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of the overall survival until time t.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        t (int): time\n        return_hazards (bool): if to keep the hazard columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with the additional overall survival columns\n\n    \"\"\"\n    if t is not None:\n        self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    all_hazards = self.predict_hazard_all(df)\n    _times = self.times[:-1] if t is None else [_t for _t in self.times[:-1] if _t &lt;= t]\n    overall = pd.DataFrame()\n    for t_i in _times:\n        cols = [f'hazard_j{e}_t{t_i}' for e in self.events]\n        t_i_hazard = 1 - all_hazards[cols].sum(axis=1)\n        t_i_hazard.name = f'overall_survival_t{t_i}'\n        overall = pd.concat([overall, t_i_hazard], axis=1)\n    overall = pd.concat([df, overall.cumprod(axis=1)], axis=1)\n\n    if return_hazards:\n        cols = all_hazards.columns[all_hazards.columns.str.startswith(\"hazard_\")]\n        cols = cols.difference(overall.columns)\n        if len(cols) &gt; 0:\n            overall = pd.concat([overall, all_hazards[cols]], axis=1)\n    return overall\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_prob_event_j_all","title":"<code>predict_prob_event_j_all(df, event)</code>","text":"<p>This function adds columns of a specific event occurrence probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_all(self, df: pd.DataFrame, event: Union[str, int]) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of a specific event occurrence probabilities.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_covariates_in_df(df.head())\n\n    if f'overall_survival_t{self.times[-2]}' not in df.columns:\n        df = self.predict_overall_survival(df, return_hazards=True)\n    for t in self.times[:-1]:\n        if f'prob_j{event}_at_t{t}' not in df.columns:\n            df = self.predict_prob_event_j_at_t(df=df, event=event, t=t)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_prob_event_j_at_t","title":"<code>predict_prob_event_j_at_t(df, event, t)</code>","text":"<p>This function adds a column with probability of occurance of a specific event at a specific a time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe with covariates columns</p> required <code>event</code> <code>Union[str, int]</code> <p>event name</p> required <code>t</code> <code>int</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe an additional probability column</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_event_j_at_t(self, df: pd.DataFrame, event: Union[str, int], t: int) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds a column with probability of occurance of a specific event at a specific a time.\n\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n        event (Union[str, int]): event name\n        t (int): time\n\n    Returns:\n        df (pandas.DataFrame): dataframe an additional probability column\n\n    \"\"\"\n    assert event in self.events, \\\n        f\"Cannot predict for event {event} - it was not included during .fit()\"\n    self._validate_t(t, return_iter=False)\n    self._validate_covariates_in_df(df.head())\n\n    if f'prob_j{event}_at_t{t}' not in df.columns:\n        if t == 1:\n            if f'hazard_j{event}_t{t}' not in df.columns:\n                df = self.predict_hazard_jt(df=df, event=event, t=t)\n            df[f'prob_j{event}_at_t{t}'] = df[f'hazard_j{event}_t{t}']\n            return df\n        elif not f'overall_survival_t{t - 1}' in df.columns:\n            df = self.predict_overall_survival(df, t=t, return_hazards=True)\n        elif not f'hazard_j{event}_t{t}' in df.columns:\n            df = self.predict_hazard_t(df, t=np.array([_t for _t in self.times[:-1] if _t &lt;= t]))\n        df[f'prob_j{event}_at_t{t}'] = df[f'overall_survival_t{t - 1}'] * df[f'hazard_j{event}_t{t}']\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.predict_prob_events","title":"<code>predict_prob_events(df)</code>","text":"<p>This function adds columns of all the events occurance probabilities. Args:     df (pandas.DataFrame): dataframe with covariates columns</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with probabilities columns</p> Source code in <code>src/pydts/base_fitters.py</code> <pre><code>def predict_prob_events(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function adds columns of all the events occurance probabilities.\n    Args:\n        df (pandas.DataFrame): dataframe with covariates columns\n\n    Returns:\n        df (pandas.DataFrame): dataframe with probabilities columns\n\n    \"\"\"\n    self._validate_covariates_in_df(df.head())\n\n    for event in self.events:\n        df = self.predict_prob_event_j_all(df=df, event=event)\n    return df\n</code></pre>"},{"location":"api/two_stages_fitter_exact/#pydts.fitters.TwoStagesFitterExact.print_summary","title":"<code>print_summary(summary_func='print_summary', summary_kwargs={})</code>","text":"<p>This method prints the summary of the fitted models for all the events.</p> <p>Parameters:</p> Name Type Description Default <code>summary_func</code> <code>(str, Optional)</code> <p>print summary method of the fitted model type (\"summary\", \"print_summary\").</p> <code>'print_summary'</code> <code>summary_kwargs</code> <code>(dict, Optional)</code> <p>Keyword arguments to pass to the model summary function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/pydts/fitters.py</code> <pre><code>def print_summary(self,\n                  summary_func: str = \"print_summary\",\n                  summary_kwargs: dict = {}) -&gt; None:\n    \"\"\"\n    This method prints the summary of the fitted models for all the events.\n\n    Args:\n        summary_func (str, Optional): print summary method of the fitted model type (\"summary\", \"print_summary\").\n        summary_kwargs (dict, Optional): Keyword arguments to pass to the model summary function.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        from IPython.display import display\n        display(self.get_beta_SE())\n\n        for event, model in self.event_models.items():\n            print(f'\\n\\nModel summary for event: {event}')\n            display(model[1].set_index([self.event_type_col, self.duration_col]))\n    except:\n        print(self.get_beta_SE())\n\n        for event, model in self.event_models.items():\n            print(f'\\n\\nModel summary for event: {event}')\n            print(model[1].set_index([self.event_type_col, self.duration_col]))\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#pydts.utils.get_expanded_df","title":"<code>pydts.utils.get_expanded_df(df, event_type_col='J', duration_col='X', pid_col='pid')</code>","text":"<p>Expands a discrete-time survival dataset into a long-format dataframe suitable for modeling. This function receives a dataframe where each row corresponds to a subject with observed  event type and duration. It returns an expanded dataframe where each subject is represented  by multiple rows, one for each time point up to their observed time. Right censoring is allowed and should be indicated by event type 0.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Original input dataframe containing one row per subject.</p> required <code>event_type_col</code> <code>str</code> <p>Name of the column indicating event type. Censoring is marked by 0.</p> <code>'J'</code> <code>duration_col</code> <code>str</code> <p>Name of the column indicating event or censoring time.</p> <code>'X'</code> <code>pid_col</code> <code>str</code> <p>Name of the column indicating subject/patient ID.</p> <code>'pid'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Expanded dataframe in long format, with one row per subject-time pair.</p> Source code in <code>src/pydts/utils.py</code> <pre><code>def get_expanded_df(\n        df: pd.DataFrame,\n        event_type_col: str = 'J',\n        duration_col: str = 'X',\n        pid_col: str = 'pid') -&gt; pd.DataFrame:\n    \"\"\"\n    Expands a discrete-time survival dataset into a long-format dataframe suitable for modeling. This function receives a dataframe where each row corresponds to a subject with observed  event type and duration. It returns an expanded dataframe where each subject is represented  by multiple rows, one for each time point up to their observed time. Right censoring is allowed and should be indicated by event type 0.\n\n    Args:\n        df (pd.DataFrame): Original input dataframe containing one row per subject.\n        event_type_col (str): Name of the column indicating event type. Censoring is marked by 0.\n        duration_col (str): Name of the column indicating event or censoring time.\n        pid_col (str): Name of the column indicating subject/patient ID.\n\n    Returns:\n        pd.DataFrame: Expanded dataframe in long format, with one row per subject-time pair.\n    \"\"\"\n    unique_times = df[duration_col].sort_values().unique()\n    result_df = df.reindex(df.index.repeat(df[duration_col]))\n    result_df[duration_col] = result_df.groupby(pid_col).cumcount() + 1\n    # drop times that didn't happen\n    result_df.drop(index=result_df.loc[~result_df[duration_col].isin(unique_times)].index, inplace=True)\n    result_df.reset_index(drop=True, inplace=True)\n    last_idx = result_df.drop_duplicates(subset=[pid_col], keep='last').index\n    events = sorted(df[event_type_col].unique())\n    result_df.loc[last_idx, [f'j_{e}' for e in events]] = pd.get_dummies(\n        result_df.loc[last_idx, event_type_col], dtype=int).values\n    result_df[[f'j_{e}' for e in events]] = result_df[[f'j_{e}' for e in events]].fillna(0)\n    result_df[f'j_0'] = 1 - result_df[[f'j_{e}' for e in events if e &gt; 0]].sum(axis=1)\n    return result_df\n</code></pre>"}]}